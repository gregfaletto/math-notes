%\documentclass{article}
%
%\usepackage{fancyhdr}
%\usepackage{extramarks}
%\usepackage{amsmath}
%\usepackage{amsthm}
%\usepackage{amsfonts}
%\usepackage{tikz}
%\usepackage{enumerate}
%\usepackage{graphicx}
%\graphicspath{ {images/} }
%\usepackage[plain]{algorithm}
%\usepackage{algpseudocode}
%\usepackage[document]{ragged2e}
%\usepackage{textcomp}
%\usepackage{color}   %May be necessary if you want to color links
%\usepackage{import}
%\usepackage{hyperref}
%\hypersetup{
%    colorlinks=true, %set true if you want colored links
%    linktoc=all,     %set to all if you want both sections and subsections linked
%    linkcolor=black,  %choose some color if you want links to stand out
%}
%\usepackage{import}
%\usepackage{natbib}
%
%\usetikzlibrary{automata,positioning}
%
%%
%% Basic Document Settings
%%
%
%\topmargin=-0.45in
%\evensidemargin=0in
%\oddsidemargin=0in
%\textwidth=6.5in
%\textheight=9.0in
%\headsep=0.25in
%\setlength{\parskip}{1em}
%
%\linespread{1.1}
%
%\pagestyle{fancy}
%\lhead{\hmwkAuthorName}
%\lfoot{\lastxmark}
%\cfoot{\thepage}
%
%\renewcommand\headrulewidth{0.4pt}
%\renewcommand\footrulewidth{0.4pt}
%
%\setlength\parindent{0pt}
%
%
%\newcommand{\hmwkTitle}{Math Review Notes---Time Series}
%\newcommand{\hmwkAuthorName}{\textbf{G. Faletto} }
%
%%
%% Title Page
%%
%
%\title{
%    \vspace{2in}
%    \textmd{\textbf{ \hmwkTitle}}\\
%}
%
%\author{Gregory Faletto}
%\date{}
%
%\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}
%
%%
%% Various Helper Commands
%%
%
%% Useful for algorithms
%\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}
%
%% For derivatives
%\newcommand{\deriv}[2]{\frac{\mathrm{d} #1}{\mathrm{d} #2}}
%
%% For partial derivatives
%\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
%
%% Integral dx
%\newcommand{\dx}{\mathrm{d}x}
%
%% Alias for the Solution section header
%\newcommand{\solution}{\textbf{\large Solution}}
%
%% Probability commands: Expectation, Variance, Covariance, Bias
%\newcommand{\E}{\mathbb{E}}
%\newcommand{\Var}{\mathrm{Var}}
%\newcommand{\Cov}{\mathrm{Cov}}
%\newcommand{\Bias}{\mathrm{Bias}}
%\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
%\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
%\DeclareMathOperator{\Tr}{Tr}
%
%\theoremstyle{definition}
%\newtheorem{theorem}{Theorem}
%\numberwithin{theorem}{section}
%\theoremstyle{definition}
%\newtheorem{proposition}[theorem]{Proposition}
%%\numberwithin{theorem}{subsection}
%\theoremstyle{definition}
%\newtheorem{lemma}[theorem]{Lemma}
%%\numberwithin{lemma}{subsection}
%\theoremstyle{definition}
%\newtheorem{corollary}{Corollary}[theorem]
%%\numberwithin{corollary}{subsection}
%\theoremstyle{definition}
%\newtheorem{definition}{Definition}[section]
%%\numberwithin{definition}{subsection}
%\newtheorem*{remark}{Remark}
%
%% Tilde
%\newcommand{\textapprox}{\raisebox{0.5ex}{\texttildelow}}
%
%\begin{document}
%
%\maketitle
%
%\pagebreak
%
%\tableofcontents
%
%\
%
%\
%
%\begin{center}
%Last updated \today
%\end{center}
%
%
%
%\newpage
%
%%
%%
%%
%%
%%
%%
%%
%%
%%
%%
%%
%%
%% Time Series

\chapter{Time Series}

These notes are based on my notes from \textit{Time Series and Panel Data Econometrics} (1st edition) by M. Hashem Pesaran \citep{pesaran-2015-text} as well as coursework for Economics 613: Economic and Financial Time Series I at USC.

%%% Chapter 6
\section{Chapter 6: ARDL Models}

In an ARDL model, if the error are serially correlated, then the coefficient estimates are biased (even as \(T \to \infty\)).


%%% Chapters 12 and 13
\section{Chapters 12 and 13: Intro to Stochastic Processes and Spectral Analysis}

\textbf{Stationarity conditions:} \( \{X_t\} \) is \textbf{strictly stationary} if the joint distribution functions of \( \{ X_{t_1}, X_{t_2}, \ldots, X_{t_k}\} \) and \( \{ X_{t_1+h}, X_{t_2+h}, \ldots, X_{t_k+h}\} \) are identical for all values of \(t_1,  t_2, \ldots, t_k\) and h and all positive integers \(k\). 

\begin{definition}\label{tf.def.cov.stationary} \(X_t\) is \textbf{weakly (or covariance) stationary} if it has a constant mean and variance and its covariance function \(\gamma(t_1, t_2)\) depends only on the absolute difference \(| t_1 - t_2|\), namely \(\gamma(t_1, t_2) = \gamma(|t_1 - t_2|)\).
\end{definition}

\begin{definition}\label{ts.12.trendstationary}\(X_t\) is said to be \textbf{trend stationary} if \(y_t = X_t - d_t\) is covariance stationary, where \(d_t\) is the perfectly predictable component of \(X_t\).
\end{definition}

The process \(\{\epsilon_t\} \) is said to be a \textbf{white noise process} if it has mean zero, a constant variance, and \(\epsilon_t\) and \(\epsilon_s\) are uncorrelated for all \(s \neq t\).

\textbf{Autocovariance generating function:} The autocovariance generating function for the general linear stationary process \(y_t = \sum_{i=0}^\infty a_i \epsilon_{t-i}\) is given by:

\[
G(z) = \sigma^2 a(z)a(z^{-1})
\]

where \(a(z) = \sum_{i=0}^\infty a_i z^i\). 

\textbf{Wold's Decomposition} (Theorem 42, p. 275, Section 12.5) Any trend-stationary process \(\{y_t\}\) can be represented in the form of \(y_t = d_t + \sum_{i=0}^\infty \alpha_i \epsilon_{t-i}\) where \(\alpha_0 = 1\) and \(\sum_{i=0}^\infty \alpha_i^2 < K < \infty\). The term \(d_t\) is a deterministic component, while \(\{\epsilon_t\}\) is a serially uncorrelated process: \(\epsilon_t = y_t - \E(y_t \mid y_{t-1}, y_{t-2}, \ldots) \).

\textbf{Stationarity conditions for an ARMA(\(p,q\)) process:} Consider the ARMA(\(p, q\)) process 

\[
y_t = \sum_{i=1}^p \phi_i y_{t-i} + \sum_{i=0}^q \theta_i \epsilon_{t-i}, \ \ \ \theta_0 = 1
\]

The MA part is stationary for any finite \(q\). The AR part is stationary if the roots of the characteristic equation

\[
\lambda^t = \sum_{i=1}^p \phi_i \lambda^{t-i}
\]

lie strictly inside the unit circle. Alternatively, in terms of \(z = \lambda^{-1}\), the process is stationary if the roots of 

\[
1 - \sum_{i=1}^p \phi_i z^i = 0
\]

lie outside the unit circle. The ARMA process is \textbf{invertible} (so that \(y_t\) can be solved uniquely in terms of its past values) if all the roots of 

\[
1 - \sum_{i=1}^p \theta_i z^i = 0
\]

fall outside the unit circle.

\textbf{Spectral Density Function:} Definition (Equation 13.3):

\[
f(\omega) = \frac{1}{2 \pi} \sum_{h = - \infty}^\infty \gamma(h) e^{i h \omega}, \omega \in (-\pi, \pi)
\]

Equation (13.5):

\[
f(\omega) = \frac{1}{2\pi} \bigg[ \gamma(0) + 2 \sum_{h=1}^\infty \gamma(h) \cos(h \omega) \bigg], \ \ \ \omega \in [0, \pi]
\]

Can also be found using the autocovariance generating function. We have (Equation 13.6, section 13.3.1)

\[
f(\omega) = \frac{1}{2\pi}G(e^{i \omega}) = \frac{\sigma^2}{2 \pi} a(e^{i \omega}) a(e^{- i \omega})
\]

\textbf{Properties of spectral density function:}

\begin{enumerate}[(1)]

\item \(f(\omega)\) always exists and is bounded if \(\gamma(h)\) is absolutely summable.

\item \(f(\omega)\) is symmetric.

\item The spectrum of a stationary process is finite at zero frequency; that is, \(f(0) < \infty\).

\end{enumerate}

Linear (time-domain) processes don't have to be stationary, but to write something as a frequency-domain process, it must be stationary.

\subsection{Worked Examples}\label{ts.ch13.examples}

% Midterm problem 2 part (2)

\textbf{Midterm Problem 2 part (1) (chapter 12 exercise 6)}

\textbf{Midterm Problem 2 part (2) (exercise 7 in chapter 12; similar to exercise 1 in chapter 14.} Suppose \(\{y_t\}\) has the following general linear process

\[
y_t = \mu + \alpha(L)\epsilon_t, \ \ \ \ \epsilon_t \sim i.i.d. \ (0, \sigma^2)
\]

where \(\alpha(L) = \alpha_0 + \alpha_1 L + \alpha_2 L^2 + \ldots; \ \alpha_0 = 1\). Let 

\[
\overline{y}_T = \frac{1}{T} \sum_{t=1}^T y_t
\]

\[
\gamma(h) = \E[(y_t - \mu)(y_{t-h} - \mu)]
\]

\[
\hat{\gamma}(h) = \frac{1}{T} \sum_{t=h+1}^T(y_t - \overline{y}_T)(y_{t-h} - \overline{y}_T)
\]

Derive the conditions under which

\begin{enumerate}[(a)]

\item \(\overline{y}_T\) is a consistent estimator of \(\mu\) as \(T \to \infty\)

\item For fixed \(h\), \(\hat{\gamma}(h)\) is a consistent estimator of \(\gamma(h)\) as \(T \to \infty\).

\end{enumerate}

\textbf{Solution.}

% 2(2)(a) solution
\begin{enumerate}[(a)]

\item 
This is an MA(\(\infty\)) process. By Chebyshev's Inequality (Theorem \ref{asym.cheby}), \(\overline{y}_T\) is a consistent estimator of \(\mu\) as \(T \to \infty\) if \(\lim_{T \to \infty} \E(\overline{y}_T) = \E(y_T) = \mu\) and \(\lim_{T \to \infty} \Var(\overline{y}_T) = 0\). In this case in particular (MA(\(\infty\)) process), we can write

\[
\overline{y}_T = \frac{1}{T} \sum_{t=1}^T \big( \mu + \alpha(L)\epsilon_t \big) = \frac{1}{T}\cdot T \mu + \frac{1}{T} \sum_{t=1}^T \alpha(L)\epsilon_t = \mu + \frac{1}{T}\sum_{t=1}^T \alpha(L)\epsilon_t 
\]

Then we have

\[
\E(\overline{y}_T) = \mu + \frac{1}{T}\E \bigg( \sum_{t=1}^T \alpha(L)\epsilon_t  \bigg)  = \mu + \frac{1}{T} \sum_{t=1}^T  \E ( \alpha(L)\epsilon_t  ) = \mu
\]

\[
\Var(\overline{y}_T) = 0 + \frac{1}{T^2} \Var \bigg( \sum_{t=1}^T \alpha(L)\epsilon_t  \bigg) = \frac{1}{T^2} \sum_{t=1}^T \Var[ \alpha(L)\epsilon_t ] = \frac{1}{T^2} \sum_{t=1}^T \E[ \alpha(L)\epsilon_t ] ^2 = \frac{1}{T} \alpha(1)^2 \E[\epsilon_t ] ^2
\]

\[
= \frac{\sigma^2}{T} \alpha(1)^2
\]

Therefore a sufficient condition for consistency is 

\[
\lim_{T \to \infty}  \frac{\sigma^2}{T} \alpha(1)^2 = 0 \iff \alpha(1)^2 < \infty \impliedby \boxed{\sum_{i=0}^\infty \alpha_i = 0}
\]

% 2(2)(b) solution
\item See section \ref{sec:tsch14.2.2}.

%By Chebyshev's Inequality, \(\hat{\gamma}(h) \) is a consistent estimator of \(\gamma(h)\) as \(T \to \infty\) if \(\lim_{T \to \infty} \E(\hat{\gamma}(h) ) = \gamma(h) \) and \(\lim_{T \to \infty} \Var(\hat{\gamma}(h) ) = 0\).

% \textbf{ask about the derivation}  Per the derivation in section~\ref{sec:tsch14.2.2}, we have that
%
%%\[
%%\hat{\gamma}(h) = \frac{1}{T} \sum_{t=h+1}^T(y_t - \overline{y}_T)(y_{t-h} - \overline{y}_T)
%%\]
%%
%%\[
%%= \frac{1}{T} \sum_{t=h+1}^T(y_t - \mu + \mu - \overline{y}_T)(y_{t-h} - \mu + \mu - \overline{y}_T)
%%\]
%
%\[
%\hat{\gamma}(h)  = \frac{1}{T} \sum_{t=h+1}^T(y_t - \mu)(y_{t-h} - \mu) + \mathcal{O}_p(T^{-1})
%\]
%
%For \(\hat{\gamma}(h)\) to be consistent, we need
%
%\[
%\frac{1}{T}\sum_{t=h_1}^T (y_t - \mu)(y_{t-h} - \mu) \xrightarrow{p} \gamma(h) \iff \lim_{T \to \infty} \Pr(|\hat{\gamma}(h)-\gamma(h)| < \epsilon) = 1, \text{ for every } \epsilon > 0
%\]
%
%First we show that \((y_t - \mu)(y_{t-h} - \mu)\) is a martingale difference process:
%
%\[
%\E[(y_t - \mu)(y_{t-h} - \mu) \mid F_{t-h}] =(y_{t-h} - \mu)  \E[y_t - \mu \mid F_{t-h}] = 0
%\]
%
%\textbf{why? which theorem is being used to prove this result? ask} We need to show that
%
%\[
%\E[(y_t - \mu)^2(y_{t-h} - \mu)^2 ] = \E \bigg[ \bigg( \sum_{i=0}^\infty \alpha_i \epsilon_{t-i} \bigg)^2 \bigg( \sum_{j=0}^\infty \alpha_j \epsilon_{t - h - j} \bigg) ^2 \bigg] < \infty
%\]
%
%By the Cauchy-Schwarz Inequality (Theorem \ref{asym.cauchy.schwarz}), we have
%
%\[
%\E \left| \bigg[ \bigg( \sum_{i=0}^\infty \alpha_i \epsilon_{t-i} \bigg)^2 \bigg( \sum_{j=0}^\infty \alpha_j \epsilon_{t - h - j} \bigg) ^2 \bigg] \right| ^2 \leq \E \bigg[ \bigg( \sum_{i=0}^\infty \alpha_i \epsilon_{t-i} \bigg)^2\bigg]^2 \E \bigg[ \bigg( \sum_{j=0}^\infty \alpha_j \epsilon_{t - h - j} \bigg) ^2\bigg]^2
%\]
%
%\[
%< \infty \iff \E \bigg[ \sum_{i=0}^\infty \alpha_i \epsilon_{t-i}\bigg]^4 < \infty, \ \ \  \E \bigg[ \sum_{j=0}^\infty \alpha_j \epsilon_{t - h - j} \bigg]^4 < \infty
%\]
%
%These conditions hold if \(\E(\epsilon_t^4) < \infty\) and \(\sum_{i=0}^\infty |\alpha_i| < \infty\). Then \(\E[(y_t - \mu)^2(y_{t-h} - \mu)^2 ] < \infty\) holds and
%\[
%\hat{\gamma} \xrightarrow{p} \gamma(h)
%\]

\end{enumerate}

%
%
%
%
%
%
%
%
%%%%%% Some time series and their properties

\section{Some time series and their properties}

%%%%%%%%% White Noise Process %%%%%%%%%%%%

\subsection{White noise process:}  \(x_t = \epsilon_t\), \(\epsilon_t \sim IID(0, \sigma^2)\)

\begin{itemize}

\item Autocovariances: 

\[
\gamma(0) = \sigma^2
\]

\[
\gamma(h) =0, \ \ \forall h \neq 0
\]

\item Spectral density function:

\[
f_x(\omega) = \frac{1}{2\pi} \cdot \sigma^2 = \frac{\sigma^2}{2 \pi} \text{ (flat spectrum)}
\]

\end{itemize}

%%%%%%%%% MA(1) Process %%%%%%%%%%%%

\subsection{MA(1) process:} \(x_t = \epsilon_t + \theta \epsilon_{t-1}\) with \(\epsilon_t \sim iid(0, \sigma^2)\), \(|\rho| < 1\). 

\begin{itemize}

\item Autocovariances: By Equation (12.2), the autocovariance function is

\[
\Cov(u_t, u_{t-h})  = \gamma(h) = \sigma^2 \sum_{i=0}^{1 - |h|} a_i a_{i + |h|} \text{ if } 0 \leq |h| \leq 1
\]

\[
\implies \E(x_t^2) = \gamma(0) = (1 + \theta^2)\sigma^2
\]

\[
 \E(x_t x_{t-1}) =  \gamma(1) = \theta \sigma^2
\]

\[
\gamma(h) = 0 \ \ \forall \ |h| > 1
\]
So the covariance matrix is

\[
\begin{pmatrix} 
\sigma^2(1+\theta^2) & \sigma^2\theta & 0 & 0 & \cdots & 0 \\
\sigma^2\theta & \sigma^2(1+\theta^2) & \sigma^2\theta & 0 & \cdots & 0 \\
0 & \sigma^2\theta & \sigma^2(1+\theta^2) & \sigma^2\theta & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \cdots & \vdots \\
 0 & 0& \cdots & \sigma^2\theta  & \sigma^2(1+\theta^2)   & \sigma^2\theta  \\
0 & 0 &  \cdots & 0  & \sigma^2\theta  & \sigma^2(1+\theta^2) 
\end{pmatrix} 
\]

\[
= \sigma^2(1 + \theta^2)I_T + \sigma^2 \theta A
\]

where \(A\) is defined as in section 14.3.2 (p. 304).

\item Spectral density function:

\[
f(\omega) = \frac{\sigma^2}{2\pi} \big[ 1+ 2  \theta \cos( \omega)  + \rho^2  \big] , \ \ \ \omega \in [0, \pi]
 \]

\end{itemize}



%where \(a_0 = 1\) and \(a_1 = \theta\). Therefore we have
%
%\[
% \Var(u_t) = \gamma_0  = \sigma^2 \sum_{i=0}^{1} a_i a_{i} = \sigma^2(1\cdot1 + \theta \cdot \theta) = \sigma^2(1+\theta^2) \ \forall t \in \{1, 2, \ldots, T\}
%\]
%
%\[
%\Cov(u_t, u_{t-1}) = \gamma(1)  = \sigma^2 \sum_{i=0}^{0} a_i a_{i + 1} = \sigma^2\theta \ \forall t \in \{1, 2, \ldots, T\}
%\]
%
%\[
%\Cov(u_t, u_{t-h}) = 0 \ \text{ if } |h| > 1 \ \forall t \in \{1, 2, \ldots, T\}
%\]

%%%%%%%%% MA(\(\infty\)) process %%%%%%%%%%%%

\subsection{MA(\(\infty\)) process:} 

This process is covariance stationary.

\begin{itemize}

\item Autocovariances:

\end{itemize}

%%%%%%%%% AR(1) Process %%%%%%%%%%%%

\subsection{AR(1) process:} \label{ts.ar1} \(x_t = \phi x_{t-1} + \epsilon_t\), \(|\phi| < 1\), \(\epsilon_t \sim IID(0, \sigma^2)\). 

\begin{itemize}

\item Yule-Walker Equations: 

\[
\E[x_t x_{t-h} ]= \E[ \phi x_{t-1}x_{t-h}] + \E[ \epsilon x_{t-h}]
\]

\[
\gamma_h = \phi \gamma_{h-1} + \E[ \epsilon x_{t-h}]
\]

\[
\implies \gamma_0 = \phi \gamma_1+\sigma^2,  \  \ \gamma_h = \phi \gamma_{h-1} \ \forall h \geq 1 
\]

\item Autocovariances:

\[
\gamma(0) = \frac{\sigma^2}{1 - \phi^2}
\]

%\[
%\gamma(1) = \frac{\sigma^2\rho}{1 - \rho^2}
%\]

\[
\gamma_h = \frac{\sigma^2 \phi^h}{1 - \rho^2} = \phi^h \gamma(0) \ \ \forall \ h \geq 1
\]

\[
\implies \Cov(x) = 
\]

\[
\begin{pmatrix} 
\sigma^2/(1 - \phi^2) &  \sigma^2\phi/(1 - \phi^2) & \sigma^2\phi^2/(1 - \phi^2)  &  \sigma^2\phi^3/(1 - \phi^2) & \cdots &  \sigma^2\phi^{T - 1} /(1 - \phi^) \\
\sigma^2\phi^/(1 - \phi)& \sigma^2/(1 - \phi^2) & \sigma^2\phi/(1 - \phi^2) & \sigma^2\phi^2/(1 - \phi^2) & \cdots & \sigma^2\phi^{T - 2}/(1 - \phi^2) \\
\sigma^2\phi^2/(1 - \phi^2) & \sigma^2\phi/(1 - \phi^2) & \sigma^2/(1 - \phi^2) & \sigma^2\phi/(1 - \phi^2) & \cdots & \sigma^2\phi^{T - 3}/(1 - \phi^2)  \\
\vdots & \vdots & \vdots & \vdots & \cdots & \vdots \\
\sigma^2\phi^{T-2}/(1 - \phi^2) & \sigma^2\phi^{T-3}/(1 - \phi^2) & \cdots &\sigma^2\phi/(1 - \phi^2)  & \sigma^2/(1 - \phi^2) &\sigma^2\phi/(1 - \phi^2) \\
\sigma^2\phi^{T-1} /(1 - \phi^2) & \sigma^2\phi^{T-2}/(1 - \phi^2) &  \cdots & \sigma^2\phi^2/(1 - \phi^2)  & \sigma^2\phi/(1 - \phi^2)  & \sigma^2/(1 - \phi^2)
\end{pmatrix} 
\]

\item If stationary, can be written as an infinite MA process with absolutely summable coefficients

\[
x_t = \sum_{i=0}^ \infty \phi^i \epsilon_{t-i} = \bigg( \frac{1}{1 - \phi L} \bigg) \epsilon_t
\]

\item Autocovariance generating function:

\[
G(z) = \bigg( \frac{\sigma^2}{1 - \phi ^2} \bigg) \bigg( 1 + \sum_{h=1}^\infty \phi^h(z^h + z^{-h}) \bigg)
\]

%\[
%= \sigma^2 \bigg(\frac{1}{1 - \phi^2}\bigg)I_T + \sigma^2 \bigg(\frac{\phi}{1 - \phi^2}\bigg) A = \frac{\sigma^2}{1 - \phi^2} ( I_T + \phi A)
%\]

\item Spectral density function: 

\[
f(\omega) = \frac{1}{2\pi} \sum_{h=-\infty}^\infty \frac{\sigma^2 \phi^{|h|}}{(1 - \phi^2)} (e^{i \omega})^h = \frac{1}{2\pi} \frac{\sigma^2}{(1 - \phi e^{i \omega})(1 - \phi e^{- i \omega})} = \frac{1}{2 \pi} \frac{\sigma^2}{1 - 2 \phi \cos(\omega) + \phi^2}
\]


\end{itemize}

%%%%%%%%% AR(2) Process %%%%%%%%%%%%

\subsection{AR(2) process:} \(x_t = \phi_1x_{t-1} + \phi_2x_{t-2} + \epsilon\), \(|\phi_1| < 1\), \(|\phi_2| < 1\), \(\epsilon_t \sim IID(0, \sigma^2)\). 

Can be written as 

\[
x_t = \frac{1}{1 - \phi L} \epsilon_t = \epsilon_t + \phi \epsilon_{t-1} + \phi^2 \epsilon_{t-2} + \ldots
\]

\begin{itemize}

\item Yule-Walker equations:

\[
\E[x_tx_{t-h} ]= \E[ \phi_1x_{t-1}x_{t-h}] +\E[ \phi_2x_{t-2}x_{t-h}] + \E[ \epsilon x_{t-h}]
\]

\[
\gamma_h = \phi_1 \gamma_{h-1} + \phi_2 \gamma_{h-2} + \E[ \epsilon x_{t-h}]
\]

\[
\implies \boxed{ \gamma_0 = \phi_1 \gamma_1 + \phi_2 \gamma_2 +\sigma^2,  \ \ \gamma_1 = \phi_1 \gamma_{0} + \phi_2 \gamma_{1}, \ \ \gamma_2 = \phi_1 \gamma_{1} + \phi_2 \gamma_{0}  }
\]

\item Autocovariances:

\end{itemize}

%%%%%%%%% AR(p) Process %%%%%%%%%%%%

\subsection{AR(\(p\)) process:} \(x_t = \phi_1x_{t-1} + \phi_2x_{t-2} + \ldots + \phi_p x_{t-p} + \epsilon\), \(|\phi_i| < 1\), \(\epsilon_t \sim IID(0, \sigma^2)\). 

\begin{itemize}

\item Stationary if the eigenvalues of \(\boldsymbol{\Phi}\) lie inside the unit circle, which is equivalent to all the roots of

\[
\phi(z) = 1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p = 0
\]

being strictly larger than unity. Under this condition the AR process has the infinite-order MA representation'

\[
x_t = \sum_{i=0}^\infty \alpha_i \epsilon_{t-i}
\]

where \(\alpha_i = \phi_1 \alpha_{i-1} + \ldots + \phi_p \alpha_{i - p}\).

\item Autocovariance generating function:

\[
G(z) = \frac{\sigma^2}{\phi(z) \phi(z^{-1})}
\]

\end{itemize}

%%%%%%%%% ARMA(1, 1) Process %%%%%%%%%%%%

\subsection{ARMA(1, 1) process:} \(x_t = \phi x_{t-1} + \epsilon_t + \theta \epsilon_{t-1}\), with \(|\phi| < 1\) (implying stationarity), \(\E(\epsilon_t^2) = \sigma^2\), \(\E(\epsilon_t \epsilon_s ) = 0 \) for \(t \neq s\).

\begin{itemize}

\item Yule-Walker Equations:

\[
\gamma(0) = \phi \gamma(1) + \sigma^2(1 + \theta^2)
\]

\[
\gamma(1) = \phi \gamma(0) + \sigma^2 \phi^2
\]

\[
\gamma(h) = \phi \gamma(h-1) \ \ \forall \ h \geq 2
\]

\item Autocovariances:

\[
\gamma(0) = \sigma^2 \bigg( 1 _ \frac{(\phi + \theta)^2}{1 - \phi^2} \bigg) 
\]

\[
\gamma(1) = \sigma^2 \bigg( \phi + \theta + \frac{(\phi + \theta)^2 \phi}{1 - \phi^2} \bigg)
\]

\[
\gamma(2) = \phi^{h-1} \gamma(1) \ \forall \ h \geq 2
\]

\item Autocorrelation function:

\[
\rho(h) = \begin{cases} 
      1 & h = 0 \\
      \frac{(\phi + \theta)(1 + \phi \theta)}{1 + 2 \phi\theta + \theta^2} & h = 1 \\
      \phi^{h-1} \rho(1) & h \geq 2
   \end{cases}
\]

\item Autocovariance generating function: the autocovariance function of an ARMA(\(p, q\)) process \(\phi(L)y_t = \theta(L) \epsilon_t\) is given by

\[
f(\omega) = \sigma^2 \frac{\theta(z) \theta(z^{- 1}) }{\phi(z) \phi(z^{-1})}
\]

Plugging in for the ARMA(1,1) case yields (\textbf{double-check})

\[
f(\omega) = \sigma^2 \frac{ (1+ \theta)^2 }{(1 - \rho)^2}
\]

\item Spectral Density Function: the spectral density function of an ARMA(\(p, q\)) process \(\phi(L)y_t = \theta(L) \epsilon_t\) is given by

\[
f(\omega) = \frac{\sigma^2}{2\pi} \frac{\theta(e^{i \omega}) \theta(e^{- i \omega}) }{\phi(e^{i \omega}) \phi(e^{- i \omega})}, \ \ \omega \in [0, 2\pi]
\]

Plugging in for the ARMA(1,1) case yields

\[
f_x(\omega)= \frac{\sigma^2}{2\pi} \frac{(e^{i \omega} - \theta e^{i \omega} )(e^{-i \omega} - \theta e^{-i \omega})}{(e^{i \omega} - \phi e^{i \omega} )(e^{- i \omega} - \phi e^{- i \omega} )} = \frac{\sigma^2}{2\pi} \frac{1 - 2\theta + \theta^2}{1 -2\phi + \phi^2}
\]

\item If \(\phi = \theta\), the ARMA(1,1) process becomes a white noise process. We can see this two ways. The ARMA(1, 1) process can be represented in the following way:

\[
(1 - \phi L)y_t = (1 - \theta L) \epsilon_t
\]

Therefore \(\phi(L) = \theta(L)\) yields \(y_t = \epsilon_t\). 


We can also see that when \(\phi = \theta\), an ARMA(1,1) process is equivalent to a white noise process as follows. Plugging in \(\phi = \theta\) to the spectral density function, we have

\[
f_x(\omega) = \frac{\sigma^2}{2\pi} \frac{1 - 2\theta + \theta^2}{1 -2\theta + \theta^2} = \frac{\sigma^2}{2\pi}
\]

showing that if \(\theta = \phi\), the spectral density function is constant and independent of \(\theta\) and \(\phi \). We can see that it in fact is a white noise process. Since a white noise process has the following covariances:

\[
\gamma(0) = \sigma^2
\]

\[
\gamma(h) =0, \ \ \forall h \neq 0
\]

for a white noise process we have

\[
f_x(\omega) = \frac{1}{2\pi} \cdot \sigma^2 = \frac{\sigma^2}{2 \pi}
\]


\end{itemize}

%
%
%
%
%
%
%%%%%% Chapter 14: Estimation of Stationary Time Series Processes
\section{Chapter 14: Estimation of Stationary Time Series Processes}

\subsection{Sufficient conditions for ergodicity of mean. (Book section 14.2.1)} By Chebyshev's Inequality (see section~\ref{sec:tsch8.6}), \(\overline{y}_T\) is a consistent estimator of \(\mu\) as \(T \to \infty\) if \(\lim_{T \to \infty} \E(\overline{y}_T) = \E(y_T) = \mu\) and \(\lim_{T \to \infty} \Var(\overline{y}_T) = 0\). We have

\[
\E(\overline{y}_T) = \frac{1}{T} \E \bigg( \sum_{t=1}^T y_t \bigg) = \frac{1}{T}  \sum_{t=1}^T \E (y_t) = \mu
\]

\[
\Var(\overline{y}_T) = \frac{1}{T^2} \Var \bigg( \sum_{t=1}^T y_t \bigg) = \frac{1}{T^2} \bigg(\sum_{t=1}^T  \Var(y_t) + 2 \sum_{0 \leq i < j \leq T } \Cov(y_i, y_j)   \bigg)
\]

\[
= \frac{1}{T^2} \bigg(\sum_{t=1}^T  \gamma(0) + 2 \sum_{0 \leq i < j \leq T } \gamma(j-i)  \bigg) = \frac{1}{T^2} \bigg( T  \gamma(0) + 2 \sum_{h = 1 }^{T-1} (T - h )\gamma(h)  \bigg)
\]

\[
= \frac{1}{T} \bigg[ \gamma(0) + 2 \sum_{h=1}^{T-1} \bigg(1 - \frac{h}{T} \bigg) \gamma(h) \bigg] = \frac{1}{T^2} \boldsymbol{1}' \Var(\boldsymbol{y}) \boldsymbol{1}
\]

where \(\boldsymbol{1}\) is a vector of ones and

\[
\Var(\boldsymbol{y}) = \begin{pmatrix} 
\gamma(0) &  \gamma(1) & \cdots  &  \gamma(T-2) & \gamma(T-1) \\
\gamma(1) &  \gamma(0) & \cdots  &  \gamma(T-3) & \gamma(T-2) \\
\vdots &  \vdots & \ddots  &  \vdots & \vdots \\
\gamma(T - 2) &  \gamma(T - 3) & \cdots  &  \gamma(0) & \gamma(1) \\
\gamma(T - 1) &  \gamma(T - 2) & \cdots  &  \gamma(1) & \gamma(0) 
\end{pmatrix} 
\]

Notice that

\[
\left| \gamma(0) + 2 \sum_{h=1}^{T-1} \bigg(1 - \frac{h}{T} \bigg) \gamma(h)  \right| < \left| 2 \sum_{h=0}^{T-1}\gamma(h) \right| \leq 2 \sum_{h=0}^{T-1} \left|  \gamma(h) \right| 
\]

Therefore
\[
\sum_{h=0}^{\infty} \left|  \gamma(h) \right| < \infty
\]

is a sufficient condition for 

\[
\lim_{T \to \infty} \Var(\overline{y}_T) = \lim_{T \to \infty}  \frac{1}{T} \bigg[ \gamma(0) + 2 \sum_{h=1}^{T-1} \bigg(1 - \frac{h}{T} \bigg) \gamma(h) \bigg]  = 0
\]

\subsection{Estimation of autocovariances (Book section 14.2.2).}
\label{sec:tsch14.2.2}

A moment estimator of \(\gamma(h) =  \E[(y_t - \mu)(y_{t-h} - \mu)]\) is

\[
\hat{\gamma}(h) = \frac{1}{T} \sum_{t=h+1}^T(y_t - \overline{y}_T)(y_{t-h} - \overline{y}_T)
\]

By Chebyshev's Inequality (Theorem \ref{asym.cheby}), \(\hat{\gamma}(h) \) is a consistent estimator of \(\gamma(h)\) as \(T \to \infty\) if \\ \(\lim_{T \to \infty} \E(\hat{\gamma}(h)) = \gamma(h)\) and \(\lim_{T \to \infty} \Var(\hat{\gamma}(h)) = 0\).

\[
\hat{\gamma}(h) = \frac{1}{T} \sum_{t=h+1}^T(y_t - \overline{y}_T)(y_{t-h} - \overline{y}_T) = \frac{1}{T} \sum_{t=h+1}^T(y_t - \mu + \mu - \overline{y}_T)(y_{t-h} - \mu + \mu -  \overline{y}_T)
\]

\[
= \frac{1}{T} \sum_{t=h+1}^T(y_t - \mu)(y_{t-h} - \mu) + (y_t - \mu)(\mu -  \overline{y}_T) + (\mu - \overline{y}_T)(y_{t-h} - \mu) + (\mu - \overline{y}_T)^2
\]

\[
= \frac{1}{T} \sum_{t=h+1}^T(y_t - \mu)(y_{t-h} - \mu) + (\mu -  \overline{y}_T)  \frac{1}{T} \sum_{t=h+1}^T (y_t - \mu)+ (\mu - \overline{y}_T) \frac{1}{T} \sum_{t=h+1}^T(y_{t-h} - \mu) + \frac{1}{T}(T - h)(\mu - \overline{y}_T)^2
\]



\[
\vdots
\]

Because \textbf{where does this line come from? on page 300 of book/331 of pdf.}

\[
\overline{y}_T = \mu + \mathcal{O}_p(T^{-1/2})
\]

and for any fixed \(h\)

\[
T^{-1/2} \sum_{t=h+1}^T (y_t - \mu) = \mathcal{O}_p(1)
\]

it follows that 

\[
 (\mu -  \overline{y}_T)  \frac{1}{T} \sum_{t=h+1}^T (y_t - \mu) =  \frac{\mu}{T} \sum_{t=h+1}^T (y_t - \mu) -  \frac{\overline{y}_T}{\sqrt{T}} \cdot \frac{1}{\sqrt{T}} \sum_{t=h+1}^T (y_t - \mu) =\mathcal{O}_p(T^{-1})
\]

\[
 (\mu - \overline{y}_T) \frac{1}{T} \sum_{t=h+1}^T(y_{t-h} - \mu)  =\mathcal{O}_p(T^{-1})
\]

\[
\frac{1}{T}(T - h)(\mu - \overline{y}_T)^2 = (\mu - \overline{y}_T)^2 - \frac{h}{T}(\mu - \overline{y}_T)^2 = \mathcal{O}_p(T^{-1})
\]

\[
\implies \hat{\gamma}(h)  = \frac{1}{T} \sum_{t=h+1}^T(y_t - \mu)(y_{t-h} - \mu) + \mathcal{O}_p(T^{-1}) 
\]

%which implies that \(\lim_{T \to \infty} \E(\hat{\gamma}(h)) = \gamma(h)\). Also using results in Bartlett (1946) \textbf{where? do we need to know how to do this?} we have
%
%\[
%\lim_{T \to \infty} \Var(\hat{\gamma}_T(h) - \gamma(h)) = 0
%\]
%
%under the assumption that 
%
%\[
%\lim_{H \to \infty} H^{-1} \sum_{h=1}^H \gamma_h^2 \to 0
%\]
%
%\[
%\vdots
%\]

%Per the derivation in section~\ref{sec:tsch14.2.2}, we have that
%
%%\[
%%\hat{\gamma}(h) = \frac{1}{T} \sum_{t=h+1}^T(y_t - \overline{y}_T)(y_{t-h} - \overline{y}_T)
%%\]
%%
%%\[
%%= \frac{1}{T} \sum_{t=h+1}^T(y_t - \mu + \mu - \overline{y}_T)(y_{t-h} - \mu + \mu - \overline{y}_T)
%%\]
%
%\[
%\hat{\gamma}(h)  = \frac{1}{T} \sum_{t=h+1}^T(y_t - \mu)(y_{t-h} - \mu) + \mathcal{O}_p(T^{-1})
%\]

For \(\hat{\gamma}(h)\) to be consistent, we need

\[
\frac{1}{T}\sum_{t=h_1}^T (y_t - \mu)(y_{t-h} - \mu) \xrightarrow{p} \gamma(h) 
\]

First we show that \((y_t - \mu)(y_{t-h} - \mu)\) is a martingale difference process:

\[
\E[(y_t - \mu)(y_{t-h} - \mu) \mid F_{t-h}] =(y_{t-h} - \mu)  \E[y_t - \mu \mid F_{t-h}] = 0
\]

%\textbf{why? which theorem is being used to prove this result? ask} 

We need to show that

\[
\E[(y_t - \mu)^2(y_{t-h} - \mu)^2 ] = \E \bigg[ \bigg( \sum_{i=0}^\infty \alpha_i \epsilon_{t-i} \bigg)^2 \bigg( \sum_{j=0}^\infty \alpha_j \epsilon_{t - h - j} \bigg) ^2 \bigg] < \infty
\]

By the Cauchy-Schwarz Inequality (Theorem \ref{asym.cauchy.schwarz}), we have

\[
\E \left| \bigg[ \bigg( \sum_{i=0}^\infty \alpha_i \epsilon_{t-i} \bigg)^2 \bigg( \sum_{j=0}^\infty \alpha_j \epsilon_{t - h - j} \bigg) ^2 \bigg] \right| ^2 \leq \E \bigg[ \bigg( \sum_{i=0}^\infty \alpha_i \epsilon_{t-i} \bigg)^2\bigg]^2 \E \bigg[ \bigg( \sum_{j=0}^\infty \alpha_j \epsilon_{t - h - j} \bigg) ^2\bigg]^2
\]

\[
< \infty \iff \E \bigg[ \sum_{i=0}^\infty \alpha_i \epsilon_{t-i}\bigg]^4 < \infty, \ \ \  \E \bigg[ \sum_{j=0}^\infty \alpha_j \epsilon_{t - h - j} \bigg]^4 < \infty
\]

These conditions hold if \(\E(\epsilon_t^4) < \infty\) and \(\sum_{i=0}^\infty |\alpha_i| < \infty\). Then \(\E[(y_t - \mu)^2(y_{t-h} - \mu)^2 ] < \infty\) holds and
\[
\hat{\gamma} \xrightarrow{p} \gamma(h)
\]


%\[
%\Var(\hat{\gamma}(h)) = 
%\]

\subsection{Worked examples}

% Midterm problem 3 parts (3) and (4)
\textbf{Midterm Problem 3 parts (3) and (4) (similar to 14.7 and 14.8 material.} Consider the following ARMA(1, 1) model

\[
y_t = \phi y_{t-1} + u_t + \theta u_{t-1}, \text{ for } t = - \infty, \ldots, -1, 0 , 1, \ldots
\]

where \(|\theta| < 1\), \(|\phi| < 1\), and \(u_t\) is i.i.d. with mean zero and variance \(\sigma_u^2\), \(\E(u_t^4) < \infty\).

\begin{enumerate}[(1)]

\item Suppose that we have the data \(\{y_t : t = 0, 1, \ldots, T\} \). Consider the following estimator of \(\phi\):

\[
\hat{\phi}_T = \frac{\sum_{t=2}^T y_t y_{t-2}}{\sum_{t=2}^T y_{t-1} y_{t-2}}
\]

Show that \(\hat{\phi}\) is a consistent estimator of \(\phi\) and derive the asymptotic distribution of \(\sqrt{T}(\hat{\phi}_T - \phi)\). Comment on the case where \(\theta = \phi\).

\item Suppose that \(\sigma_u^2 = 1\) is known. Show that \(\theta\) can be consistently estimated by 

\[
\hat{\theta}_T = \frac{1}{T} \sum_{t=1}^T y_t y_{t-1} - \frac{\hat{\phi}_T}{T}\sum_{t=1}^T y_{t-1}^2
\]

\end{enumerate}

\textbf{Solution.}

\begin{enumerate}[(1)]

% Question 3 part 3
\item From the results in Question 2 part 2(b) (in section \ref{ts.ch13.examples}), since \(\E(y_t) = \E(y_{t-1}) = \E(y_{t-2}) = 0\), we know that

\[
\hat{\phi}_T = \frac{\sum_{t=2}^T y_t y_{t-2}}{\sum_{t=2}^T y_{t-1} y_{t-2}} = \frac{T^{-1}\sum_{t=2}^T y_t y_{t-2}}{T^{-1}\sum_{t=2}^T y_{t-1} y_{t-2}} \xrightarrow{p} \frac{\gamma(2)}{\gamma(1)} 
\]

By the result from Question 3 part (2), we have \(\gamma(h) = \phi \gamma(h-1)\) for \(h \geq 2\). Therefore \(\gamma(2)/\gamma(1) = \phi\), so \(\hat{\phi}_T\) is a consistent estimator for \(\phi\). To obtain the asymptotic distribution, note that

%\[
%\sqrt{T}(\hat{\phi}_T - \phi) = \sqrt{T} \bigg(\frac{T^{-1}\sum_{t=2}^T y_t y_{t-2}}{T^{-1}\sum_{t=2}^T y_{t-1} y_{t-2}}  - \frac{y_t - u_t - \theta u_{t-1}}{y_{t-1}}\bigg)
%\]

\[
\sqrt{T}(\hat{\phi}_T - \phi) = \sqrt{T} \bigg(\frac{T^{-1}\sum_{t=2}^T y_t y_{t-2}}{T^{-1}\sum_{t=2}^T y_{t-1} y_{t-2}}  - \phi \bigg) 
\]

\[
= \frac{T^{-1/2}\sum_{t=2}^T (\phi y_{t-1} + u_t + \theta u_{t-1}) y_{t-2}}{T^{-1}\sum_{t=2}^T y_{t-1}y_{t-2}} -  \frac{\phi T^{-1/2}\sum_{t=2}^T y_{t-1} y_{t-2}}{T^{-1}\sum_{t=2}^T y_{t-1} y_{t-2}}
\]

\[
= \frac{T^{-1/2}\sum_{t=2}^T (u_t + \theta u_{t-1}) y_{t-2}}{T^{-1}\sum_{t=2}^T y_{t-1}y_{t-2}} 
\]

In Question 2 part 2(b) (in section \ref{ts.ch13.examples}), we showed that 

\[
\frac{1}{T}\sum_{t=h_1}^T (y_t - \mu)(y_{t-h} - \mu) \xrightarrow{p} \gamma(h)
\]

Therefore in the denominator, since \(\E(y_{t-1}) = \E(y_{t-h} )_= 0\), we have

\[
T^{-1}\sum_{t=2}^T y_{t-1}y_{t-2} \xrightarrow{p} \gamma(1)
\]

In the numerator, 

\[
T^{-1/2}\sum_{t=2}^T (u_t + \theta u_{t-1}) y_{t-2} = \frac{1}{\sqrt{T}}\sum_{t=2}^T \big[u_t y_{t-2} + \theta u_{t-1} y_{t-2} \big] 
\]

\[
= \frac{1}{\sqrt{T}}\sum_{t=2}^T u_t y_{t-2} +  \frac{1}{\sqrt{T}}\sum_{t=2}^T  \theta  u_{t-1} y_{t-2}  = \frac{1}{\sqrt{T}}\bigg(\sum_{t=2}^{T-1} u_t y_{t-2} + u_T y_{T-2} \bigg) +  \frac{1}{\sqrt{T}}\sum_{t'=1}^{T-1}  \theta  u_{t'} y_{t'-1} 
\]

\[
= \frac{1}{\sqrt{T}} \bigg(\sum_{t=2}^{T-1} u_t y_{t-2} + u_T y_{T-2}\bigg)  + \frac{1}{\sqrt{T}} \bigg( \theta u_1 y_0 + \sum_{t=2}^{T-1}  \theta  u_{t} y_{t-1} \bigg)  = \frac{1}{\sqrt{T}} \bigg( \sum_{t=2}^{T-1} u_t(y_{t-2} + \theta y_{t-1}) + \theta u_1 y_0 + u_T y_{T-2} \bigg)
\] 

Since \(\E( u_t(y_{t-2} + \theta y_{t-1}) \mid F_{t-1} ) = 0\). Further, \(T^{-1/2} (\theta u_1 y_0 + u_{T-1}y_{T-2}) = o_p(1)\). Then by the Central Limit Theorem in martingale difference processes (Theorem (\ref{asym.clt.mart.ds})):

\noindent\fbox{
\parbox{\textwidth}{
\textbf{Theorem 28 (Central limit theorem for martingale difference sequences).} Let \(\{x_t\}\) be a martingale difference sequence with respect to the information set \(\Omega_t\). Let \(\overline{\sigma}_T^2 = \Var( \sqrt{T} \overline{x}_T) = T^{-1} \sum_{t=1}^T \sigma_t^2\). If \(\E(|x_t|^r) < K < \infty\), \(r > 2\) and for all \(t\), and

\[
\frac{1}{T} \sum_{t=1}^T x_t^2 - \overline{\sigma}_T^2 \xrightarrow{p} 0
\]

then 

\[
\sqrt{T} \cdot \frac{\overline{x}_T }{ \overline{\sigma}_T} \xrightarrow{d} \mathcal{N}(0, 1)
\]
}
}

we have

\[
\sqrt{T} \cdot \frac{\overline{x}_T }{ T^{-1/2}\sqrt{ \sum_{t=1}^T \sigma_t^2}} \xrightarrow{d} \mathcal{N}(0, 1)
\]

\[
\vdots
\]

\[
\frac{1}{ \sigma^2} \frac{\gamma(1)^2}{(1+\theta)^2) \gamma(0) + 2 \theta \gamma(1)} \sqrt{T}(\hat{\phi}_T - \phi) \xrightarrow{d} \mathcal{N} (0, 1)
\]

\[
\iff \sqrt{T}(\hat{\phi}_T - \phi) \xrightarrow{d} \mathcal{N} \bigg( 0, \sigma^2 \frac{(1+\theta)^2) \gamma(0) + 2 \theta \gamma(1)}{\gamma(1)^2} \bigg)
\]


% Question 3 part 4
\item From the results of Question 2 part 2(b) (in section \ref{ts.ch13.examples}), where we showed that 

\[
\frac{1}{T}\sum_{t=h_1}^T (y_t - \mu)(y_{t-h} - \mu) \xrightarrow{p} \gamma(h)
\]

(and since \(\E(y_{t-1}) = \E(y_{t-h} )_= 0\),)

\[
T^{-1}\sum_{t=2}^T y_{t}y_{t-1} \xrightarrow{p} \gamma(1), \ \ \ T^{-1}\sum_{t=2}^T y_{t-1}^2\xrightarrow{p} \gamma(0)
\]


and by the Weak Law of Large Numbers (Theorem \ref{asym.wlln}) we have

\[
\hat{\theta}_T = \frac{1}{T} \sum_{t=1}^T y_ty_{t-1} - \frac{\hat{\phi}_T}{T} \sum_{t=1}^T y_{t-1}^2 \xrightarrow{p} \gamma(1) - \phi \gamma(0) = \phi \gamma(0) + \theta \sigma^2 - \phi \gamma(0) =  \theta
\]

\end{enumerate}

%%%%% Chapter 14 Question 3

\textbf{Chapter 14 Question 3.} The time series \(\{y_t\}\) and \(\{x_t\}\) are independently generated according to the following schemes:

\[
y_t = \lambda y_{t-1} + \epsilon_{1t}, \ \ |\lambda| < 1
\]

\[
x_t = \rho x_{t-1} + \epsilon_{2t}, \ \ |\rho| < 1
\]

for \(t= 1, 2,\ldots, T\), where \(\epsilon_{1t}\) and \(\epsilon_{2t}\) are non-autocorrelated and distributed independently of each other with zero means and variances equal to \(\sigma_1^2\) and \(\sigma_2^2\) respectively. An investigator estimates the simple regression

\[
y_t = \beta x_t + u_t \ \ \ \ t = 1, 2, \ldots T
\]

by the OLS method. Show that

\begin{enumerate}[(a)]

\item \(\hat{\beta} \xrightarrow{p} 0 \text{ as } T \to \infty\)

\item 

\[
t_{\hat{\beta}}^2 = \frac{\hat{\beta}^2}{\widehat{\Var}(\hat{\beta})} = \frac{(T - 1)r^2}{1-r^2}
\]

\item 

\[
Tr^2 \xrightarrow{p} \frac{1 + \lambda \rho}{1 - \lambda \rho} \text{ as } T \to \infty
\]

\end{enumerate}

where \(\hat{\beta}\) is the OLS estimator of \(\beta\), \(\widehat{\Var}(\hat{\beta})\) is the estimated variance of \(\hat{\beta}\), and \(r\) is the sample correlation coefficient between \(x\) and \(y\), i.e.

\[
r^2 = \left.  \bigg(\sum_{t=1}^T x_t y_t \bigg)^2 \middle/ \bigg( \sum_{t=1}^T x_t^2 \sum_{t=1}^T y_t^2 \bigg)  \right.
\]

What are the implications of these results for problems of spurious correlation in economic time series analysis?

%%%%% Chapter 14 Question 3 Solution

\textbf{Solution.} 

\begin{enumerate}[(a)]


%%%%%%%%%%% 2(a)
\item

\[
\hat{\beta} = \frac{\sum_{t=1}^T x_t y_t }{\sum_{t=1}^T x_t^2} = \frac{T^{-1}\sum_{t=1}^T x_t y_t }{T^{-1}\sum_{t=1}^T x_t^2} 
\]

By the Weak Law of Large Numbers (Theorem \ref{asym.wlln}), we have

\[
T^{-1}\sum_{t=1}^T x_t y_t = T^{-1}\sum_{t=1}^T x_t y_t \xrightarrow{p} \E(x_ty_t) = \Cov(x_t, y_t) = 0
\]

because of the i.i.d. distributions of \(\epsilon_{1}\) and \(\epsilon_{2t}\). By the Law of Large Numbers (Theorem \ref{asym.lln2}),

\[
T^{-1}\sum_{t=1}^T x_t^2 \xrightarrow{a.s.} \E(x_t^2) = \gamma_x(0) >0
\]

Therefore

\[
\hat{\beta} \xrightarrow{p} \frac{0}{\gamma_x(0)} = 0
\]


%
%
%%%%%%%% 2(b)
\item 

\[
\widehat{\Var}(\hat{\beta}) = \frac{\hat{\sigma}^2}{S_{XX}}
\]

where

\[
\hat{\sigma}^2 = \frac{1}{T-1} \sum_{t=1}^T (y_t - \hat{\beta} x_t)^2 = \frac{1}{T-1} \sum_{t=1}^T \big( y_t^2 - 2y_t\hat{\beta} x_t + \hat{\beta}^2x_t^2\big)
\]

\[
= \frac{1}{T-1} \sum_{t=1}^T  y_t^2 - \frac{1}{T-1} \sum_{t=1}^T  \bigg( 2y_t x_t  \frac{\sum_{t=1}^T x_t y_t }{\sum_{t=1}^T x_t^2}\bigg)  +  \frac{1}{T-1} \sum_{t=1}^T  \bigg(  \bigg[ \frac{\sum_{t=1}^T x_t y_t }{\sum_{t=1}^T x_t^2}  \bigg]^2 x_t^2\bigg)
\]

\[
= \frac{1}{T-1} \sum_{t=1}^T  y_t^2 - 2\cdot \frac{1}{T-1} \frac{\big(\sum_{t=1}^T x_t y_t \big)^2}{\sum_{t=1}^T x_t^2} +  \frac{1}{T-1} \frac{\big(\sum_{t=1}^T x_t y_t \big)^2}{\sum_{t=1}^T x_t^2} 
\]

\[
= \frac{1}{T-1}\bigg( \sum_{t=1}^T  y_t^2 -  \frac{\big(\sum_{t=1}^T x_t y_t \big)^2}{\sum_{t=1}^T x_t^2} \bigg)
\]

\[
\implies  \hat{t}^2 =  \left. \frac{\big(\sum_{t=1}^T x_t y_t \big)^2}{\big(\sum_{t=1}^T x_t^2 \big)^2} \cdot \sum_{t=1}^T x_t^2 \cdot (T-1) \middle/ \bigg( \sum_{t=1}^T  y_t^2 -  \frac{\big(\sum_{t=1}^T x_t y_t \big)^2}{\sum_{t=1}^T x_t^2} \bigg) \right.
\]

\[
=  \left. \frac{\big(\sum_{t=1}^T x_t y_t \big)^2}{\sum_{t=1}^T x_t^2 } \cdot (T-1) \middle/ \bigg( \sum_{t=1}^T  y_t^2 -  \frac{\big(\sum_{t=1}^T x_t y_t \big)^2}{\sum_{t=1}^T x_t^2} \bigg) \right.
\]

\[
=  \left. \bigg(\sum_{t=1}^T x_t y_t \bigg)^2 \cdot (T-1) \middle/ \bigg( \sum_{t=1}^T  y_t^2 \sum_{t=1}^T x_t^2 -  \big(\sum_{t=1}^T x_t y_t \big)^2 \bigg) \right. 
\]

Note that

\[
r^2 = \left.  \bigg(\sum_{t=1}^T x_t y_t \bigg)^2 \middle/ \bigg( \sum_{t=1}^T x_t^2 \sum_{t=1}^T y_t^2 \bigg)  \right. , \ \ \ 1 - r^2 =   \left. \bigg[\sum_{t=1}^T x_t^2 \sum_{t=1}^T y_t^2  -  \bigg(\sum_{t=1}^T x_t y_t \bigg)^2 \bigg] \middle/ \bigg( \sum_{t=1}^T x_t^2 \sum_{t=1}^T y_t^2 \bigg)  \right. 
\]

\[
\implies \frac{r^2}{1-r^2} =  \left.  \bigg(\sum_{t=1}^T x_t y_t \bigg)^2 \middle/   \bigg[\sum_{t=1}^T x_t^2 \sum_{t=1}^T y_t^2  -  \bigg(\sum_{t=1}^T x_t y_t \bigg)^2 \bigg] \right. 
\]

Therefore

\[
\hat{t}^2 =  \left. \bigg(\sum_{t=1}^T x_t y_t \bigg)^2 \cdot (T-1) \middle/ \bigg( \sum_{t=1}^T  y_t^2 \sum_{t=1}^T x_t^2 -  \big(\sum_{t=1}^T x_t y_t \big)^2 \bigg) \right.   = \boxed{\frac{(T-1)r^2}{1 - r^2}}
\]

%
%
%%%%%%%% 2(c)
\item

\[
Tr^2 = T \cdot \frac{\big(T^{-1}\sum_{t=1}^T x_t y_t \big)^2}{T^{-1}\sum_{t=1}^T x_t^2 \cdot T^{-1} \sum_{t=1}^T y_t^2 } = \frac{T^{-1}\big(2 \sum_{1 \leq i < j \leq T} x_i x_j y_i y_j + \sum_{t=1}^T x_t^2y_t^2 \big)}{T^{-1}\sum_{t=1}^T x_t^2 \cdot T^{-1} \sum_{t=1}^T y_t^2 }
\]

\[
= \frac{2T^{-1} \sum_{1 \leq i < j \leq T} x_ix_j y_i y_j + T^{-1}\sum_{t=1}^T x_t^2y_t^2 }{T^{-1}\sum_{t=1}^T x_t^2 \cdot T^{-1} \sum_{t=1}^T y_t^2 }
\]

Note that 

\[
2T^{-1} \sum_{1 \leq i < j \leq T} (x_ix_j)( y_i y_j ) = \sum_{h=1}^{T-1} 2 T^{-1} (T-h)(x_t x_{t-h})(y_t y_{t-h})
\]

By the Weak Law of Large Numbers (Theorem \ref{asym.wlln}) and the independence of \(x_t\) and \(y_t\),

\[
\sum_{h=1}^{T-1} 2 T^{-1} (T-h)(x_t x_{t-h})(y_t y_{t-h}) \approx  2 T^{-1} \sum_{h=1}^{T-1}(x_t x_{t-h})(y_t y_{t-h}) \xrightarrow{p}2 \sum_{h=1}^{\infty}    \gamma_x(h) \gamma_y(h) 
\]

Recall that for an AR(1) process with coefficient \(\phi\), \(\gamma(h) = \phi^h \gamma(0)\). (See section \ref{ts.ar1}).

\[
= 2 \sum_{h=1}^{\infty}  \rho^h \gamma_x(0) \lambda^h \gamma_y(0)  = 2 \gamma_x(0) \gamma_y(0)   \sum_{h=1}^{\infty}    (\rho \lambda)^h 
\]

By the Weak Law of Large Numbers (Theorem \ref{asym.wlln}) and the independence of \(x_t\) and \(y_t\),

\[
T^{-1}\sum_{t=1}^T x_t^2y_t^2 \xrightarrow{a.s.} \E(x_t^2 y_t^2) = \E(x_t^2) \E( y_t^2) = \Var(x_t) \Var(y_t) = \sigma_1^2 \sigma_2^2 = \gamma_x(0) \gamma_y(0)
\]

Therefore in the numerator, we have

\[
2T^{-1} \sum_{1 \leq i < j \leq T} x_ix_j y_i y_j + T^{-1}\sum_{t=1}^T x_t^2y_t^2 \xrightarrow{p} 2 \gamma_x(0) \gamma_y(0)   \sum_{h=1}^{\infty}    (\rho \lambda)^h  + \gamma_x(0) \gamma_y(0) 
\]

\[
=  \gamma_x(0) \gamma_y(0)  \bigg( 2 \frac{\rho \lambda}{1 - \rho \lambda}     +1 \bigg)  =  \gamma_x(0) \gamma_y(0)  \bigg(  \frac{1 + \rho \lambda}{1 - \rho \lambda}      \bigg) 
\]

Next we examine the denominator. By the Law of Large Numbers (Theorem \ref{asym.lln2}),

\[
T^{-1}\sum_{t=1}^T x_t^2 \xrightarrow{a.s.} \E(x_t^2) = \Var(x_t) =  \gamma_x(0) = \sigma_2^2
\]

By the Law of Large Numbers (Theorem \ref{asym.lln2}),

\[
T^{-1}\sum_{t=1}^T y_t^2 \xrightarrow{a.s.} \E(y_t^2)  = \gamma_y(0) = \sigma_1^2
\]

Therefore in the denominator, we have

\[
T^{-1}\sum_{t=1}^T x_t^2 \cdot T^{-1} \sum_{t=1}^T y_t^2  \xrightarrow{a.s.} \gamma_x(0) \gamma_y(0) = \sigma_1^2 \sigma_2^2
\]

This yields

\[
\boxed{
Tr^2 \xrightarrow{p}  \frac{1 + \rho \lambda}{1 - \rho \lambda} }
\]

\end{enumerate}

\textbf{Lidan's explanation:} Because as \(T \to \infty\)

\[
t_{\hat{\beta}} \to \sqrt{Tr^2} = \sqrt{ \frac{1 + \lambda \rho}{1 - \lambda \rho}}
\]

so if \(\lambda \rho \approx 1\), at very high probability we will reject the null \(\hat{\beta} = 0\) when in fact \(\hat{\beta} \xrightarrow{p} 0\).

\

\textbf{My original explanation:} Because \(\hat{\beta}\) converges in probability to 0 and \(t_{\hat{\beta}}^2\) is proportional to \(r^2\) (which we would expect to be close to 0), this suggests that a regression of uncorrelated variables should result in an insignificant \(\hat{\beta}\), but if \(r^2\) is high due to a spurious correlation, the \(\hat{\beta}\) could be found to be statistically significant even if there is no meaningful relationship between \(x\) and \(y\).

%
%
%
%
%
%
%%%%%% 
%%%%% Chapter 15: Unit Root Processes

\section{Chapter 15: Unit Root Processes}

\textbf{Need to review concepts in this chapter.}

\subsection{Worked Problems}

%%%% Chapter 15 Problem 3
\textbf{Problem 3.} Suppose that a time series of interest can be decomposed into a deterministic trend, a random walk component, and stationary errors:

\begin{equation} \label{ts.hw4.1eq1}
y_t = \alpha + \delta t + \gamma_t + v_t
\end{equation}

\[
\gamma_t = \gamma_{t-1} + u_t
\]

with

\[
v_t \sim iid \ \mathcal{N}(0, \sigma_v^2), \ \ \ u_t \sim iid \ \mathcal{N}(0, \sigma_u^2), \ \ \ u_t \indep v_t
\]

Let \(\lambda = \sigma_u^2/\sigma_v^2\).

\begin{enumerate}[(a)]

\item Show that under \(\lambda =0\), \(y_t\) reduces to a trend stationary process.

\item Alternatively, suppose that \(y_t\) follows and ARIMA(0,1,1) process of the form

\begin{equation} \label{ts.hw4.1eq2}
y_t = \delta + y_{t-1} + w_t
\end{equation}

\[
w_t = \epsilon_t + \theta \epsilon_{t-1}
\]

where \(\epsilon_t\) are iid \(\mathcal{N}(0, \sigma_\epsilon^2)\). In this case show that under \(\theta = -1\), \(y_t\) is a trend stationary process.

\item Derive a relation between \(\lambda\) and the MA(1) parameter \(\theta\), and hence or otherwise show that a test of \(\theta = -1\) in (\ref{ts.hw4.1eq2}) is equivalent to a test of \(\lambda =0\) in (\ref{ts.hw4.1eq1}).

\item Show that (\ref{ts.hw4.1eq2}) as a characterization of (\ref{ts.hw4.1eq1}) implies \(\theta < 0\).

\end{enumerate}


%%%% Chapter 15 Problem 3 Solution

\textbf{Solution.}

\begin{enumerate}[(a)]

% 1a
\item \(\lambda = 0 \implies \sigma_u^2 = 0 \implies u_t = 0 \text{ (constant)}\)

\[
\implies \gamma_t = \gamma_{t-1} + 0 \iff \gamma_t = \gamma_0
\]

\[
\implies y_t =  \alpha + \delta t +  \gamma_0  + v_t = ( \alpha +  \gamma_0) + \delta  t + v_t 
\]

which is trend stationary because \(d_t = ( \alpha +  \gamma_0) + \delta t\) is perfectly predictable, and \(y_t - d_t = v_t\) is covariance stationary.

\

Also note that \(\Var(y_t - \delta t) = \sigma_v^2\), \(\Cov\big( [y_t-\delta t][y_{t-h} - \delta(t-h)]\big) = 0\), which implies trend stationarity of \(y_t\). (Recall Definition \ref{ts.12.trendstationary}: ``\(X_t\) is said to be \textbf{trend stationary} if \(y_t = X_t - d_t\) is covariance stationary, where \(d_t\) is the perfectly predictable component of \(X_t\).")

% 1b
\item 

\(\theta = -1 \implies w_t = \epsilon_t - \epsilon_{t-1} =  (1 - L)\epsilon_t\)

\[
\implies (1 - L)y_t = \delta + (1 - L)\epsilon_t \iff y_t = (1 - L)^{-1}\delta + \epsilon_t
\]

which is trend stationary because \(d_t =  (1 - L)^{-1}\delta\) is perfectly predictable, and \(y_t - d_t = \epsilon_t\) is covariance stationary.

% 1c
\item 



\textbf{Lidan's solution:} From (\ref{ts.hw4.1eq1}), let 

\[
z_t = y_t - y_{t-1} = \alpha + \delta t + \gamma_{t-1} + u_t + v_t - (\alpha + \delta (t-1) + \gamma_{t-1} + v_{t-1}) 
\]

\[
=  \delta + u_t + v_t - v_{t-1}
\]

From (\ref{ts.hw4.1eq2}), let

\[
a_t = y_t - y_{t-1} = \delta + y_{t-1} + w_t - y_{t-1} =  \delta +  \epsilon_t + \theta \epsilon_{t-1}
\]

Calculate the autocovariances for each and set them equal (using the serial independence of \(u_t\), \(v_t\), and \(\epsilon_t\) as well as the independence of \(u_t\), \(v_t\), and \(\epsilon_t\) for all \(t\)):

\begin{itemize}

\item
\[
\gamma(0): \Var(z_t) = \Var(a_t) \iff \Var(u_t + v_t - v_{t-1}) = \Var(\epsilon_t + \theta \epsilon_{t-1}) 
\]

\begin{equation}  \label{ts.hw4.1c.gamma1}
\iff  \sigma_u^2 + 2 \sigma_v^2 = \sigma_\epsilon^2 (1 + \theta^2)
\end{equation}

\item
\[
\gamma(1): \Cov(z_t, z_{t-1}) = \Cov(a_t, a_{t-1}) \iff \Cov(-v_{t-1}, v_{t-1}) = \Cov(\theta\epsilon_{t-1}, \epsilon_{t-1})
\]

\begin{equation} \label{ts.hw4.1c.gamma2}
\iff -\sigma_v^2 = \theta \sigma_\epsilon^2
\end{equation}

\item
\[
\gamma(h) \ (h \geq 2): 0 = 0
\]

\end{itemize}

Plugging (\ref{ts.hw4.1c.gamma2}) into (\ref{ts.hw4.1c.gamma1}) and using \(\lambda = \sigma_u^2/\sigma_v^2\) we have

\[
\sigma_u^2 + 2 \sigma_v^2 =  -\frac{\sigma_v^2 }{\theta}(1 + \theta^2) \iff \sigma_u^2 = \sigma_v^2(-1/\theta - \theta - 2) \iff \lambda = -1/\theta - \theta - 2
\]

\[
\iff \theta^2 + (2 + \lambda)\theta + 1 = 0 \iff \theta = \frac{-(2 + \lambda) \pm \sqrt{(2+\lambda)^2 - 4}}{2}
\]

\[
\iff \boxed{\theta = \frac{-(2 + \lambda) \pm \sqrt{\lambda^2 + 4 \lambda }}{2}}
\]

Clearly if \(\lambda = 0\) then \(\theta = -1\). The reverse is also true:

\[
\theta = -1 \implies \lambda = -1/(-1)- (-1) - 2 = 1 +  1 -2 = 0
\]

Therefore a test of \(\theta = -1\) in (\ref{ts.hw4.1eq2}) is equivalent to a test of \(\lambda = 0\) in (\ref{ts.hw4.1eq1}).

\

\textbf{original solution:} 

\[
y_t = \alpha + \delta t +  \gamma_{t-1} + u_t + v_t
\]

\[
y_{t-1} =  \alpha + \delta (t - 1) +  \gamma_{t-1} + v_{t-1}
\]

\[
\implies y_t = y_{t-1} + \delta + u_t - v_{t-1} + v_t
\]

Comparing this to the second expression, \(y_t = \delta + y_{t-1} + \epsilon_t + \theta \epsilon_{t-1}\), they match if \( u_t - v_{t-1} + v_t =  \epsilon_t + \theta \epsilon_{t-1}\). Since the distributions of \(\epsilon_t\) and \(v_t\) are both i.i.d. normal, this is the case if \(\lambda = 0\) (so that \(\sigma_u^2 = 0\) and \(u_t = 0 \ \forall \ t\)), \(\theta = -1\), and \(\sigma_\epsilon^2 = \sigma_v^2\).

% 1d
\item 

\textbf{Lidan's Solution:} By (\ref{ts.hw4.1c.gamma2}) (which follows from (\ref{ts.hw4.1eq2})),

\[
-\sigma_v^2 = \theta \sigma_\epsilon^2 \implies \theta < 0
\]

\textbf{My solution:} Again consider (2)

\[
y_t = \delta + y_{t-1} +  \epsilon_t + \theta \epsilon_{t-1}
\]

which is clearly a unit root process with a drift, compared to a re-written version of (1)

\[
y_t =  \delta + y_{t-1}  + u_t + v_t  - v_{t-1}
\]

which has a strong resemblance to a unit root process with a drift. These match up if \(\epsilon_t  = u_t + v_t\) and \(\theta < 0\). 

\end{enumerate}

%%%%%%%% Chapter 15 Problem 4

\textbf{Problem 4}



%%%%%%%%%%%% Homework 4 Question 3

\textbf{Homework 4 Problem 3.} Let \(\{u_t\}\) be an i.i.d. sequence with mean zero and variance \(\sigma^2\), and let

\[
y_t = u_1 + u_2 + \ldots + u_t
\]

with \(y_0 =0\).

\begin{enumerate}[(a)]

\item Show that

\[
 T^{-3/2} \sum_{t=1}^T y_{t-1} =T^{-1/2}  \sum_{t=1}^{T- 1} u_{t} -  T^{-3/2} \sum_{t=1}^{T - 1} t u_{t}
\]

\item Show that

\[
\begin{bmatrix} T^{-1/2} \sum_{t=1}^T u_t \\ T^{-3/2} \sum_{t=1}^T y_{t-1} \end{bmatrix} \xrightarrow{d} \mathcal{N} \bigg(0, \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1/3 \end{bmatrix} \bigg)
\]

\item Use the functional central limit theorem (Donsker's Theorem, Theorem \ref{stoch.donsker}) to show that 

\[
T^{-3/2} \sum_{t=1}^T y_{t-1} \xrightarrow{d} \sigma \int_0^1 W(r) dr
\]

where \(W(\cdot)\) is a standard Brownian motion process.

\item Use parts (a) - (c) to show that

\[
T^{-3/2} \sum_{t=1}^T t u_t \xrightarrow{d} \sigma \cdot W(a) - \sigma \int_0^1 W(r) dr
\]

\end{enumerate}

%%%%%%%%%% Homework 4 Problem 3 Solution

\textbf{Solution.} \[
y_t = u_1 + u_2 + \ldots + u_t = \sum_{i=1}^t u_i = \sum_{i=0}^t u_i
\]

where the last equality follows because \(y_0 = 0 \iff u_0 = 0\).

\[
u_t \sim iid \ (0, \sigma^2), \ \ \ y_0 = 0
\]

\begin{enumerate}[(a)]

% 3a
\item 

\[
y_T = \sum_{t=1}^T u_t
\]

\[
T^{-3/2} \sum_{t=1}^T y_{t-1} = T^{-3/2} \sum_{t=1}^T \bigg( \sum_{i=0}^{t-1} u_i \bigg)  = T^{-3/2} \sum_{t=1}^T(T - (t - 1)) u_{t-1} = T^{-3/2} \sum_{t'=0}^{T-1} (T - t') u_{t'}
\]

\[ 
 =T^{-3/2} \cdot  T \sum_{t'=0}^{T- 1} u_{t'} -  T^{-3/2} \sum_{t'=0}^{T - 1} t' u_{t'}   =T^{-1/2}  \sum_{t'=1}^{T- 1} u_{t'} -  T^{-3/2} \sum_{t'=1}^{T - 1} t' u_{t'} 
\]

\begin{equation} \label{ts.hw4.3a.result}
\implies \boxed{ T^{-3/2} \sum_{t=1}^T y_{t-1} =T^{-1/2}  \sum_{t'=1}^{T- 1} u_{t'} -  T^{-3/2} \sum_{t'=1}^{T - 1} t' u_{t'} }
\end{equation}

% 3b
\item 

By the Central Limit Theorem (Theorem \ref{asym.clt}), since \(u_t \sim (0, \sigma^2)\),  we have

\[
\frac{1}{\sqrt{T\sigma^2}} \sum_{t=1}^T u_t \xrightarrow{d} \mathcal{N}(0,1)
\]

which implies
\begin{equation}\label{ts.hw4.3b.clt}
\boxed{
 \frac{1}{\sqrt{T}} \sum_{t=1}^T u_t \xrightarrow{d} \mathcal{N}(0,\sigma^2)}
\end{equation}

The distribution of \(T^{-3/2}\sum_{t=1}^T y_{t-1}\) is trickier. Note that from (\ref{ts.hw4.3a.result}),

\[
T^{-3/2}\sum_{t=1}^T y_{t-1} = T^{-1/2}\bigg( \sum_{t=0}^{T-1}   u_{t} -  \sum_{t=0}^{T-1} \frac{ t u_{t}}{T}  \bigg)
\]

Note that \(x_t = ( t u_{t})/T \) is a martingale difference process because 

\[
\E \bigg(  \frac{ t u_{t}}{T}  \mid u_{t-1}, u_{t-2}, \ldots \bigg) = 0 
\]

and \(\Var(x_t) = \frac{1}{T^2} \Var(t u_t) = \frac{t^2}{T^2} \sigma^2 < \infty \). By the Central Limit Theorem in martingale difference processes (Theorem (\ref{asym.clt.mart.ds})):

\noindent\fbox{
\parbox{\textwidth}{
\textbf{Theorem 28 (Central limit theorem for martingale difference sequences).} Let \(\{x_t\}\) be a martingale difference sequence with respect to the information set \(\Omega_t\). Let \(\overline{\sigma}_T^2 = \Var( \sqrt{T} \overline{x}_T) = T^{-1} \sum_{t=1}^T \sigma_t^2\). If \(\E(|x_t|^r) < K < \infty\) for any \(r > 2\) and for all \(t\), and

\[
\frac{1}{T} \sum_{t=1}^T x_t^2 - \overline{\sigma}_T^2 \xrightarrow{p} 0
\]

then 

\[
\sqrt{T} \cdot \frac{\overline{x}_T }{ \overline{\sigma}_T} \xrightarrow{d} \mathcal{N}(0, 1)
\]
}
}

since

\[
T^{-1} \sum_{t=1}^T \sigma_t^2 = T^{-1} \sum_{t=1}^T  \frac{t^2}{T^2} \sigma^2 = \frac{\sigma^2}{T^3} \cdot \frac{T(T+1)(2T+1)}{6} = \frac{\sigma^2}{6} \cdot \frac{2T^2 + 3T + 1}{T^2},
\]

\[
\E(|x_t|^r) = \E\bigg( \left| \frac{ t u_{t}}{T} \right| ^r \bigg) = \frac{t^r}{T^r} \E(|u_t|^r) 
\]

which is finite for \(r=4\) if \(u_t\) has finite fourth moment, and since by the Weak Law of Large Numbers (Theorem \ref{asym.wlln})

\[
T^{-1}\sum_{t=1}^T x_t^2 -  T^{-1} \sum_{t=1}^T  \frac{t^2}{T^2} \sigma^2 = T^{-1} \sum_{t=1}^T \frac{t^2}{T^2} \big( u_t^2 - \sigma^2\big)  
\]

\[
\left| T^{-1} \sum_{t=1}^T \frac{t^2}{T^2} \big( u_t^2 - \sigma^2\big)   \right| \leq \left| T^{-1} \sum_{t=1}^T  \big( u_t^2 - \sigma^2\big) \right| \leq T^{-1} \sum_{t=1}^T   \left|  u_t^2 - \sigma^2 \right| \xrightarrow{p} 0
\]

\[
\implies T^{-1}\sum_{t=1}^T x_t^2 -  T^{-1} \sum_{t=1}^T  \frac{t^2}{T^2} \sigma^2 \xrightarrow{p} 0
\]

we have (if \(u_t\) has finite fourth moment)

\[
\left. \sqrt{T} \cdot \frac{1}{T} \bigg( \sum_{t=1}^T \frac{t u_t}{T} \bigg) \middle/\sqrt{  \frac{\sigma^2}{6} \cdot \frac{2T^2 + 3T + 1}{T^2} }  \xrightarrow{d} \mathcal{N}(0, 1) \right.
\]

\[
\iff  T^{-3/2} \bigg( \sum_{t=1}^T t u_t \bigg) \cdot \sqrt{ \frac{6}{\sigma^2} \cdot \frac{1}{2T^2 + 3T + 1}}  \xrightarrow{d} \mathcal{N}(0, 1) \implies  \sqrt{T}  \bigg( \sum_{t=1}^T t u_t \bigg) \cdot \sqrt{ \frac{6}{\sigma^2} \cdot \frac{1}{2T^2 } }  \xrightarrow{d} \mathcal{N}(0, 1)
\]


\[
\iff  T^{-3/2} \bigg( \sum_{t=1}^T \frac{t u_t}{T} \bigg) \cdot \sqrt{ \frac{3}{\sigma^2}  }   \xrightarrow{d} \mathcal{N}(0, 1) \iff T^{-1/2} \frac{\sqrt{3}}{\sigma} \sum_{t=0}^{T-1} \frac{ t u_{t}}{T}   \xrightarrow{d} \mathcal{N}\bigg(0, 1\bigg)
\]



\begin{equation}\label{ts.hw4.3b.dist}
\iff T^{-1/2} \sum_{t=0}^{T-1} \frac{ t u_{t}}{T}   \xrightarrow{d} \mathcal{N}\bigg(0, \frac{\sigma^2}{3} \bigg)
\end{equation}

To get the covariance between these distributions, we have (using the serial independence of \(u_t\))

\[
\Cov \bigg(T^{-1/2}\sum_{t=0}^{T-1} u_t  , T^{-3/2}\sum_{t=0}^{T-1} t u_t \bigg) = \E \bigg[T^{-1/2}\sum_{t=0}^{T-1} u_t  \cdot T^{-3/2}\sum_{t=0}^{T-1} t u_t \bigg] = T^{-2} \cdot \E \bigg[\sum_{t=0}^{T-1} u_t  \cdot \sum_{t=0}^{T-1} t u_t \bigg]
\]

\begin{equation}\label{ts.hw4.3b.cov.mid}
= T^{-2} \cdot \E \bigg[\sum_{t=0}^{T-1} t u_t^2   \bigg]= T^{-2} \sum_{t=0}^{T-1} t  \E( u_t^2) = \frac{\sigma^2}{T^2} \cdot \frac{T(T-1)}{2}  \to \boxed{\frac{ \sigma^2}{2}}
\end{equation}

Putting (\ref{ts.hw4.3b.clt}), (\ref{ts.hw4.3b.dist}), and (\ref{ts.hw4.3b.cov.mid}) together, we have \(T^{-3/2}\sum_{t=1}^T y_{t-1} \xrightarrow{d} \mathcal{N}(0,\sigma^2 + \sigma^2/3 - 2 \cdot \sigma^2/2 ) = \boxed{\mathcal{N}(0, \sigma^2/3)}\).  

Lastly, to get the covariance between these distributions, we have (again using the serial independence of \(u_t\))

\[
\Cov \bigg(T^{-1/2}\sum_{t=1}^{T-1} u_t  , T^{-3/2}\sum_{t=1}^T y_{t-1} \bigg) = T^{-2} \Cov \bigg(\sum_{t=1}^{T-1} u_t  , \sum_{t=1}^T(T - (t - 1)) u_{t-1} \bigg) 
\]

\[
= T^{-2} \Cov \bigg(\sum_{t=1}^{T-1} u_t  , \sum_{t'=0}^{T-1} (T - t') u_{t'} \bigg)  = T^{-2} \E \bigg( \sum_{t=1}^{T-1} (T - t) u_t^2  \bigg) = \frac{(T-1)T - T(T-1)/2}{T^2} \sigma^2 
\]

\begin{equation}\label{ts.hw4.3b.cov.fin}
= \frac{(T-1)T}{2T^2} \sigma^2  \to \boxed{ \sigma^2/2}
\end{equation}

which yields the result.
% 3c
\item  

 Let \(r \in [0, 1)\), \(t \in [0, T]\). Define

\[
R_T(r) = \frac{1}{\sigma \sqrt{T}} y_{ \big[rT \big] }
\]

where \(\big[rT \big]\) denotes the largest integer part of \(rT\) and \(y_{ \big[rT \big] } = 0\) if \(\big[rT \big] = 0\). That is,

\[
R_T(r) = \begin{cases}
0 & 0 \leq r < 1/T \\
\frac{y_1}{\sigma \sqrt{T}} & 1/T \leq r < 2/T \\
\frac{y_2}{\sigma \sqrt{T}} & 2/T \leq r < 3/T \\
\vdots & \vdots \\
\frac{y_{T-1}}{\sigma \sqrt{T}}& (T-1)/T \leq r < 1
\end{cases}
\]

We have

\[
T^{-3/2} \sum_{t=1}^T y_{t-1} = T^{-3/2} \sum_{t'=0}^{T-1} y_{t'} = T^{-3/2} \sum_{t'=0}^{T-1} (y_{t'-1} + u_{t'}) = \sigma \sum_{t'=0}^{T-1} \int_{t'/T}^{(t'+1)/T} R_T(r) dr + o_p(1)
\]

\[
=\sigma \int_{0}^1 R_T(r)dr + o_p(1) \implies \sigma \int_{0}^1 W(r) dr \text { as } T \to \infty
\]

where the last step follows from Donsker's Theorem (Theorem \ref{stoch.donsker}, the functional central limit theorem) and the continuous mapping theorem (Theorem \ref{stoch.cont.map}) and \(W(r)\) is a standard Weiner process.


\noindent\fbox{
\parbox{\textwidth}{\textbf{Donsker's Theorem, Theorem 43, p.335, Section 15.6.3.} Let \(a \in [0, 1)\), \(t \in [0, T]\), and suppose \((J - 1)/T \leq a < J/T, J = 1, 2, \ldots, T\). Define

\[
R_T(a) = \frac{1}{\sqrt{T}} s_{ \big[Ta \big] }
\]

where

\[
s_{ \big[Ta \big] } = \epsilon_1 + \epsilon_2 + \ldots + \epsilon_{ \big[Ta \big] }
\]

\(\big[Ta \big]\) denotes the largest integer part of \(Ta\) and \(s_{ \big[Ta \big] } = 0\) if \(\big[Ta \big] = 0\). Then \(R_T(a)\) weakly converges to \(w(a)\), i.e., 

\[
R_T(a) \to w(a)
\]

where \(w(a)\) is a Wiener process. Note that when \(a = 1\), \(R_T(1) = 1/\sqrt{T} \cdot S_{\big[T \big]} = 1/\sqrt{T} \cdot (\epsilon_1 + \epsilon_2 + \ldots + \epsilon_T\). Since \(\epsilon_t\)'s are IID, by the central limit theorem, \(R_T(1) \to \mathcal{N}(0, 1)\). 

}
}

\noindent\fbox{
\parbox{\textwidth}{\textbf{Continuous Mapping Theorem (Theorem 44 of Pesaran in 15.6.3).} Let \(a \in [0, 1)\), \(i \in [0, n]\), and suppose \((J-1)/n \leq a < J/n, J = 1, 2, \ldots, n\). Define \(R_n(a) = n^{-1/2} S_{\big[ n \cdot a \big] }\). If \(f(\cdot)\) is continuous over \([0, 1)\), then 

\[
f[R_n(a)] \xrightarrow{d} f[w(a)]
\]
}
}


% 3d
\item We have \(T^{-1/2} \sum_{t=1}^T u_t =\sigma \cdot  R_T(1) \implies \sigma \cdot W(1)\) by Donsker's Theorem. Using that and the result from (c), we have

\[
T^{-3/2} \sum_{t=1}^{T-1} t u_t = T^{-1/2} \sum_{t=1}^{T-1} u_t - T^{-3/2}\sum_{t=1}^T y_{t-1} \xrightarrow{d} \sigma \cdot W(1) - \sigma \int_0^1 W(r) dr
\]


\end{enumerate}

%
%
%
%
%
%
%%%%%% 
%%%%% Chapter 17: Introduction to Forecasting

\section{Chapter 17: Introduction to Forecasting}

\textbf{Feel pretty good on concepts}

% 17.7: Iterated and direct multi-step AR methods
\subsection{17.7: Iterated and direct multi-step AR methods}

Suppose \(y_t\) follows the AR(1) model: 

\begin{equation} \label{eqn:17.7ar}
y_t = a + \phi y_{t-1} + \epsilon_t, \ \ \ \ |\phi| < 1, \epsilon_t \sim iid(0, \sigma_\epsilon^2)
\end{equation}

\[
\iff y_t =  \frac{a}{1 - \phi} + \sum_{i=0}^\infty \phi^i \epsilon_{t-i}
\]

\begin{equation} \label{eqn:17.7ar.it}
\iff y_t = a \bigg( \frac{1 - \phi^h}{1 - \phi} \bigg) + \phi^h y_{t-h} + \sum_{j=0}^{h-1} \phi^j \epsilon_{t-j}
\end{equation}

We have two methods for forecasting \(y_{t+h}\) \(h >1\) steps ahead.

\begin{enumerate}[(1)]

\item \textbf{Iterated method:} In this method, we first calculate the OLS estimates of \(\hat{a}_T\) and \(\hat{\phi}_T\) in Equation (\ref{eqn:17.7ar}) using all available data \(\Omega_T\). Then we use the form of Equation (\ref{eqn:17.7ar.it}):

\[
\hat{y}_{T+h \mid T}^* = \hat{a}_T \bigg( \frac{1 - \hat{\phi}_T^h}{1 - \hat{\phi}_T} \bigg) + \hat{\phi}_T^h y_{T} 
\] 

\item \textbf{Direct method:} We directly calculate OLS estimates of the parameters in Equation (\ref{eqn:17.7ar.it}) using all available data \(\Omega_T\):

\[
\tilde{y}_{T+h \mid T}^* = \tilde{a}_{h,T} + \tilde{\phi}_{h,T} y_{T} 
\] 

\end{enumerate}

\begin{proposition}\textbf{(Pesaran Chapter 17 Proposition 45.)} Suppose data is generated by Equation (\ref{eqn:17.7ar}). If \(u_t = \sum_{i=0}^\infty \phi^i \epsilon_{t-i}\) and \(v_t = \sum_{j=0}^{h-1} \phi^j \epsilon_{t-j}\) are symmetrically distributed around zero and have finite second moments, and if \(\E(\hat{\phi}_T\) and \(\E(\tilde{\phi}_{h,T} )\) exist, then for any finite \(T\) and \(h\) we have

\[
\E(\hat{y}_{T+h \mid T}^* - y_{T +h}) = \E (\tilde{y}_{T+h \mid T}^* - y_{T+h}) = 0
\]
\end{proposition}

\subsection{Worked Problems}

\textbf{Problem 4 (Homework 5 Question 3---fine on all but part (c), which even Lidan is hazy on.}

Consider the AR(1) model

\begin{equation} \label{eqn:17.7ar}
y_t =  \phi y_{t-1} + u_t, \ \ \ \ u_t \sim iid(0, \sigma^2)
\end{equation}

\begin{enumerate}[(a)]

\item Derive iterated and direct forecasts of \(y_{T+2}\) condition on \(y_T\), and show that they can be estimated as 

\[
\text{Iterated: } \hat{y}_{T+2\mid T}^{(it)} = \hat{\phi}^2 y_T
\]

\[
\text{Direct: } \hat{y}_{T+2\mid T}^{(d)} = \hat{\phi}_2 y_T
\]

where \(\hat{\phi}\) and \( \hat{\phi}_2\) are OLS coefficients in the regressions of \(y_t\) on \(y_{t-1}\) and \(y_{t-2}\), respectively, using the \(M\) observations \(y_{T-M+1}, y_{T-M+2}, \ldots, y_{T}\).

\item Show that conditional on \(y_t\),

\[
\E(y_{T+2} - \hat{y}_{T+2\mid T}^{(it)})^2 = \E(\phi^2 - \hat{\phi}^2)^2 y_T^2 + (1 + \phi^2)\sigma^2
\]

\[
\E(y_{T+2} - \hat{y}_{T+2\mid T}^{(d)})^2 = \E(\phi^2 - \hat{\phi}_2)^2 y_T^2 + (1 + \phi^2)\sigma^2
\]

\item Hence, or otherwise, show that

\[
\lim_{M \to \infty} \E(d_{T+2}) = 0
\]

where \(d_{T+2}\) is the loss differential of the two forecasting methods, defined by

\[
d_{T+2} = (y_{T+2}- \hat{y}_{T+2\mid T}^{(it)})^2 + (y_{T+2} - \hat{y}_{T+2\mid T}^{(d)})^2
\]

\end{enumerate}

\textbf{Solution.} 

\begin{enumerate}[(a)]

% 3a
\item Note that we can write

\[
y_t =  \phi \big[\phi y_{t-2} + u_{t-1} \big] + u_t =  \phi^2y_{t-2} + u_t + \phi u_{t-1}
\]

\[
\implies y_{T+2} =  \phi^2y_{T} + u_{T+2} + \phi u_{T+1}
\]

\[
\implies \E(y_{T+2} \mid y_{T - M + 1}, y_{T-M + 2}, \ldots, y_T) =  \phi^2y_{T} +  \E(u_{T+2} \mid y_{T - M + 1}, \ldots, y_T) + \phi  \E( u_{T+1} \mid y_{T - M + 1}, \ldots, y_T)
\]

\[
 = \phi^2y_{T}
\]

\begin{equation} \label{eqn:17.7ar.2}
\iff  \E(y_{T+2} \mid y_{T - M + 1}, y_{T-M + 2}, \ldots, y_T) = \phi^2y_{T}
\end{equation}

If we calculate the OLS estimate \(\hat{\phi}\) in Equation (\ref{eqn:17.7ar}), we can substitute that into Equation (\ref{eqn:17.7ar.2}) to obtain the iterated estimate of \(y_{T+2}\):

\[
\hat{y}_{T+2\mid T}^{(it)} = \hat{\phi}^2y_{T}
\]

Since we have no intercept term, the OLS estimate would be simply

\[
\hat{\phi} = (x'x)^{-1}x'y = \frac{\sum_{t=T-M+2}^{T} y_t y_{t-1}}{\sum_{t=T-M+1}^{T-1} y_t^2}
\]

Alternatively, we could directly calculate the OLS estimate \(\hat{\phi}_2\) of \(\phi^2\) in Equation (\ref{eqn:17.7ar.2})

\[
\hat{y}_{T+2\mid T}^{(d)} = \hat{\phi}_2y_{T}
\]

The OLS estimate would be simply

\[
\hat{\phi}_2 = (x'x)^{-1}x'y = \frac{\sum_{t=T-M+3}^{T} y_t y_{t-2}}{\sum_{t=T-M+1}^{T-2} y_t^2}
\]

% 3b
\item We have

\[
\E(y_{T+2} - \hat{y}_{T+2\mid T}^{(it)})^2 = \E\big(y_{T+2}^2\big) + \E \big((\hat{y}_{T+2\mid T}^{(it)})^2 \big)  -2 \E \big(y_{T+2} \cdot \hat{y}_{T+2\mid T}^{(it)}\big) 
\]

\[
= \E((\phi^2 y_T + u_{T+2} + \phi u_{T+1})^2) + \E \big((\hat{\phi}^2y_{T})^2 \big)  -2 \E \big((\phi^2 y_T + u_{T+2} + \phi u_{T+1}) \cdot \hat{\phi}^2y_{T}\big) 
\]

\[
= \E(\phi^4 y_T^2 + (u_{T+2} + \phi u_{T+1})^2 + 2\phi^2 y_T( u_{T+2} + \phi u_{T+1})) + y_{T}^2 \E \big(\hat{\phi}^2 \big)  -2\phi^2 y_T \E \big(  \hat{\phi}^2y_{T}\big) 
\]

\[
= \E(\phi^4)  y_T^2+ \E(u_{T+2}^2 + \phi^2 u_{T+1}^2 ) + y_{T}^2 \E \big(\hat{\phi}^2 \big)  -2\phi^2 y_T^2 \E \big(  \hat{\phi}^2\big)  = \E(\phi^4)  y_T^2+ (1 + \phi^2)\sigma^2 + y_{T}^2 \E \big(\hat{\phi}^2 \big)  -2\phi^2 y_T^2 \E \big(  \hat{\phi}^2\big) 
\]

\[
= y_T^2 \big[ \E \big(\phi^4   +\hat{\phi}^2 -2  \phi^2 \hat{\phi}^2\big) \big] + (1 + \phi^2)\sigma^2 = \boxed{\E(\phi^2 - \hat{\phi}^2)^2 y_T^2 + (1 + \phi^2) \sigma^2}
\]

%
%
%
%
%
%

\[
\E(y_{T+2} - \hat{y}_{T+2\mid T}^{(d)})^2 = \E\big(y_{T+2}^2\big) + \E \big((\hat{y}_{T+2\mid T}^{(d)})^2 \big)  -2 \E \big(y_{T+2} \cdot \hat{y}_{T+2\mid T}^{(d)}\big) 
\]

\[
= \E((\phi^2 y_T + u_{T+2} + \phi u_{T+1})^2) + \E \big((\hat{\phi}_2y_{T})^2 \big)  -2 \E \big((\phi^2 y_T + u_{T+2} + \phi u_{T+1}) \cdot \hat{\phi}_2y_{T}\big) 
\]

\[
= \E(\phi^4 y_T^2 + (u_{T+2} + \phi u_{T+1})^2 + 2\phi^2 y_T( u_{T+2} + \phi u_{T+1})) + y_{T}^2 \E \big(\hat{\phi}_2 \big)  -2\phi^2 y_T \E \big(  \hat{\phi}_2y_{T}\big) 
\]

\[
= \E(\phi^4)  y_T^2+ \E(u_{T+2}^2 + \phi^2 u_{T+1}^2 ) + y_{T}^2 \E \big(\hat{\phi}_2 \big)  -2\phi^2 y_T^2 \E \big(  \hat{\phi}_2\big)  = \E(\phi^4)  y_T^2+ (1 + \phi^2)\sigma^2 + y_{T}^2 \E \big(\hat{\phi}_2 \big)  -2\phi^2 y_T^2 \E \big(  \hat{\phi}_2\big) 
\]

\[
= y_T^2 \big[ \E \big(\phi^4   +\hat{\phi}_2 -2  \phi^2 \hat{\phi}_2\big) \big] + (1 + \phi^2)\sigma^2 = \boxed{\E(\phi^2 - \hat{\phi}_2)^2 y_T^2 + (1 + \phi^2) \sigma^2}
\]

% 3c
\item Begin by expanding \(\hat{\phi}^2\) in a first order Taylor series about \(\phi\):

\[
\hat{\phi} = \phi + \mathcal{O}_p(M^{-1/2}) \implies \hat{\phi}^2 = \phi^2 + 2 \phi(\hat{\phi} - \phi) + \mathcal{O}_p(M^{-1})
\]

Then

\[
\E\big[ (\hat{\phi}^2 - \phi^2)^2 \mid y_T \big] = \E\big[ \big( 2 \phi(\hat{\phi} - \phi) + \mathcal{O}_p(M^{-1}) \big)^2 \mid y_T \big] = 4 \phi^2 \E\big[ \big( (\hat{\phi} - \phi) + \mathcal{O}_p(M^{-1}) \big)^2 \mid y_T \big] 
\]

\[
= 4 \phi^2 \E\big[ \big( (\hat{\phi} - \phi) + \mathcal{O}_p(M^{-1}) \big)^2 \mid y_T \big] 
\]

\end{enumerate}


%
%
%
%
%
%
%%%%%%
%%%%% Chapter 18: Measurement and Modeling of Volatility

\section{Chapter 18: Measurement and Modeling of Volatility}

\textbf{Maybe review a little, but feel pretty okay on concepts}

GARCH(1, 1) model (Pesaran Equation 18.5):

\begin{equation} \label{ts.eq.18.5}
h_t^2 = \alpha_0 + \alpha_1 \epsilon_{t-1}^2 + \phi_1 h_{t-1}^2, \ \ \ \alpha_0 > 0
\end{equation}

\begin{itemize}
\item This process is unconditionally stationary if \(|\alpha_1 + \phi_1| < 1\). 

\item The unconditional variance exists and is fixed if \(|\alpha_1 + \phi_1| < 1\). 

\item The case where \(\alpha_1 + \phi_1 = 1\) is known as the Integrated GARCH(1,1), or IGARCH(1,1) for short. The RiskMetrics exponentially weighted formulation of \(h_t^2\) for large \(H\) is a special case of the IGARCH(1,1) model where \(\alpha_0\) is set to 0. RiskMetrics formulation avoids the variance non-existence problem by focusing on \(H\) fixed.
\end{itemize}

% Higher order GARCH models (Pesaran Section 18.4.2)
\subsection{Higher order GARCH models (Pesaran Section 18.4.2)} The various members of the GARCH and GARCH-M class of models can be written compactly as

\begin{equation} \label{ts.eq.18.6}
y_t = \boldsymbol{\beta}' \boldsymbol{x}_{t-1} + \gamma h_t^2 + \epsilon_t
\end{equation}

where

\begin{equation} \label{ts.eq.18.7}
h_t^2 = \Var(\epsilon_t \mid \Omega_{t-1}) = \E(\epsilon_t^2 \mid \Omega_{t-1}) = \alpha_0 + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2 + \sum_{i=1}^p \phi_i h_{t-i}^2
\end{equation}

and \(\Omega_{t-1}\) is the information set at time \(t - 1\) containing at least \((\boldsymbol{x}_{t-1}, \boldsymbol{x}_{t-2}, \ldots, y_{t-1}, y_{t-2}, \ldots)\). The unconditional variance of \(\epsilon_t\) is determined by 

\[
\sigma_t^2 = \alpha_0 + \sum_{i=1}^q \alpha_i \sigma_{t-i}^2 + \sum_{i=1}^p \phi_i \sigma_{t-i}^2
\]

and yields a stationary outcome if all the roots of

\[
1 - \sum_{i=1}^q \alpha_i \lambda^i + \sum_{i=1}^p \phi_i \lambda^i = 0
\]

like outside the unit circle. In that case

\begin{equation} \label{ts.eq.18.8}
\Var(\epsilon_t) = \sigma^2 = \frac{\alpha_0}{1 - \sum_{i=1}^q \alpha_i - \sum_{i=1}^p \phi_i } > 0
\end{equation}

Clearly the necessary condition for (\ref{ts.eq.18.7}) to be covariance stationary is given by

\[
\sum_{i=1}^q \alpha_i + \sum_{i=1}^p \phi_i < 1
\]

% Testing for GARCH effects (Pesaran Section 18.5.1)
\subsection{Testing for GARCH effects (Pesaran Section 18.5.1)} \label{ts.pesaran.18.5.1}

If we consider \(y_t\) as periodic data defined by \(y_t = r_t - \overline{r}\) with \(r_t\) representing, say, asset return and \(\overline{r}\) representing the unconditional mean, then we have the GARCH(1,1) representation of volatility:

\[
\Var(y_t \mid \Omega_{t-1}) = h_t^2 =  \overline{\sigma}^2(1 - \alpha - \beta) + \alpha y_{t-1}^2 + \beta h_{t-1}^2
\]

Then the test for GARCH effects would test

\[
H_0:  \alpha = 0
\]

against 

\[
H_1: \alpha \neq 0 
\]

GARCH(1,1) can be approximated by ARCH(\(q\)):

\[
\Var(y_t \mid \Omega_{t-1}) = \frac{\overline{\sigma}^2(1 - \alpha - \beta)}{1 - \beta} + \alpha y_{t-1}^2 + \alpha \beta y_{t-2}^2 + \ldots + \alpha \beta^{q-1} y_{t-q}^2
\]

\[
= \tilde{\alpha}_0 + \tilde{\alpha}_1 y_{t-1}^2 + \tilde{\alpha}_2 z_{t-2}^2 + \ldots + \tilde{\alpha}_q z_{t-q}^2
\]

which means that we can approximate this hypothesis test by instead using the Lagrange multiplier test proposed by Engle:

\[
H_0: \tilde{\alpha}_1 = \tilde{\alpha}_2 = \ldots = \tilde{\alpha}_q
\]

against

\[
H_1: \tilde{\alpha}_1 \neq 0, \tilde{\alpha}_2 \neq 0, \ldots, \tilde{\alpha}_q \neq 0
\]


\subsection{Worked Problems}


%%%%%%%%%% Chapter 18 Problem 1
\textbf{Problem 1.} Consider the generalized autoregressive heteroskedastic model 

\[
y_t = h_t z_t
\]

where

\begin{equation}\label{ts.hw6.eq1.1}
z_t \mid \Omega_{t-1} \sim IID\mathcal{N}(0,1)
\end{equation}

\begin{equation}\label{ts.hw6.eq1.2}
h_t^2 = \Var(y_t \mid \Omega_{t-1}) = \E(y_t^2 \mid \Omega_{t-1}) = \overline{\sigma}^2(1 - \alpha - \beta) + \alpha y_{t-1}^2 + \beta h_{t-1}^2 
\end{equation}

and \(\Omega_t\) is the information set that contains at least \(y_t\) and its lagged values.

\begin{enumerate}[(a)]

\item Derive the conditions under which \(\{y_t\}\) is a stationary process.

\item Are the observations \(\{y_t\}\) serially independent and/or serially uncorrelated?

\item Develop a test of the GARCH effect and discuss the estimation of the above model by the maximum likelihood method.

\item Discuss the relevance of GARCH models for the analysis of financial time series data.

\end{enumerate}

%%%%%%% Chapter 18 Problem 1 Solution
\textbf{Solution.} \begin{enumerate}[(a)]

% 1a
\item

Note that (using the fact that \(y_t\) and \(h_t\) are conditionally independent given \(\Omega_{t-1}\))

\[
\E(y_t) = \E(h_t z_t) = \E \big[ \E(h_t z_t \mid \Omega_{t-1}) \big] = \E \big[ \E(h_t \mid \Omega_{t-1}) \E(z_t \mid \Omega_{t-1}) \big] = \E \big[ \E(h_t \mid \Omega_{t-1})\cdot 0 \big] = 0 
\]

We have

\[
\Var(y_t) = \E\big[ \big(y_t - \E(y_t)\big)^2\big] = \E\big[y_t^2\big] = \E\big[ \E(y_t^2 \mid \Omega_{t-1})\big] = \E(h_t^2) = \E(\overline{\sigma}^2(1 - \alpha - \beta) + \alpha y_{t-1}^2 + \beta h_{t-1}^2 )
\]

\[
= \E\big(\overline{\sigma}^2(1 - \alpha - \beta) \big) + \alpha  \E(y_{t-1}^2 )+ \beta \E( h_{t-1}^2 ) = \overline{\sigma}^2(1 - \alpha - \beta) +( \alpha + \beta) \Var( y_{t-1})
\]

Therefore in order for this to be a stationary process, we require \( \boxed{|\alpha + \beta| <1}\).

\

\textbf{more stuff I did on original homework}

By Definition \ref{tf.def.cov.stationary} \(\{y_t\}\) is a stationary process if it has constant mean and its covariance function depends only on the absolute difference \(|t_1 - t_2|\); that is,

\[
\Cov(y_{t_1}, y_{t_2}) = \gamma(t_1, t_2) = \gamma(|t_1 - t_2|) \text{ for all } t_1, t_2
\]

\[
\vdots
\]

In order for this to be true, we must first show that \(h_t\) is finite for all \(t\). It is sufficient to find the conditions that make \(h_t\) stationary. Using Equation (\ref{ts.eq.18.7})

\[
h_t^2 =\alpha_0 + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2 + \sum_{i=1}^p \phi_i h_{t-i}^2
\]

From equation (\ref{ts.hw6.eq1.2}) we have

\[
h_t^2 = \overline{\sigma}^2(1 - \alpha - \beta) + \alpha y_{t-1}^2 + \beta h_{t-1}^2
\]

Therefore we note that \(p = q = 1\) and we have a GARCH(1,1) model. From Equation (\ref{ts.eq.18.5}):

\[
h_t^2 = \alpha_0 + \alpha_1 \epsilon_{t-1}^2 + \phi_1 h_{t-1}^2, \ \ \alpha_0 > 0
\]

is unconditionally stationary if \(|\alpha_1 + \phi_1| < 1\). In this case, we require \(\boxed{|\alpha + \beta| < 1}\) for stationarity of \(h_t\).


\[
\vdots
\]

Assume without loss of generality that \(t_2 \geq t_1\). (Note that this implies \(z_{t_2} \indep z_{t_1}, h_{t_2}, h_{t_1} \mid \Omega_{t_2- 1}\).)

\[
\Cov(y_{t_1}, y_{t_2} ) = \E\big[ \big( y_{t_1} - \E(y_{t_1}) \big) \big( y_{t_2} - \E(y_{t_2}) \big) \big] = \E\big[y_{t_1} y_{t_2} \big] = \E\big[h_{t_1}h_{t_2} z_{t_1}z_{t_2} \big]
\]

\[
= \E\big[ \E(h_{t_1}h_{t_2} z_{t_1}z_{t_2} \mid \Omega_{t_2-1} ) \big] = \E\big[ \E(h_{t_1}h_{t_2} z_{t_1} \mid \Omega_{t_2-1} ) \E(z_{t_2} \mid \Omega_{t_2-1}) \big]
\]

\[
= \E\big[ \E(h_{t_1}h_{t_2} z_{t_1} \mid \Omega_{t_2-1} )\cdot 0 \big] = \boxed{0}
\]

Therefore given that \(h_t\) is finite (that is, given \(|\alpha + \beta| < 1\)), we have that \(\E(y_t) = 0\), \(\Cov(y_{t_1}, y_{t_2}) = 0\ \ \forall \ t_1, t_2\) which implies that under these conditions \(y_t\) is stationary.

% 1b
\item 

\(y_t\) is serially uncorrelated because \(\Cov(y_{t_1}, y_{t_2} )= 0 \ \ \forall \ t_1, t_2\). However, it is clear that \(y_t\) is not serially independent since past values of \(y_t\) affect \(h_t^2 = \Var(y_t)\). Observe that

\[
\Pr(y_t \leq y \mid y_{t-1}) = \Pr(h_t z_t \leq y \mid y_{t-1}) = \Pr \bigg (z_t \sqrt{\overline{\sigma}^2(1 - \alpha - \beta) + \alpha y_{t-1}^2 + \beta h_{t-1}^2 } \leq y \mid y_{t-1} \bigg)
\]

In other words, even though the mean of \(y_t\) remains constant, because \(y_{t-1}\) affects the variance of \(y_t\), it affects the heaviness of the tails of \(y_t\), changing the probability distribution of \(y_t\). Therefore the conditional cumulative distribution function of \(y_t\) given \(y_{t-1}\) is not equal to the unconditional cdf, so \(y_t\) and \(y_{t-1}\) are not independent.

\

\textbf{Lidan's Explanation:} From (a) we know that \(\{y_t\}\) is serially uncorrelated, but they are not necessarily independent because from Equation (\ref{ts.hw6.eq1.2}) we know that \(h_t\) is not independent from \(h_{t-1}\).

% 1c
\item See section \ref{ts.pesaran.18.5.1}.

% 1d
\item \textbf{Lidan:} Usually financial time series data are fat-tailed. So the series may not look stationary, and therefore the local variance would be clustered in some very low and very high values. To capture this serial correlation and heterogeneity in volatility, we need to use a GARCH model.

\

\textbf{Book:} In financial econometrics, ARCH and GARCH are fundamental tools for analyzing the time-variation of conditional variance. In many applications in finance, the assumption that the conditional variance of the disturbances is constant over time is not valid. GARCH models allow for time variation in volatility, relating (unobserved) volatility to squares of past innovations in price changes. However, this approach only partly overcomes the deficiency of the historical measure and continues to respond very slowly when volatility undergoes rapid changes.

\end{enumerate}

%
%
%
%
%
%
%%%%%%
%%%%% Chapter 21: Vector Autoregressive Models

\section{Chapter 21: Vector Autoregressive Models}

\textbf{Feel ok on concepts.}

\subsection{Worked Problems} \label{ts.ch21.worked.problems}

%%% Ch. 21 Ex. 1 Problem
\textbf{Problem 1.} Consider the bivariate autoregressive model:

\begin{equation}\label{ts.hw6.2.3}
Y_t = \boldsymbol{\Phi}Y_{t-1} + U_t, \ \ \ \ \ U_t \sim IID \mathcal{N}(0, \Sigma)
\end{equation}

where

\[
Y_t = (y_{1t}, y_{2t})', \ Y_{t-1} = (y_{1, t-1}, y_{2, t-1})', \ U_t = (u_{1t}, u_{2t})'
\]

and

\[
\boldsymbol{\Phi} = \begin{bmatrix}\phi_{11} & \phi_{12} \\ \phi_{21} & \phi_{22} \end{bmatrix}, \ \ \Sigma = \begin{bmatrix} \sigma_{11} & \sigma_{12} \\ \sigma_{12} & \sigma_{22} \end{bmatrix} \succ 0
\]


%where \(Y_t = (y_{1t}, y_{2t})'\), \(U_t = (u_{1t}, u_{2t})'\), and 
%
%\[
%\Phi = \begin{bmatrix}\phi_{11} & \phi_{12} \\ \phi_{21} & \phi_{22} \end{bmatrix}, \ \ \ \ \Sigma = \begin{bmatrix}\sigma_{11} & \sigma_{12} \\ \sigma_{12} & \sigma_{22} \end{bmatrix}
%\]

\begin{enumerate}[(a)]

\item Derive the conditional mean and variance of \(y_{1t}\) with respect to \(y_{2t}\) and lagged values of \(y_{1t}\) and \(y_{2t}\).

\item Show that the univariate representation of \(y_{1t}\) is an ARMA(2,1) process.

\end{enumerate}

%%%%% Ch. 21 Ex. 1 Solution
\textbf{Solution} 

% Problem 2

\begin{enumerate}[(a)]

% 2a
\item
 Note that 

\begin{equation}\label{ts.hw6.2.y1t}
y_{1t} = \phi_{11} y_{1,t-1} + \phi_{12} y_{2, t-1} + u_{1t}
\end{equation}

\begin{equation}\label{ts.hw6.2.y2t}
y_{2t} = \phi_{21} y_{1,t-1} + \phi_{22} y_{2, t-1} + u_{2t}
\end{equation}

%\begin{itemize}

%\item

First,

\[
\E(y_{1t} \mid y_{2t}, y_{2, t-1}, y_{2, t-2}, \ldots, y_{1, t-1}, y_{1, t-2}, \ldots) = \E(y_{1t} \mid y_{2t}, \Omega_{t-1}) 
\]

\[
= \phi_{11} y_{1,t-1} + \phi_{12} y_{2, t-1} + \E(u_{1t} \mid y_{2t})  =  \phi_{11} y_{1,t-1} + \phi_{12} y_{2, t-1} + \E(u_{1t} \mid \phi_{21} y_{1,t-1} + \phi_{22} y_{2, t-1} + u_{2t})
\]

\[
=  \phi_{11} y_{1,t-1} + \phi_{12} y_{2, t-1} + \E(u_{1t} \mid u_{2t})
\]

Recall Proposition \ref{prob.cond.bivar.norm.dist}: for a bivariate normal distribution with mean 0, the conditional distribution of \(u_{1t}\) given \(u_{2t}\) is

\[
u_{1t} \mid u_{2t} \sim \mathcal{N} \bigg(\rho \frac{\sqrt{\sigma_{11}}}{\sqrt{\sigma_{22}}}u_{2t}, (1 - \rho^2)\sigma_{11} \bigg) 
\]

where \(\rho = \sigma_{12}/\sqrt{\sigma_{11}}\sqrt{\sigma_{22}}\).

\[
\iff u_{1t} \mid u_{2t} \sim \mathcal{N} \bigg(\frac{\sigma_{12}}{\sqrt{\sigma_{11}}\sqrt{\sigma_{22}}} \cdot \frac{\sqrt{\sigma_{11}}}{\sqrt{\sigma_{22}}}u_{2t}, \bigg[1 - \bigg(\frac{\sigma_{12}}{\sqrt{\sigma_{11}}\sqrt{\sigma_{22}}} \bigg) ^2 \bigg]\sigma_{11} \bigg) 
\]

\[
\iff u_{1t} \mid u_{2t} \sim \mathcal{N} \bigg(\frac{\sigma_{12}}{\sigma_{22}} u_{2t}, \sigma_{11} - \frac{\sigma_{12}^2}{\sigma_{22}}  \bigg) 
\]

\[
\implies  \E(u_{1t} \mid u_{2t}) = \frac{\sigma_{12}}{\sigma_{22}} u_{2t}, \ \Var(u_{1t} \mid u_{2t}) = \sigma_{11} - \frac{\sigma_{12}^2}{\sigma_{22}}
\]

\[
\implies \E(y_{1t} \mid y_{2t}, \Omega_{t-1})  =  \phi_{11} y_{1,t-1} + \phi_{12} y_{2, t-1} + \frac{\sigma_{12}}{\sigma_{22}} u_{2t}
\]

\[
=  \phi_{11} y_{1,t-1} + \phi_{12} y_{2, t-1} + \frac{\sigma_{12}}{\sigma_{22}}( y_{2t} - \phi_{21}y_{1,t-1} - \phi_{22} y_{2,t-1})
\]

\[
= \boxed{ \bigg( \phi_{11}  - \frac{\sigma_{12}}{\sigma_{22}}\phi_{21} \bigg) y_{1,t-1} + \bigg(\phi_{12} -   \frac{\sigma_{12}}{\sigma_{22}} \phi_{22} \bigg) y_{2, t-1} + \frac{\sigma_{12}}{\sigma_{22}} y_{2t} }
\]



Second,
%\item 
\[
\Var(y_{1t} \mid y_{2t}, \Omega_{t-1}) = \phi_{11}^2 \Var( y_{1,t-1} \mid y_{2t}, \Omega_{t-1}) + \phi_{12}^2 \Var( y_{2, t-1}\mid y_{2t}, \Omega_{t-1} ) + \Var(u_{1t} \mid y_{2t}, \Omega_{t-1}) 
\]

\[
= \Var(u_{1t} \mid u_{2t})  = \boxed{\sigma_{11} - \frac{\sigma_{12}^2}{\sigma_{22}}}
\]

%\end{itemize}
% 2b
\item

Again, using equations (\ref{ts.hw6.2.y1t}) and (\ref{ts.hw6.2.y2t}),

\[
y_{1t} = \phi_{11} y_{1,t-1} + \phi_{12} y_{2, t-1} + u_{1t}, \ \ y_{2t} = \phi_{21} y_{1,t-1} + \phi_{22} y_{2, t-1} + u_{2t}
\]

\begin{equation}\label{ts.hw6.2b.lag.y1t}
\implies (1 - \phi_{11}L)y_{1t} = \phi_{12}  L y_{2t} + u_{1t}
\end{equation}

\begin{equation}\label{ts.hw6.2b.lag.y2t}
 (1 - \phi_{22}L)y_{2t} = \phi_{21}  L y_{1t} + u_{2t}
 \end{equation}

We can multiply \( (1 - \phi_{22}L)\) on both sides of (\ref{ts.hw6.2b.lag.y1t}) to yield

\[
(1 - \phi_{22}L)(1 - \phi_{11}L)y_{1t} = \phi_{12} L(1 - \phi_{22}L)  y_{2t} + (1 - \phi_{22}L)u_{1t}
\]

Using (\ref{ts.hw6.2b.lag.y2t}), we have

\[
(1 - (\phi_{22} + \phi_{11})L+ \phi_{11}\phi_{22}L^2)y_{1t} = \phi_{12}L \big[  \phi_{21}  L y_{1t} + u_{2t} \big]+ (1 - \phi_{22}L)u_{1t}
\]

\begin{equation}\label{ts.hw6.2b.arma}
\iff (1 - (\phi_{22} + \phi_{11})L+ (\phi_{11}\phi_{22} - \phi_{12}\phi_{21}) L^2)y_{1t} = \phi_{12}L u_{2t} + (1 - \phi_{22}L)u_{1t}
\end{equation}

If we can show that the left side of (\ref{ts.hw6.2b.arma}) is an AR(2) process and the right side is an MA(1) process, we are done. \textbf{Lidan: ``It is obvious that the left hand side of (\ref{ts.hw6.2b.arma}) is an AR(2) process."} The left side is a stationary AR(2) process if the absolute values of all the roots of 

\[
1 - (\phi_{22} + \phi_{11})z+ (\phi_{11}\phi_{22} - \phi_{12}\phi_{21}) z^2 = 0
\]

are greater than 1. The roots are 

\[
z = \frac{\phi_{22} + \phi_{11} \pm \sqrt{(\phi_{22} + \phi_{11})^2 - 4(\phi_{11}\phi_{22} - \phi_{12}\phi_{21})}}{2(\phi_{11}\phi_{22} - \phi_{12}\phi_{21})} = \frac{\phi_{22} + \phi_{11} \pm \sqrt{\phi_{22}^2 + \phi_{11}^2 - 2 \phi_{22}\phi_{11}+ 4\phi_{12}\phi_{21}}}{2(\phi_{11}\phi_{22} - \phi_{12}\phi_{21})}
\]

\[
= \frac{\phi_{22} + \phi_{11} \pm \sqrt{(\phi_{22} - \phi_{11})^2 + 4\phi_{12}\phi_{21}}}{2(\phi_{11}\phi_{22} - \phi_{12}\phi_{21})}
\]

\[
\vdots
\]

To check if the right side of (\ref{ts.hw6.2b.arma}) is MA(1), we will write \(x_t = \phi_{12}L u_{2t} + (1 - \phi_{22}L)u_{1t}\) as an MA(1) process; that is,

\[
x_t = \phi_{12} u_{2,t-1} +u_{1t}   - \phi_{22}u_{1,t-1} = \xi_t + \theta \xi_{t-1}
\]

with \(\xi_t \sim iid(0, \sigma_{\xi}^2)\), \(|\theta| < 1\). If \(x_t\) is an MA(1) process, it must satisfy

\[
\gamma(0) = \E(x_t^2) = (1 + \theta^2)\sigma_{\xi}^2, \ \ \ \gamma(1) = \E(x_t x_{t-1}) = \theta \sigma_{\xi}^2 
\]

We have (using the serial independence of the \(u_{1t}, u_{2t}\))

%\begin{itemize}

%\item

\[
\E(x_t^2) = \E\big( \big[\phi_{12} u_{2,t-1} +u_{1t}   - \phi_{22}u_{1,t-1} \big]^2 \big)  = \E\big(\phi_{12}^2 u_{2,t-1}^2 -2 \phi_{12}\phi_{22} u_{2,t-1}u_{1,t-1} +  \phi_{22}^2u_{1,t-1} ^2  + u_{1t}^2\big) 
\]

\[
= \phi_{12}^2 \E\big(u_{2,t-1}^2\big) -2 \phi_{12}\phi_{22} \E \big(u_{2,t-1}u_{1,t-1}\big)  +  \phi_{22}^2\E \big(u_{1,t-1} ^2  \big) + \E \big(u_{1t}^2 ) 
\]


\[
= \phi_{12}^2 \sigma_{22} - 2 \phi_{12} \phi_{22} \sigma_{12}  + \phi_{22}^2 \sigma_{11} +\sigma_{11}  = ( 1 + \phi_{22}^2 )\sigma_{11}  - 2 \phi_{12} \phi_{22} \sigma_{12} +  \phi_{12}^2 \sigma_{22} 
\]

%\item

\[
\E(x_t x_{t-1})= \E\big( \big[\phi_{12} u_{2,t-1} +u_{1t}   - \phi_{22}u_{1,t-1} \big]\big[\phi_{12} u_{2,t-2} +u_{1,t-1}   - \phi_{22}u_{1,t-2} \big]  \big) 
\]

\[
= \E\big( \phi_{12} u_{2,t-1}u_{1,t-1}  - \phi_{22}u_{1,t-1} ^2 \big) = \phi_{12}\sigma_{12} - \phi_{22} \sigma_{11}
\]

Therefore we require

\begin{equation}\label{ts.hw6.2b.req1}
 (1 + \theta^2)\sigma_{\xi}^2 = ( 1 + \phi_{22}^2 )\sigma_{11}  - 2 \phi_{12} \phi_{22} \sigma_{12} +  \phi_{12}^2 \sigma_{22}
\end{equation}

\begin{equation}\label{ts.hw6.2b.req2}
\theta \sigma_{\xi}^2  =  \phi_{12}\sigma_{12} - \phi_{22} \sigma_{11}
\end{equation}

Dividing (\ref{ts.hw6.2b.req1}) by (\ref{ts.hw6.2b.req2}) we have

\begin{equation}\label{ts.hw6.2b.solve.theta}
\frac{ 1 + \theta^2}{\theta} = \frac{( 1 + \phi_{22}^2 )\sigma_{11}  - 2 \phi_{12} \phi_{22} \sigma_{12} +  \phi_{12}^2 \sigma_{22}}{ \phi_{12}\sigma_{12} - \phi_{22} \sigma_{11}}
\end{equation}

For simplicity, let the right side of (\ref{ts.hw6.2b.solve.theta}) be \(A \in \mathbb{R}\). Then we have

\[
\theta^2 - A \theta + 1 = 0 \iff \boxed{\theta_1 = \frac{1}{2} \big( A + \sqrt{A^2 - 4} \big), \theta_2 = \frac{1}{2} \big( A + \sqrt{A^2 - 4} \big)}
\]

Then the corresponding \(\sigma_{\xi}^2\) are

\[
\boxed{\sigma_{\xi, 1}^2  = \sigma_{\xi, 2}^2  =   \frac{\phi_{12}\sigma_{12} - \phi_{22} \sigma_{11}}{\theta_1}}
\]

%\end{itemize}

As a double check,

\[
\E(x_t x_{t-2}) = \E\big( \big[\phi_{12} u_{2,t-1} +u_{1t}   - \phi_{22}u_{1,t-1} \big]\big[\phi_{12} u_{2,t-3} +u_{1,t-2}   - \phi_{22}u_{1,t-3} \big]  \big) = 0
\]

as expected for an MA(1) process. Therefore the right side of (\ref{ts.hw6.2b.arma}) is an MA(1) process (provided that \(0 < |\theta_1| < 1\) and/or \(0 < |\theta_2| < 1\)). Since the left side is an AR(2) process, this proves that the univariate representation (\ref{ts.hw6.2b.arma}) of \(y_{1t}\) is an ARMA(2,1) process.

\end{enumerate}



%%%% Ch. 21 Problem 2
\textbf{Problem 2.} Consider the VAR(2) model in the \(m\)-dimensional vector \(Y_t\):

\begin{equation}\label{ts.hw6.3.10}
Y_t = \mu + \Phi_1 Y_{t-1} + \Phi_2 Y_{t-2} + U_t, \ \ \ U_t \sim(0, \Sigma)
\end{equation}

where \(\mu\) is an \(m\)-vector of fixed constants.

\begin{enumerate}[(a)]

\item Derive the conditions under which the VAR(2) model defined in (\ref{ts.hw6.3.10}) is stationary.

\item Derive the error correction form of (\ref{ts.hw6.3.10}) and discuss what is meant by the process \(Y_t\) being cointegrated.

\item Suppose now that one or more elements of \(Y_t\) is I(1). Derive suitable restrictions on the intercepts \(\mu\) such that despite the I(1) nature of the variables in (\ref{ts.hw6.3.10}), \(Y_t\) has a fixed mean. Discuss the importance of such restrictions for the analysis of cointegration.

\end{enumerate}

%%%% Ch. 21 Problem 2 Solution
\textbf{Solution.} 

\begin{enumerate}[(a)]

% 3a
\item 

%\textbf{section 21.2.2 (pp. 508 - 509); Chapter 21 exercise 2 and Chapter 22 exercise 2(a)}

Let 

\begin{equation}\label{ts.hw6.3a.def.ystar}
Y_t^* = Y_t - (I - \Phi_1 - \Phi_2)^{-1}\mu.
\end{equation}

Then

\[
Y_t^* = \mu + \Phi_1 Y_{t-1} + \Phi_2 Y_{t-2} + U_t - (I - \Phi_1 - \Phi_2)^{-1}\mu 
\]

\[
= \mu + \Phi_1 Y_{t-1} + \Phi_2 Y_{t-2} + U_t - [(I - \Phi_1 - \Phi_2) + \Phi_1 + \Phi_2](I - \Phi_1 - \Phi_2)^{-1}\mu
\]

\[
= \mu  - (I - \Phi_1 - \Phi_2)(I - \Phi_1 - \Phi_2)^{-1} \mu + \Phi_1( Y_{t-1} - (I - \Phi_1 - \Phi_2)^{-1}\mu) + \Phi_2 (Y_{t-2} - (I - \Phi_1 - \Phi_2)^{-1}\mu) + U_t 
\]

\begin{equation}\label{ts.hw6.3a.varp}
= \Phi_1Y_{t-1}^* + \Phi_2 Y_{t-2}^* + U_t 
\end{equation}

Equation (\ref{ts.hw6.3a.varp}) can be rewritten in the companion form as follows:

\begin{equation}\label{ts.hw6.3a.vde}
\begin{bmatrix} Y_{t}^* \\ Y_{t-1}^* \end{bmatrix} = \begin{bmatrix}\Phi_1 & \Phi_2 \\ I & 0 \end{bmatrix}\begin{bmatrix} Y_{t-1}^* \\ Y_{t-2}^* \end{bmatrix} + \begin{bmatrix}U_t \\0 \end{bmatrix} 
\end{equation}

which is a VAR(1) model. If \(Y_{-M+1}, Y_{-M+2}\) are given, equation (\ref{ts.hw6.3a.vde}) can be solved iteratively from \(t = -M + 2\) to obtain

\begin{equation}\label{ts.hw6.3a.sol}
\begin{bmatrix}Y_{t}^* \\ Y_{t-1}^* \end{bmatrix} = \begin{bmatrix}\Phi_1 & \Phi_2 \\ I & 0 \end{bmatrix}^{t+M-2}\begin{bmatrix} Y_{-M+2}^* \\ Y_{-M+1}^* \end{bmatrix} + \sum_{j=0}^{t+M-3} \begin{bmatrix}\Phi_1 & \Phi_2 \\ I & 0 \end{bmatrix}^{j}\begin{bmatrix}U_{t-j} \\0 \end{bmatrix} 
\end{equation}

Then the condition for (\ref{ts.hw6.3a.sol}) to be covariance stationary is for all the eigenvalues of 

\[
\Phi = \begin{bmatrix}\Phi_1 & \Phi_2 \\ I & 0 \end{bmatrix}
\]

to lie inside the unit circle; that is, the solutions of

\[
|\Phi - \lambda I_4| = 0
\]

must satisfy \(|\lambda| < 1\). Equivalently, the stability condition can be written in terms of the roots of the determinantal equation

\[
|I_2 - \Phi_1z - \Phi_2z^2| = 0
\]

in which case the process \(Y_{t}^*\) will be stationary if all the roots lie outside the unit circle (\(|z| > 1\)). Then if \(Y_{t}^*\) is stationary, so is \(Y_t\), so either of these conditions (plus the invertibility of \((I - \Phi_1 - \Phi_2)\) ) ensure stationarity of \(Y_t\). 

% 3b
\item 

%\textbf{Chapter 22 exercise 2(c): 22.4 (p. 529 - 530); possibly 22.8 (pp. 536 - 538), 22.14 (pp. 550 - 552)}

From (\ref{ts.hw6.3a.varp}) we have

\[
Y_t^*  = \Phi_1Y_{t-1}^* + \Phi_2 Y_{t-2}^* + U_t 
\]

Note that (letting \(\Delta Y_t^* = Y_t^* - Y_{t-1}^*\))

\[
Y_t^* - Y_{t-1}^* + Y_{t-1}^* = \Phi_1Y_{t-1}^* + \Phi_2 \big(Y_{t-2}^* - Y_{t-1}^* + Y_{t-1}^* \big) + U_t
\]

\[
\iff \Delta Y_t^* + Y_{t-1}^* =  \Phi_1Y_{t-1}^* + \Phi_2 \big(Y_{t-1}^* - \Delta Y_{t-1}^* \big) + U_t
\]

\[
\iff \Delta Y_t^*  = -(I -   \Phi_1 - \Phi_2 ) Y_{t-1}^* - \Phi_2 \Delta Y_{t-1}^* + U_t \iff \boxed{\Delta Y_t^*  = - \Pi Y_{t-1}^* + \Gamma \Delta Y_{t-1}^* + U_t}
\]

where \(\Pi = (I -   \Phi_1 - \Phi_2 )\) and \(\Gamma = -\sum_{i=2}^2 \Phi_i = -\Phi_2\). Because 

\[
\Delta Y_t^* = Y_t^* - Y_{t-1}^* = Y_t - (I - \Phi_1 - \Phi_2)^{-1}\mu - \big[Y_{t-1} - (I - \Phi_1 - \Phi_2)^{-1}\mu\big] = Y_t - Y_{t-1} = \Delta Y_t,
\]

this can also be written as

\[
\Delta Y_t = - \Pi \big[Y_{t-1} - (I - \Phi_1 - \Phi_2)^{-1}\mu  \big] + \Gamma \Delta Y_{t-1} + U_t
\]

\[
= - \Pi \big[Y_{t-1} -\Pi^{-1}\mu  \big] + \Gamma \Delta Y_{t-1} + U_t   = \Pi \cdot \Pi^{-1}\mu - \Pi Y_{t-1} + \Gamma \Delta Y_{t-1} + U_t
\]

\begin{equation}\label{ts.hw6.3b.vecm}
\implies \boxed{
\Delta Y_t = \mu - \Pi Y_{t-1} + \Gamma \Delta Y_{t-1} + U_t}
\end{equation}

For the definition of cointegration, see Definition \ref{ts.ch22.def.cointegrated} and Section \ref{ts.pesaran.22.4}. In this particular case, if \(Y_{t-1}^* \sim I(1)\) and the linear combinations \(\Pi Y_{t-1}^*\) of \(Y_{t-1}^*\) are covariance stationary (that is, \(\Pi Y_{t-1}^* \sim I(0)\)), we say \(Y_{t}^*\) is cointegrated (and therefore so is \(Y_t\)).

%\[
%Y_t = \mu + \Phi_1Y_{t-1} + \Phi_2 Y_{t-2} + U_t, \ \ U_t \sim (0, \Sigma)
%\]
%
%Note that (letting \(\Delta Y_t = Y_t - Y_{t-1}\))
%
%\[
%Y_t - Y_{t-1} + Y_{t-1}= \mu + \Phi_1Y_{t-1} + \Phi_2 \big(Y_{t-2} - Y_{t-1} + Y_{t-1} \big) + U_t
%\]
%
%\[
%\iff \Delta Y_t + Y_{t-1}= \mu + \Phi_1Y_{t-1} + \Phi_2 \big(Y_{t-1} - \Delta Y_{t-1} \big) + U_t
%\]
%
%\[
%\iff \Delta Y_t + Y_{t-1}= \mu + \Phi_1Y_{t-1} + \Phi_2 \big(Y_{t-1} - \Delta Y_{t-1} \big) + U_t
%\]

% 3c
\item 

%\textbf{Chapter 22 exercise 2(b): possibly 22.15 (pp. 552 - 556),  22.5 (particularly example 54)}

Since (\ref{ts.hw6.3b.vecm}) is I(0), for \(Y_t\) to have fixed mean, take expectations on both sides of (\ref{ts.hw6.3b.vecm}):

\[
\mu - (I_m - \Phi_1 - \Phi_2) \E(Y_{t-1}) = 0
\]

If this restriction is violated, then (\ref{ts.hw6.3b.vecm}) becomes a stationary process with a drift, which implies (\ref{ts.hw6.3.10}) has a time trend.

\end{enumerate}

%
%
%
%
%
%
%%%%%%
%%%%% Chapter 22

\section{Chapter 22: Cointegration Analysis}

\textbf{Feel pretty good except for long run effects, examples we went over in class, the restrictions, and the 5 cases. Don't need to understand SURE.}

\subsection{22.4 Cointegrating VAR: multiple cointegrating relations and 22.5: Identification of long-run effects}\label{ts.pesaran.22.4}
\begin{definition}\label{ts.ch22.def.cointegrated} We say that the \(m\) variables in \(Y_t\) are \textit{cointegrated} if they are individually integrated (or have a random walk component) but there exist linear combinations of them which are stationary. That is, \(y_{it} \sim I(1) \ \text{ for } i = 1, 2, \ldots, m\), but there exists an \(m \times r\) matrix \(\beta\) such that \(\beta'Y_t = \xi_t \sim I(0)\). 
\end{definition}

\begin{itemize}

\item In this case \(r\) denotes the number of cointegrating vectors, also known as the dimension of the cointegration space. 

\item The cointegrating relations summarized in the \(r \times 1\) vector \(\beta'Y_t \) are also known as long-run relations.

\item \(r = \textbf{rank}(\Pi)\) is the dimension of the cointegration space. 

\item Cointegration is present if \(\Pi\) is rank-deficient; that is, \(r < m\).

\end{itemize}

When \(\textbf{rank}(\Pi) = r < m\), we can write \(\Pi\) as 

\begin{equation}\label{ts.eq.22.18}
\Pi = \alpha \beta'
\end{equation}

where \(\alpha\) and \(\beta\) are \(m \times r\) matrices of full column rank. Then

\[
\Pi y_{t-1} = \alpha \beta' y_{t-1} \sim I(0)
\]

and the VECM can be written as

\begin{equation}\label{ts.eq.22.19}
\Delta y_{t} = -\alpha \beta' y_{t-1} + \sum_{j=1}^{p-1} \Gamma_j \Delta y_{t-j} + u_t
\end{equation}

Since \(\alpha\) is full rank, we have

\[
\beta' y_{t-1} \sim I(0)
\]

where \(\beta'y_t\) is the \(r\)-vector of cointegrating relations, also known as the long-run relations.

\

However, \(\beta\) as defined above is not uniquely determined. Consider a linear transformation of \(\beta\) by a nonsingular \(r \times r\) matrix \(Q\): \(\tilde{\beta} = \beta Q\). Then since \(\textbf{rank}(\Pi) = r < m\), \(\Pi\) can be expressed as \(\Pi = \alpha \beta'\) where \(\alpha\) is a rank \(r\) \(m \times r\) matrix. But consider that 

\[
\Pi = \alpha \beta = (\alpha {Q'}^{-1})(Q' \beta') = \tilde{\alpha} \tilde{\beta}'
\]

with \(\tilde{\alpha} = \alpha {Q'}^{-1}\). Therefore \(\beta\) is not uniquely determined without \(r^2\) exact- or just-identifying restrictions, \(r\) restrictions on each of the \(r\) cointegrating relations. 

\subsection{Worked Problems}

\textbf{Problem 2:} See Chapter 21 Problem 2.

%
%
%
%
%
%
%%%%%%
%%%%% Chapter 23

\section{Chapter 23: VARX Modeling}

\textbf{Lidan says will not be on final.}

\subsection{Worked Problems}

%
%
%
%
%
%
%%%%%%
%%%%% Chapter 24

\section{Chapter 24: Impulse Response Analysis}

\textbf{Feel ok on concepts, should review and do an example problem.}

\subsection{Worked Problems}

%%%%%%%%% Chapter 24 Problem 1

\textbf{Chapter 24 Problem 1.} Consider the VAR(2) model

\[
x_t = \Phi_1 x_{t-1} + \Phi_2 x_{t-2} + \epsilon_t, \ \ \ \epsilon_t \sim IID(0, \Sigma)
\]

in the \(m \times 1\) vectof of random variables \(x_t\) and \(\Sigma\) is the covariance matrix of the errors with typical element \(\sigma_{ij}\). 

\begin{enumerate}[(a)]

\item Derive the conditions under which this process is stationary, and show that it has the following moving average representation:

\begin{equation} \label{ts.ch24.p1.eq}
x_t = \sum_{j=0}^\infty A_j \epsilon_{t-j}
\end{equation}

\item Derive the coefficient matrices \(A_j\) in terms of \(\Phi_1\) and \(\Phi_2\).

\item Using the above result, write down the orthogonalized (OIR) and generalize impulse (GIR) response functions of one standard error shock (i.e. \(\sqrt{\sigma_{ii}}\) to the error of the \(i\)th equation, \(\epsilon_{it} = s_i' \epsilon_t\), where \(s_i\) is am \(m \times 1\) selection vector.

\item What are the main differences betwen OIR and GIR functions?

\end{enumerate}


%%%%%%%%% Chapter 24 Problem 1 Solution

\textbf{Chapter 24 Problem 1 Solution.}



\begin{enumerate}[(a)]

% Chapter 24  Problem 1 Part a
\item For stationarity conditions, see Ch. 21 Problem 2 in Section \ref{ts.ch21.worked.problems}. To get the MA representation

\begin{equation}  \label{ts.ch24.p1.ma}
x_t = \sum_{j=0}^\infty A_j \epsilon_{t-j}
\end{equation}

of (\ref{ts.ch24.p1.eq}) note that 

\[
(I - \Phi_1L - \Phi_2 L^2) x_t = \epsilon_t
\]

\[
\implies x_t = (I - \Phi_1L - \Phi_2 L^2)^{-1} \epsilon_t = \sum_{j=0}^\infty A_j \epsilon_{t-j}
\]

for some \(\{A_j\}\).

%\[
%\vdots
%\]
%
%\[
%\begin{bmatrix}y_t \\ y_{t-1} \end{bmatrix} = \begin{bmatrix} \Phi_1 & \Phi_2 \\ I & 0 \end{bmatrix} \begin{bmatrix}y_{t-1} \\ y_{t-2} \end{bmatrix} + \begin{bmatrix} \epsilon_t \\ 0 \end{bmatrix}
%\]
%
%\[
%Y_t = \Phi Y_{t-1} + U_t
%\]
%
%\[
%\implies (I - \Phi L) Y_t = U_t
%\]
%
%\[
%Y_t = (I - \Phi L)^{-1} U_t
%\]
%
%\[
%\vdots
%\]
%
%\[
%(I - \Phi L)^{-1} = \cdots \begin{bmatrix} \sum_{j=0}^\infty A_j  \\ 0 \end{bmatrix}
%\]
%
%\[
%\vdots
%\]
%
%\[
%A_j = \Phi_1 A_{j-1} + \Phi_2 A_{j-2}
%\]
%
%\[
%\implies x_t =  \sum_{j=0}^\infty (\Phi_1 A_{j-1} + \Phi_2 A_{j-2}) \epsilon_{t-j}
%\]

% Chapter 24  Problem 1 Part b
\item Now we seek to evaluate \((I - \Phi_1L - \Phi_2 L^2)^{-1}   \). To do this, let \((I - \Phi_1L - \Phi_2 L^2)^{-1} = A_0 + A_1L + A_2L^2  + A_3 L^3 + \ldots \) and note that

\[
(I - \Phi_1 L - \Phi_2 L^2)(I - \Phi_1L - \Phi_2 L)^{-1} = I
\]

\[
 \iff  (I - \Phi_1 L - \Phi_2 L^2)( A_0 + A_1L + A_2L^2 + A_3 L^3 + \ldots + A_j L^j + \ldots ) = I
\]

\[
\iff A_0 +( A_1  - \Phi_1) L + (A_2  - \Phi_1 A_1 - \Phi_2)  L^2 + (A_3  - \Phi_1 A_2 - \Phi_2 A_1)L^3 + \ldots 
\]

\[
+ (A_j - \Phi_1 A_{j-1} + \Phi_2 A_{j-2})L^j + \ldots = I
\]

In order for this equation to hold, all the lag terms must equal zero and the constant matrix \(A_0\) must equal \(I\).

\[
\implies \boxed{A_0 = I}
\]

\[
A_1 - \Phi_1 = 0 \iff \boxed{A_1 = \Phi_1}
\]

\[
A_2 -\Phi_1 A_1 - \Phi_2 = 0 \iff A_2 - \Phi_1^2 - \Phi_2 = 0 \iff  \boxed{A_2 = \Phi_1^2 + \Phi_2}
\]

\[
A_3  - \Phi_1 A_2 - \Phi_2 A_1 = 0 \iff A_3 - \Phi_1^3 - \Phi_1 \Phi_2 - \Phi_2 \Phi_1 = 0 \iff \boxed{A_3 = \Phi_1^3 + \Phi_1 \Phi_2 + \Phi_2 \Phi_1}
\]

and, in general,

\[
\boxed{
A_j = \Phi_1 A_{j-1} + \Phi_2 A_{j-2}}
\]

% Chapter 24  Problem 1 Part c
\item

\begin{itemize}

 \item \textbf{OIR:} We employ the Cholesky decomposition of \(\Sigma\):

\begin{equation}\label{ts.eqn.24.5}
\Sigma = P P'
\end{equation}

where \(P\) is a lower-triangular matrix. Then the MA representation (\ref{ts.ch24.p1.ma}) can be written as

\begin{equation}\label{ts.eqn.24.6}
x_t = \sum_{j=0}^\infty (A_j P)(P^{-1}u_{t-j}) = \sum_{j=0}^\infty B_j \eta_{t-j}
\end{equation}

where \(B_j = A_jP\), \(\eta_t = P^{-1}u_t\), so we have

\[
\E(\eta_t \eta_t') = P^{-1} \E(u_t u_t') (P^{-1})' = P^{-1} \Sigma  (P^{-1})'  = P^{-1}P P'  (P^{-1})' = I_m
\]

so the new errors \(\eta_{1t}, \eta_{2t}, \ldots, \eta_{mt}\) are contemporaneously uncorrelated. Then the orthogonalized impact of a unit shock at time \(t\) to the \(i\)th equation on \(y\) at time \(t + n\) is given by

\begin{equation}\label{ts.eqn.24.7}
B_n e_i, n = 0, 1, \ldots
\end{equation}

where \(e_i\) is an \(m \times 1\) selection vector. Written more compactly, the orthogonalized impulse response function of a unit (one standar error) shock to the \(i\)th variable on the \(j\)th variable is given by

\begin{equation}\label{ts.eqn.24.9}
OI_{ij,n} = e_j' A_n P e_i, \ \ \ i, j = 1, 2, \ldots, m
\end{equation}

These orthogonalized impulse responses are not unique and depend on the particular ordering of the variables in the VAR. The orthogonalized responses are invariant to the ordering of the variables only if \(\Sigma\) is diagonal.

 \item \textbf{GIR:} If the VAR model is perturbed by a shock of size \(\delta_i = \sqrt{\sigma_{ii}}\) to its \(i\)th equation at time \(t\), by the definition of the generalized IR function we have

\begin{equation}\label{ts.eqn.24.14}
GI_y(n, \delta_i, \Omega_{t-1}^0) = \E(y_t \mid u_{it} = \delta_i, \Omega_{t-1}^0) - \E(y_t \mid \Omega_{t-1}^0)
\end{equation}

Once again using the MA(\(\infty\)) representation (\ref{ts.ch24.p1.ma} we obtain

\begin{equation}\label{ts.eqn.24.15}
GI_y(n, \delta_i, \Omega_{t-1}^0) =A_n \E(u_t \mid u_{it} = \delta_i)
\end{equation}

which is history invariant (i.e. does not depend on \(\Omega_{t=1}^0\). The computation of the conditional expectations \(\E(u_t \mid u_{it} = \delta_i)\) depends on the nature of the multivariate distribution assumed for the disturbances \(u_t\). In the case where \(u_t \sim IID \mathcal{N}(0, \Sigma)\), we hae

\begin{equation}\label{ts.eqn.24.16}
\E(u_t \mid u_{it} = \delta_i) = \begin{bmatrix} \sigma_{1i}/\sigma_{ii} \\ \sigma_{2i}/\sigma_{ii} \\ \vdots \\ \sigma_{mi}/\sigma_{ii} \end{bmatrix} \delta_i
\end{equation}

where as before \(\Sigma = [\sigma_{ij}]\). Hence for a unit shock \(\delta_i = \sqrt{\sigma_{ii}}\) we have

\begin{equation}\label{ts.eqn.24.17}
GI_y(n, \delta_i  = \sqrt{\sigma_{ii}}, \Omega_{t-1}^0) = \frac{A_n \Sigma e_i}{\sqrt{\sigma_{ii}}}, \ \ i, j = 1, 2, \ldots, m
\end{equation}

The GIRF of a unit shock to the \(i\)th equation in the VAR(\(p\)) model 

\begin{equation}\label{ts.eqn.24.1}
y_t = \Phi_1 y_{t-1} + \Phi_2 y_{t-2} + \ldots + \Phi_p y_{t-p} + u_t, \ \ \ u_t \sim IID(0, \Sigma)
\end{equation}

on the \(j\)th variable at horizon \(n\) is given by the \(j\)th element of (\ref{ts.eqn.24.17}), expressed more compactly by

\begin{equation}\label{ts.eqn.24.18}
GI_y(n, \delta_i  = \sqrt{\sigma_{ii}}, \Omega_{t-1}^0) = \frac{e_j'A_n \Sigma e_i}{\sqrt{\sigma_{ii}}}, \ \ i, j = 1, 2, \ldots, m
\end{equation}

\end{itemize}

% Chapter 24  Problem 1 Part d
\item The GIRF circumvents the problem of the dependence of the orthogonalized impulse responses to the ordering of the variables in the VAR. Unlike the OIR responses in (\ref{ts.eqn.24.9}), the GIR responses in (\ref{ts.eqn.24.18}) are invariant to the ordering of the variables in the VAR. The two responses coincide only for the first variable in the VAR or when \(\Sigma\) is diagonal. 

\end{enumerate}


%%%%%%% Chapter 24 Problem 4

\textbf{Chapter 24 Problem 4.}


%%%%%%%%% Chapter 24 Problem 4 Solution

\textbf{Chapter 24 Problem 4 Solution.}

%
%
%
%
%
%
%%%%%%
%%%%% Chapter 33

\section{Chapter 33: Theory and Practice of GVAR Modeling}

\textbf{Lidan says will not be on final.}



%
%
%
%
%
%
%\bibliographystyle{abbrvnat}
%\bibliography{mybib2fin}
%\end{document}