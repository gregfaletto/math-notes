\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage[document]{ragged2e}
\usepackage{textcomp}
% \usepackage{amssymb}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\: \hmwkTitle}
%\rhead{\firstxmark}
%\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

%\newcommand{\enterProblemHeader}[1]{
%    \nobreak\extramarks{}{Question \arabic{#1} continued on next page\ldots}\nobreak{}
%    \nobreak\extramarks{Question \arabic{#1} (cont.)}{Question \arabic{#1} continued on next page\ldots}\nobreak{}
%}
%
%\newcommand{\exitProblemHeader}[1]{
%    \nobreak\extramarks{Question \arabic{#1} (cont.)}{Question \arabic{#1} continued on next page\ldots}\nobreak{}
%    \stepcounter{#1}
%    \nobreak\extramarks{Question \arabic{#1}}{}\nobreak{}
%}
%
%\setcounter{secnumdepth}{0}
%\newcounter{partCounter}
%\newcounter{homeworkProblemCounter}
%\setcounter{homeworkProblemCounter}{1}
%\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%%
%% Homework Problem Environment
%%
%% This environment takes an optional argument. When given, it will adjust the
%% problem counter. This is useful for when the problems given for your
%% assignment aren't sequential. See the last 3 problems of this template for an
%% example.
%%
%\newenvironment{homeworkProblem}[1][-1]{
%    \ifnum#1>0
%        \setcounter{homeworkProblemCounter}{#1}
%    \fi
%    \section{Problem \arabic{homeworkProblemCounter}}
%    \setcounter{partCounter}{1}
%    \enterProblemHeader{homeworkProblemCounter}
%}{
%    \exitProblemHeader{homeworkProblemCounter}
%}

\theoremstyle{definition}
\newtheorem{exercise}{Exercise}
\newtheorem{definition}{Definition}
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{corollary}{Corollary}[theorem]

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{2017 In-Class Exam}
%\newcommand{\hmwkDueDate}{Apr. 26, 2019}
\newcommand{\hmwkClass}{DSO}
%\newcommand{\hmwkClassTime}{Section A}
%\newcommand{\hmwkClassInstructor}{S. Heilman}
\newcommand{\hmwkAuthorName}{\textbf{G. Faletto} }

%\renewcommand{\subset}{\subseteq}
\renewcommand{\supset}{\supseteq}
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\abs}[1]{\left|#1\right|}                   % Absolute value notation
\newcommand{\absf}[1]{|#1|}                             % small absolute value signs
\newcommand{\vnorm}[1]{\left|\left|#1\right|\right|}    % norm notation
\newcommand{\vnormf}[1]{||#1||}                         % norm notation, forced to be small
\newcommand{\im}[1]{\mbox{im}#1}                        % Pieces of English for math mode
\newcommand{\tr}[1]{\mbox{tr}#1}
\newcommand{\Proj}[1]{\mbox{Proj}#1}
\newcommand{\Vol}[1]{\mbox{Vol}#1}
\newcommand{\Z}{\mathbb{Z}}                             % Blackboard notation
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\figoneawidth}{.5\textwidth}                % Image formatting parameters
\newcommand{\lbreak}{\\}                                % Linebreak
\newcommand{\italicize}[1]{\textit {#1}}                % formatting commands for bibliography
%\newcommand{\embolden}[1]{\textbf {#1}}
\newcommand{\embolden}[1]{{#1}}
\newcommand{\undline}[1]{\underline {#1}}
\newcommand{\e}{\varepsilon}
\renewcommand{\epsilon}{\varepsilon}
%\renewcommand{:=}{=}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{DSO Screening Exam:\ \hmwkTitle}}\\
%    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
%    \vspace{0.1in}\large{\textit{Instructor: Dr. Steven Heilman\ }}
    \vspace{3in}
}

\author{Gregory Faletto}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[2]{\frac{\mathrm{d} #1}{\mathrm{d} #2}}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{Solution.}}

% Probability commands: Expectation, Variance, Covariance, Bias
%\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\DeclareMathOperator{\Tr}{Tr}

\newcommand{\nPr}[2]{\,_{#1}P_{#2}}
\newcommand{\nCr}[2]{\,_{#1}C_{#2}}
%\binom{n}{r}

% Tilde
\newcommand{\textapprox}{\raisebox{0.5ex}{\texttildelow}}

\begin{document}

\maketitle

\pagebreak


% Problem 1
\begin{exercise}[\textbf{Probability/Analysis}]

\begin{enumerate}[(a)]

% 1a
\item

\[
\dot{Z}(t) =  \pderiv{}{t} \sum_{i=1}^k \gamma_i(t) X_i =   \sum_{i=1}^k   \pderiv{\gamma_i(t) }{t} X_i 
\]

\[
\implies \E \left( \dot{Z}(t)  \right) =    \sum_{i=1}^k   \E \left( \pderiv{\gamma_i(t) }{t} X_i \right) = 0
\]

\[
\implies \Cov(Z(t), \dot{Z}(t)) = \E \big[ (Z(t) - \E[ Z(t)])(\dot{Z}(t)  - \E[\dot{Z}(t) ]) \big]  = \E \big[ Z(t) \dot{Z}(t)  \big]  
\]

\[
= \E \left[  \left(  \sum_{i=1}^k \gamma_i(t) X_i   \right) \left( \sum_{i=1}^k   \pderiv{\gamma_i(t) }{t} X_i  \right)  \right] = \sum_{i=1}^k \E \left( \gamma_i(t)  \pderiv{\gamma_i(t) }{t}  X_i^2 \right) + 0 = \sum_{i=1}^k \gamma_i(t)  \pderiv{\gamma_i(t) }{t} 
\]

% 1b
\item

\[
\ddot{Z}(t) =  \sum_{i=1}^k   \pderiv{^2\gamma_i(t) }{t^2} X_i 
\]

\end{enumerate}

\end{exercise}

% Problem 2
\begin{exercise}[\textbf{Mathematical statistics; hypothesis test. so maybe don't worry about?}]

\begin{enumerate}[(a)]

% 2a
\item

% 2b
\item

\end{enumerate}

\end{exercise}

% Problem 3
\begin{exercise}[\textbf{Probability; Wen says we should do this}]

\begin{enumerate}[(a)]

% 3a
\item Let \(u_j = \exp(\theta_j)\). In the \(n=1\) case we have

\[
I(m, \lambda) = \int_{\mathbb{R}} \frac{ \exp(\lambda \theta_1m_1) }{(1 + \exp(\theta_1) ) ^{\lambda}} d \theta_1 = \int_{\mathbb{R}} \frac{ \exp( \theta_1)^{\lambda m_1} }{(1 + \exp(\theta_1) ) ^{\lambda}} d \theta_1
\]

Let \(u_1 = 1 + \exp(\theta_1) \implies \partial u_1 = \exp(\theta_1)  \partial \theta_1\).

\[
= \int_{\mathbb{R}} \frac{ (u_1 - 1)^{\lambda m_1 - 1} }{u_1^{\lambda}} d u_1
\]

\[
= \ldots = \frac{\Gamma(\lambda (1 - m_1)) \Gamma(\lambda m_1)}{\Gamma(\lambda)}  = \frac{\Gamma(\lambda m_0) \Gamma(\lambda m_1)}{\Gamma(\lambda)}
\]

% 3b
\item

\end{enumerate}

\end{exercise}

% Problem 4
\begin{exercise}[\textbf{Convex Optimization}]

\textbf{Like theorem 2 in convex optimization lecture notes 3; probably don't need to worry about since not covered in Boyd. Don't prioritize.}

\end{exercise}

% Problem 5
\begin{exercise}[\textbf{High-dimensional statistics}]

\begin{enumerate}[(a)]

% 5a
\item If \(p > n\), then even if \(\boldsymbol{X}\) is full rank, it has a nullspace with nonzero entries. That is, the columns of \(\boldsymbol{X}\) must be linearly dependent, so there are infinitely many non-zero vectors \(\boldsymbol{v}\) such that \(\boldsymbol{X}\boldsymbol{v} = \boldsymbol{0}\)). Therefore for any solution \(\hat{\boldsymbol{\beta}}\) satisfying 

\[
\hat{\boldsymbol{\beta}} \in \underset{\boldsymbol{\beta} \in \mathbb{R}^p}{\arg \min} \lVert \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} \rVert_2^2 
\]

and any vector \(\boldsymbol{v}\) in the nullspace of \(\boldsymbol{X}\), \(\hat{\boldsymbol{\beta}} + \boldsymbol{v}\) is also a solution since

\[
 \underset{\boldsymbol{\beta} \in \mathbb{R}^p}{\min} \lVert \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} \rVert_2^2 =  \lVert \boldsymbol{y} - \boldsymbol{X} \hat{\boldsymbol{\beta}} \rVert_2^2  =  \lVert \boldsymbol{y} - \boldsymbol{X} \hat{\boldsymbol{\beta}} - \boldsymbol{0} \rVert_2^2  =  \lVert \boldsymbol{y} - \boldsymbol{X} (\hat{\boldsymbol{\beta}} + \boldsymbol{v}) \rVert_2^2 
 \]
 
 \[
 \iff \boldsymbol{\beta} + \boldsymbol{v} \in \underset{\boldsymbol{\beta} \in \mathbb{R}^p}{\arg \min} \lVert \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} \rVert_2^2 .
\]

% 5b
\item If we assume that \( \lVert \boldsymbol{\beta}_0 \rVert_0 = s \leq n\), then it may be that there is only one \(\boldsymbol{v}\) in the nullspace of \(\boldsymbol{X}\) such that \(\lVert \hat{\boldsymbol{\beta}}\rVert_0 = s\). Intuitively, this makes sense because as long as the features with zero coefficients in \(\boldsymbol{\beta}_0\) are sufficiently uncorrelated with the features with nonzero coefficients and the true coefficients are not too small, it is very unlikely that it is possible to construct another \(s\)-sparse vector with equally low empirical risk by replacing one of the true features with only one of the other features (or some combination thereof). (We would also hope that \(\operatorname{rank}(\boldsymbol{X} > s\). Otherwise, it is either the case that features in the true model are linearly dependent, in which case the true model is not identifiable, or some of the noise features are linearly dependent with some of the true features, which could also preclude identifiability.)

% 5c
\item \textbf{Solution in Section 1.9.1 of Linear Regression notes.}

% 5d
\item \textbf{Solution in Section 1.9.1 of Linear Regression notes.}

\end{enumerate}

\end{exercise}


\end{document}