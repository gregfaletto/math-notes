\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage[document]{ragged2e}
\usepackage{textcomp}
% \usepackage{amssymb}
\usepackage{import}
\usepackage{natbib}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\: \hmwkTitle}
%\rhead{\firstxmark}
%\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

%\newcommand{\enterProblemHeader}[1]{
%    \nobreak\extramarks{}{Question \arabic{#1} continued on next page\ldots}\nobreak{}
%    \nobreak\extramarks{Question \arabic{#1} (cont.)}{Question \arabic{#1} continued on next page\ldots}\nobreak{}
%}
%
%\newcommand{\exitProblemHeader}[1]{
%    \nobreak\extramarks{Question \arabic{#1} (cont.)}{Question \arabic{#1} continued on next page\ldots}\nobreak{}
%    \stepcounter{#1}
%    \nobreak\extramarks{Question \arabic{#1}}{}\nobreak{}
%}
%
%\setcounter{secnumdepth}{0}
%\newcounter{partCounter}
%\newcounter{homeworkProblemCounter}
%\setcounter{homeworkProblemCounter}{1}
%\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%%
%% Homework Problem Environment
%%
%% This environment takes an optional argument. When given, it will adjust the
%% problem counter. This is useful for when the problems given for your
%% assignment aren't sequential. See the last 3 problems of this template for an
%% example.
%%
%\newenvironment{homeworkProblem}[1][-1]{
%    \ifnum#1>0
%        \setcounter{homeworkProblemCounter}{#1}
%    \fi
%    \section{Problem \arabic{homeworkProblemCounter}}
%    \setcounter{partCounter}{1}
%    \enterProblemHeader{homeworkProblemCounter}
%}{
%    \exitProblemHeader{homeworkProblemCounter}
%}

\theoremstyle{definition}
\newtheorem{exercise}{Exercise}
\newtheorem{definition}{Definition}
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{corollary}{Corollary}[theorem]

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{2017 In-Class Exam}
%\newcommand{\hmwkDueDate}{Apr. 26, 2019}
\newcommand{\hmwkClass}{DSO}
%\newcommand{\hmwkClassTime}{Section A}
%\newcommand{\hmwkClassInstructor}{S. Heilman}
\newcommand{\hmwkAuthorName}{\textbf{G. Faletto} }

%\renewcommand{\subset}{\subseteq}
\renewcommand{\supset}{\supseteq}
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\abs}[1]{\left|#1\right|}                   % Absolute value notation
\newcommand{\absf}[1]{|#1|}                             % small absolute value signs
\newcommand{\vnorm}[1]{\left|\left|#1\right|\right|}    % norm notation
\newcommand{\vnormf}[1]{||#1||}                         % norm notation, forced to be small
\newcommand{\im}[1]{\mbox{im}#1}                        % Pieces of English for math mode
\newcommand{\tr}[1]{\mbox{tr}#1}
\newcommand{\Proj}[1]{\mbox{Proj}#1}
\newcommand{\Vol}[1]{\mbox{Vol}#1}
\newcommand{\Z}{\mathbb{Z}}                             % Blackboard notation
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\figoneawidth}{.5\textwidth}                % Image formatting parameters
\newcommand{\lbreak}{\\}                                % Linebreak
\newcommand{\italicize}[1]{\textit {#1}}                % formatting commands for bibliography
%\newcommand{\embolden}[1]{\textbf {#1}}
\newcommand{\embolden}[1]{{#1}}
\newcommand{\undline}[1]{\underline {#1}}
\newcommand{\e}{\varepsilon}
\renewcommand{\epsilon}{\varepsilon}
%\renewcommand{:=}{=}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{DSO Screening Exam:\ \hmwkTitle}}\\
%    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
%    \vspace{0.1in}\large{\textit{Instructor: Dr. Steven Heilman\ }}
    \vspace{3in}
}

\author{Gregory Faletto}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[2]{\frac{\mathrm{d} #1}{\mathrm{d} #2}}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{Solution.}}

% Probability commands: Expectation, Variance, Covariance, Bias
%\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\DeclareMathOperator{\Tr}{Tr}

\newcommand{\nPr}[2]{\,_{#1}P_{#2}}
\newcommand{\nCr}[2]{\,_{#1}C_{#2}}
%\binom{n}{r}

% Tilde
\newcommand{\textapprox}{\raisebox{0.5ex}{\texttildelow}}

\begin{document}

\maketitle

\pagebreak


% Problem 1
\begin{exercise}[\textbf{Probability/Analysis}]

\begin{enumerate}[(a)]

% 1a
\item

\[
\dot{Z}(t) =  \pderiv{}{t} \sum_{i=1}^k \gamma_i(t) X_i =   \sum_{i=1}^k   \dot{\gamma}_i(t) X_i 
\]

\[
\implies \E \left( \dot{Z}(t)  \right) =    \sum_{i=1}^k   \E \left( \dot{\gamma}_i(t) X_i \right) = 0
\]

\[
\implies \Cov(Z(t), \dot{Z}(t)) = \E \big[ (Z(t) - \E[ Z(t)])(\dot{Z}(t)  - \E[\dot{Z}(t) ]) \big]  = \E \big[ Z(t) \dot{Z}(t)  \big]  
\]

\[
= \E \left[  \left(  \sum_{i=1}^k \gamma_i(t) X_i   \right) \left( \sum_{i=1}^k   \dot{\gamma}_i(t) X_i  \right)  \right] = \sum_{i=1}^k \E \left( \gamma_i(t)  \dot{\gamma}_i(t)  X_i^2 \right) + 0 = \sum_{i=1}^k \gamma_i(t)  \dot{\gamma}_i(t) 
\]

since \(\E(X_i^2) = 1\). But 

\begin{equation}\label{2017.screen.1.a.deriv.id}
\sum_{i=1}^k \gamma_i \dot{\gamma}_i(t) = 0 
\end{equation}

because

\[
\sum_{i=1}^k \gamma_i^2(t) = 1 \iff  \pderiv{}{t} \left( \sum_{i=1}^k \gamma_i^2(t) \right) = 0 \iff 2 \sum_{i=1}^k \gamma_i(t) \dot{\gamma}_i(t) = 0,
\]

so the conclusion follows.

% 1b
\item

\[
Z(t) =  \sum_{i=1}^k   \gamma_i(t)  X_i \implies Z(t) \sim \mathcal{N} \left(0, \sum_{i=1}^k \gamma_i^2(t)  \right) = \mathcal{N} \left(0,1 \right)
\]

\[
\ddot{Z}(t) =  \sum_{i=1}^k   \ddot{\gamma}_i(t)  X_i \implies \ddot{Z}(t) \sim \mathcal{N} \left(0, \sum_{i=1}^k \ddot{\gamma}_i^2(t)  \right)
\]

Also, we have

%Therefore \(Z(t) \indep \ddot{Z}(t)\), so \(\E(Z(t) \mid \ddot{Z}(t)) = \E(Z(t)) = 0\).
%
%\[
%\vdots
%\]

%\[
%\E(\ddot{Z}(t)) = \sum_{i=1}^k \ddot{\gamma}_i(t) \E(X_i) = 0 
%\]

\[
\Cov(Z(t), \ddot{Z}(t)) = \E \big[ (Z(t) - \E[ Z(t)])(\ddot{Z}(t)  - \E[\ddot{Z}(t) ]) \big]  = \E \big[ Z(t) \ddot{Z}(t)  \big]  
\]

\[
= \E \left[  \left(  \sum_{i=1}^k \gamma_i(t) X_i   \right) \left( \sum_{i=1}^k   \ddot{\gamma}_i(t) X_i  \right)  \right] = \sum_{i=1}^k \E \left( \gamma_i(t)  \ddot{\gamma}_i(t)  X_i^2 \right) + 0 = \sum_{i=1}^k \gamma_i(t)  \ddot{\gamma}_i(t) = -\sum_{i=1}^k \dot{\gamma}_i^2(t) 
\]
 
because  differentiating (\ref{2017.screen.1.a.deriv.id}) yields

\[
\pderiv{}{t} \sum_{i=1}^k \gamma_i \dot{\gamma}_i(t) = 0  \iff \sum_{i=1}^k \left( \dot{\gamma}_i^2(t) + \gamma_i(t) \ddot{\gamma}_i(t) \right) = 0 \iff  \sum_{i=1}^k  \gamma_i(t) \ddot{\gamma}_i(t) = -\sum_{i=1}^k \dot{\gamma}_i^2(t) .
\]

Since both distributions are Gaussian, we have

\[
 \E(Z(t) \mid \ddot{Z}(t)) =  \E[Z(t)] + \frac{\Cov[ Z(t), \ddot{Z}(t)] }{ \Var[\ddot{Z}(t)]} ( \ddot{Z}(t) - \E[ \ddot{Z}(t)] )  =  \left[ \sum_{i=1}^k \ddot{\gamma}_i^2(t) \right]^{-1} \cdot \left[ -\sum_{i=1}^k \dot{\gamma}_i^2(t) \right]  \ddot{Z}(t).
 \]
 
% \[
% = 0 + \left[ \sum_{i=1}^k \ddot{\gamma}_i^2(t) \right]^{-1} \cdot \left[ -\sum_{i=1}^k \dot{\gamma}_i^2(t) \right]  \ddot{Z}(t)
% \]
%
%\[
%\vdots
%\]
%
%\[
%\E(Z(t) \mid \ddot{Z}(t) = \ddot{z}) = \int_{-\infty}^\infty z \cdot f_{Z \mid \ddot{Z}}(z \mid \ddot{z}) \ dz
%\]
%
%\[
%= \int_0^\infty \Pr(Z(t) \geq z \mid \ddot{Z}(t) = \ddot{z}) \ dz
%\]
%
%Note that
%
%\[
% f_{Z \mid \ddot{Z}}(z \mid \ddot{z}) = \frac{f_{Z, \ddot{Z}}(z, \ddot{z}) }{f_{\ddot{Z}}(\ddot{z})}
% \]
%
%Differentiating again, we have
%
%\[
% \pderiv{}{t} \left(   \sum_{i=1}^k\dot{\gamma}_i^2(t) \right) = \pderiv{}{t} \left(  - \sum_{i=1}^k  \gamma_i(t) \ddot{\gamma}_i(t) \right) \iff \sum_{i=1}^k 2 \dot{\gamma}_i(t) \ddot{\gamma}_i(t) = - \sum_{i=1}^k \left( \dot{\gamma}_i(t) \ddot{\gamma}_i(t) + \gamma_i(t) \dddot{\gamma}_i(t) \right)
%\]
%
%\[
% \iff 3\sum_{i=1}^k  \dot{\gamma}_i(t) \ddot{\gamma}_i(t) = - \sum_{i=1}^k  \gamma_i(t) \dddot{\gamma}_i(t)
%\]

\end{enumerate}

\end{exercise}

% Problem 2
\begin{exercise}[\textbf{Mathematical statistics; hypothesis test. so don't worry about}]

\begin{enumerate}[(a)]

% 2a
\item

% 2b
\item

\end{enumerate}

\end{exercise}

% Problem 3
\begin{exercise}[\textbf{Probability; Wen says we should do this}]

\begin{enumerate}[(a)]

% 3a
\item In the \(n=1\) case we have

\[
I(m, \lambda) = \int_{\mathbb{R}} \frac{ \exp(\lambda \theta_1m_1) }{(1 + \exp(\theta_1) ) ^{\lambda}} d \theta_1 = \int_{\mathbb{R}} \frac{ \exp( \theta_1)^{\lambda m_1} }{(1 + \exp(\theta_1) ) ^{\lambda}} d \theta_1
\]

%Let \(u_1=  \exp(\theta_1) \implies d_{u_1} =\exp(\theta_1) \ d_{\theta_1} \iff d_{\theta_1} = \frac{1}{u_1} d_{u_1}\). 
%
%%Let \(u_1 = 1 + \exp(\theta_1) \implies \partial u_1 = \exp(\theta_1)  \partial \theta_1\).
%%
%%\[
%%= \int_{\mathbb{R}} \frac{ (u_1 - 1)^{\lambda m_1 - 1} }{u_1^{\lambda}} d u_1
%%\]
%%
%
%\[
%\implies I(m, \lambda) =\int_{0}^\infty \frac{ u_1^{\lambda m_1} }{ (u_1 + 1)  ^{\lambda}} \ \frac{ d u_1}{u_1} =\int_{0}^\infty  u_1 ^{\lambda m_1 - 1} (u_1 + 1)  ^{-\lambda} \ d u_1
%\]
%
%\[
%= \ldots   = \int_0^1 u_1^{ - \lambda (m_1 - 1) - 1} (1-u_1)^{\lambda m_1 - 1} \ du_1 = \int_0^1 u_1^{\lambda(1 - m_1)- 1} (1-u_1)^{\lambda m_1 - 1} \ du_1= \frac{\Gamma(\lambda (1 - m_1)) \Gamma(\lambda m_1)}{\Gamma(\lambda [1 - m_1] + \lambda m_1)}  
%\]
%
%\[
%= \frac{\Gamma(\lambda m_0) \Gamma(\lambda m_1)}{\Gamma(\lambda)}
%\]
%
%where we used the definition of the Beta function:
%
%\[
%B(z_1, z_2) := \int_0^1 t^{z_1 - 1} (1-t)^{z_2 - 1} \ dt= \frac{ \Gamma(z_1) \Gamma(z_2)}{\Gamma(z_1 + z_2)}
%\]

Let \(u_1= 1 + \exp(\theta_1) \implies d_{u_1} =\exp(\theta_1) \ d_{\theta_1} \iff d_{\theta_1} = \frac{1}{u_1- 1} d_{u_1 }\). 

%Let \(u_1 = 1 + \exp(\theta_1) \implies \partial u_1 = \exp(\theta_1)  \partial \theta_1\).
%
%\[
%= \int_{\mathbb{R}} \frac{ (u_1 - 1)^{\lambda m_1 - 1} }{u_1^{\lambda}} d u_1
%\]
%

\[
\implies I(m, \lambda) =\int_{0}^\infty \frac{ (u_1 - 1)^{\lambda m_1} }{ u_1  ^{\lambda}} \ \frac{ d u_1}{u_1 - 1} =\int_{0}^\infty  (u_1 - 1)^{\lambda m_1 - 1} u_1  ^{-\lambda} \ d u_1
\]

\[
= \ldots   = \int_0^1 u_1^{ - \lambda (m_1 - 1) - 1} (1-u_1)^{\lambda m_1 - 1} \ du_1 = \int_0^1 u_1^{\lambda(1 - m_1)- 1} (1-u_1)^{\lambda m_1 - 1} \ du_1= \frac{\Gamma(\lambda (1 - m_1)) \Gamma(\lambda m_1)}{\Gamma(\lambda [1 - m_1] + \lambda m_1)}  
\]

\[
= \frac{\Gamma(\lambda m_0) \Gamma(\lambda m_1)}{\Gamma(\lambda)}
\]

where we used the definition of the Beta function:

\[
B(z_1, z_2) := \int_0^1 t^{z_1 - 1} (1-t)^{z_2 - 1} \ dt= \frac{ \Gamma(z_1) \Gamma(z_2)}{\Gamma(z_1 + z_2)}
\]

% 3b
\item

\[
\lim_{\lambda \to 0} \lambda^n I(m, \lambda) = \lim_{\lambda \to 0} \lambda^n \cdot   \frac{\Gamma(\lambda m_0) \Gamma(\lambda m_1) \cdots \Gamma(\lambda m_n)}{\Gamma(\lambda)} = \lim_{\lambda \to 0}  \frac{ \Gamma(\lambda m_0) }{\Gamma(\lambda)} \cdot  [\lambda  \Gamma(\lambda m_1)] \cdots[ \lambda \Gamma(\lambda m_n)]
\]

\[
= \lim_{\lambda \to 0}  \frac{ \Gamma(\lambda m_0) }{\Gamma(\lambda)}\cdot \frac{\Gamma(\lambda m_1)}{\Gamma(\lambda)} \cdots \frac{\Gamma(\lambda m_n)}{\Gamma(\lambda)}  \cdot  [\lambda  \Gamma(\lambda )] \cdots[ \lambda \Gamma(\lambda )]
\]

\[
= \lim_{\lambda \to 0}  \frac{ \Gamma(\lambda m_0) }{\Gamma(\lambda)}\cdot \frac{\Gamma(\lambda m_1)}{\Gamma(\lambda)} \cdots \frac{\Gamma(\lambda m_n)}{\Gamma(\lambda)}  
\]

\item Stirling's formula: 

\[
\lambda! \sim \lambda^\lambda e^{-\lambda} \sqrt{2\pi \lambda} = \lambda^{\lambda + 1/2} e^{-\lambda} \sqrt{2\pi } \iff \lim_{\lambda \to \infty} \frac{ \Gamma(\lambda+1) e^\lambda}{\lambda^{\lambda + 1/2}  \sqrt{2\pi }} = 1 \iff \lim_{\lambda \to \infty} \frac{\lambda \Gamma(\lambda) e^\lambda}{\lambda^{\lambda + 1/2}  \sqrt{2\pi }} = 1
\]

\[
 \iff \lim_{\lambda \to \infty} \frac{ \Gamma(\lambda) e^\lambda}{\lambda^{\lambda - 1/2}  } = \sqrt{2\pi }
\]


\[
\lim_{\lambda \to \infty} \lambda^{n/2} I(m, \lambda) = \lim_{\lambda \to \infty} \lambda^{n/2} \cdot \frac{\Gamma(\lambda m_0) \Gamma(\lambda m_1) \cdots \Gamma(\lambda m_n)}{\Gamma(\lambda)} 
\]

\item

\

\begin{center}
\noindent\fbox{
\parbox{0.9\textwidth}{
\begin{definition}[\textbf{Exponential Family}]\label{prob.defn.exp.fams} Let \(n, k\) be positive integers and let \(\mu\) be a measure on \(\mathbb{R}^n\). Let \(t_1, \ldots, t_k : \mathbb{R}^n \to \mathbb{R}\). Let \(h: \mathbb{R}^n \to [0, \infty]\), and assume \(h\) is not identically zero. For any \(w = (w_1, \ldots, w_k) \in \mathbb{R}^k\), define

\[
a(w) := \log \bigg[ \int_{\mathbb{R}^n} h(x) \exp \bigg( \sum_{i=1}^k w_i t_i(x) \bigg) d \mu(x) \bigg], \ \ \ \forall x \in \mathbb{R}^n
\]

The set \(\{w \in \mathbb{R}^k\}\) is called the \textbf{natural parameter space}. On this set, the function

\[
f_w(x) := h(x) \exp \bigg( \sum_{i=1}^k w_i t_i(x) - a(w) \bigg), \ \ \ \forall x \in \mathbb{R}^n
\]

satisfies \(\int_{\mathbb{R}^n} f_w(x) d \mu(s) = 1\) (by the definition of \(a(w)\)). The set of functions (which can be interpreted as probability density functions, or as probability mass functions according to \(\mu\)) \(\{f_w: \theta \in \Theta: a(w(\theta)) < \infty \}\) is called a \textbf{\(k\)-parameter exponential family in canonical form.} 
 
% \
% 
% More generally, let \(\Theta \in \mathbb{R}^k\) be any set an let \(w: \Theta \to \mathbb{R}^k\). We define a \textbf{\(k\)-parameter exponential family} to be a set of functions \(\{f_{\theta}: \theta \in \Theta\}\), where
% 
% \[
% f_{\theta}(x) := h(x) \exp \bigg( \sum_{i=1}^k w_i(\theta) t_i(x) - a(w(\theta))\bigg), \ \ \ \ \forall x \in \mathbb{R}^n
% \]
 
\end{definition}
}
}
\end{center}

Let \(X_{p_0, \ldots, p_j}: \Omega \to \mathbb{R}^n\) be the random variable described by this distribution, parameterized by \(p_0, \ldots, p_j\) and with \(\delta_{e_0}, \ldots, \delta_{e_n}\) fixed (otherwise this is not an exponential family, for reasons described below). Then we have

\[
f_{p_0, \ldots, p_j} (x) = \mathbb{P}(X_{p_0, \ldots, p_j} = x) = \begin{cases}
p_j/\sum_{i=0}^n p_i & x = \delta_{e_j} \\
0 & \text{otherwise}
\end{cases} =  I(x = \delta_{e_{k}}) p_{k}, k \in \{ 0, 1, \ldots, n\}.
\]

Let

\[
h(x) := \begin{cases}
1 & x \in \{\delta_{e_0}, \ldots, \delta_{e_n}\} \\
0 & \text{otherwise.}
\end{cases}
\]

Let \(k = n+1\), and let \(t_i(x) := I(x = \delta_{e_{i-1}})\) (an indicator function that equals 1 if \(x =  \delta_{e_{i-1}}\) and 0 otherwise). Let \(w_i := \log(p_{i-1})\), and let \(a(w) := 0\). Then

\[
 h(x) \exp \left( \sum_{i=1}^k w_i t_i(x) - a(w) \right) =  I(x \in \{\delta_{e_0}, \ldots, \delta_{e_n}\}) \exp \left( \sum_{i=1}^k I(x = \delta_{e_{i-1}}) \log(p_{i-1})  \right)
 \]
 
 \[
  = I(x = \delta_{e_{i-1}}) p_{i-1}, i \in \{ 1, \ldots, n + 1\} =  I(x = \delta_{e_{k}}) p_{k}, k \in \{ 0, 1, \ldots, n\} = f_{p_0, \ldots, p_j} (x),
\] 

so \(F\) is an exponential family. (Where the mass function equals 0 depends on \(\delta_{e_0}, \ldots, \delta_{e_n}\). So if these are not fixed and are instead parameters of the distribution, then where the mass function equals 0 depends on the parameters of the distribution, which is not allowed in an exponential family; \(h(x)\) determines where the mass function equals 0, and it may only depend on \(x\), not the parameters of the distribution.)

\textbf{stuff with conjugate prior has to do with Bayesian statistics; not our concern.}

\end{enumerate}

\end{exercise}

% Problem 4
\begin{exercise}[\textbf{Convex Optimization}]

\textbf{Like theorem 2 in convex optimization lecture notes 3; probably don't need to worry about since not covered in Boyd. Don't prioritize.}

\end{exercise}

% Problem 5
\begin{exercise}[\textbf{High-dimensional statistics}]

\begin{enumerate}[(a)]

% 5a
\item If \(p > n\), then even if \(\boldsymbol{X}\) is full rank, it has a nullspace with nonzero entries. That is, the columns of \(\boldsymbol{X}\) must be linearly dependent, so there are infinitely many non-zero vectors \(\boldsymbol{v}\) such that \(\boldsymbol{X}\boldsymbol{v} = \boldsymbol{0}\)). Therefore for any solution \(\hat{\boldsymbol{\beta}}\) satisfying 

\[
\hat{\boldsymbol{\beta}} \in \underset{\boldsymbol{\beta} \in \mathbb{R}^p}{\arg \min} \lVert \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} \rVert_2^2 
\]

and any vector \(\boldsymbol{v}\) in the nullspace of \(\boldsymbol{X}\), \(\hat{\boldsymbol{\beta}} + \boldsymbol{v}\) is also a solution since

\[
 \underset{\boldsymbol{\beta} \in \mathbb{R}^p}{\min} \lVert \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} \rVert_2^2 =  \lVert \boldsymbol{y} - \boldsymbol{X} \hat{\boldsymbol{\beta}} \rVert_2^2  =  \lVert \boldsymbol{y} - \boldsymbol{X} \hat{\boldsymbol{\beta}} - \boldsymbol{0} \rVert_2^2  =  \lVert \boldsymbol{y} - \boldsymbol{X} (\hat{\boldsymbol{\beta}} + \boldsymbol{v}) \rVert_2^2 
 \]
 
 \[
 \iff \boldsymbol{\beta} + \boldsymbol{v} \in \underset{\boldsymbol{\beta} \in \mathbb{R}^p}{\arg \min} \lVert \boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} \rVert_2^2 .
\]

% 5b
\item If we assume that \( \lVert \boldsymbol{\beta}_0 \rVert_0 = s \leq n\), then it may be that there is only one \(\boldsymbol{v}\) in the nullspace of \(\boldsymbol{X}\) such that \(\lVert \hat{\boldsymbol{\beta}}\rVert_0 = s\). Intuitively, this makes sense because as long as the features with zero coefficients in \(\boldsymbol{\beta}_0\) are sufficiently uncorrelated with the features with nonzero coefficients and the true coefficients are not too small, it is very unlikely that it is possible to construct another \(s\)-sparse vector with equally low empirical risk by replacing one of the true features with only one of the other features (or some combination thereof). (We would also hope that \(\operatorname{rank}(\boldsymbol{X}) > s\). Otherwise, it could either be the case that features in the true model are linearly dependent, in which case the true model is not identifiable, or some of the noise features are linearly dependent with some of the true features, which could also preclude identifiability.)

% 5c
\item \textbf{Solution in Section 1.9.1 of Linear Regression notes.}

We will follow the analysis from  \citet{Zhao2006}. The lasso problem is convex but not necessarily strictly convex if \(p > n\). That is, there is some flat region, so the minimizer may not be unique. Consider the KKT conditions from convex optimization:

\[
g(\beta) = \arg \min \left\{ \frac{1}{2n} \lVert y - X \beta \rVert_2^2 + \lambda \lVert \beta \rVert_1 \right\} =  \arg \min \left\{ f_1(\beta) + f_2(\beta) \right\}
\]

Then \(\hat{\beta}\) is a lasso solution if and only if 0 is in the subdifferential of \(g(\hat{\beta})\). Note that 

\[
\partial g(\hat{\beta}) = \nabla f_1 + \partial f_2 = \frac{1}{n} X^T(X \beta - \boldsymbol{y}) + \lambda \begin{bmatrix}  \vdots \\ \partial | \beta_j | \\ \vdots \end{bmatrix} = \frac{1}{n} X^T(X \beta - \boldsymbol{y}) + \lambda \begin{bmatrix}  \vdots \\ \begin{cases}  \operatorname{sgn}(\beta_j) & \beta_j \neq 0 \\ [-1, 1] & \beta_j = 0 \end{cases} \\ \vdots \end{bmatrix}
\]

Now assume \(\operatorname{supp}(\hat{\beta}) = \operatorname{supp}(\beta_0)\) (that is, assume lasso recovers the correct support). Suppose the first \(s\) features are nonzero and consider one of them (so we know that we should have \(\hat{\beta_j} \neq 0\)):

\[
0 \in \partial g(\hat{\beta}) \implies 0 \in \partial_j g(\hat{\beta}) = \bigg[ \frac{1}{n} X^T(X \beta - \boldsymbol{y}) \bigg]_j + \lambda \operatorname{sgn}(\hat{\beta}_j) 
\]

Therefore

\begin{equation}\label{linreg.lasso.first.cond}
\frac{1}{n} X_A^T(\boldsymbol{X} \boldsymbol{\hat{\beta}} - \boldsymbol{y}) + \lambda  \operatorname{sgn}(\hat{\beta}_j) = 0
\end{equation}

where \(X_A\) is a submatrix of \(\boldsymbol{X}\) containing the columns corresponding to the features in the true support, is our first condition. Next, consider what happens for \(j > s\) (features not in the true support). We have

\[
0 \in \partial g(\hat{\beta}) \implies 0 \in \partial_j g(\hat{\beta}) = \bigg[ \frac{1}{n}X^T(X \beta - \boldsymbol{y}) \bigg] + \lambda [-1, 1]
\]

\begin{equation}\label{linreg.lasso.bound.cond}
\implies \left\lVert \frac{1}{n} X_{A^C}^T (\boldsymbol{X} \boldsymbol{\hat{\beta}} - \boldsymbol{y}) \right\rVert_\infty \leq \lambda
\end{equation}

where \(X_{A^C}\) is a submatrix of \(\boldsymbol{X}\) containing the columns corresponding to the features not in the true support, is our boundary condition. Recall the true model

\[
y = X \beta_0 + \boldsymbol{\epsilon}
\]

and consider the case \(X = \begin{bmatrix}\boldsymbol{X}_1 & \boldsymbol{X}_2 \end{bmatrix}\) where \(\boldsymbol{X}_1\) are the features in the true model and \(\boldsymbol{X}_2\) are noise features; that is, \(\beta_0 = \begin{bmatrix} \boldsymbol{\beta}_1 \\ 0 \end{bmatrix}\). Then we are assuming

\[
\hat{\beta}_{\text{lasso}} = \begin{bmatrix} \hat{\boldsymbol{\beta}_1} \\ 0 \end{bmatrix}.
\]

We have from (\ref{linreg.lasso.first.cond})

\[
0 = \frac{1}{n} \boldsymbol{X}_1^T(\boldsymbol{X} \boldsymbol{\hat{\beta}} - \boldsymbol{y}) + \lambda  \operatorname{sgn}(\boldsymbol{\hat{\beta}}_1) = \frac{1}{n} \boldsymbol{X}_1^T(\boldsymbol{X}_1 \boldsymbol{\hat{\beta}}_1 - \boldsymbol{X}_1 \boldsymbol{\beta}_1 - \boldsymbol{\epsilon}) + \lambda  \operatorname{sgn}(\boldsymbol{\hat{\beta}}_1) 
\]
% = \frac{1}{n}(  \boldsymbol{X}_1^T\boldsymbol{X}_1 \boldsymbol{\hat{\beta}}_1 - \boldsymbol{X}_1^T\boldsymbol{X}_1 \boldsymbol{\beta}_1 - \boldsymbol{X}_1^T \boldsymbol{\epsilon}) + \lambda  \operatorname{sgn}(\boldsymbol{\hat{\beta}}_1)
\[
 \iff \frac{1}{n} \boldsymbol{X}_1^T\boldsymbol{X}_1 (\boldsymbol{\hat{\beta}}_1 - \boldsymbol{\beta}_1)  =  \frac{1}{n} \boldsymbol{X}_1^T \boldsymbol{\epsilon} - \lambda  \operatorname{sgn}(\boldsymbol{\hat{\beta}}_1)
 \]
 
 Let's assume that \(\operatorname{sgn}(\hat{\beta}) = \operatorname{sgn}(\beta_0)\) (sign consistency).
 
 \[
  \iff \frac{1}{n} \boldsymbol{X}_1^T\boldsymbol{X}_1 (\boldsymbol{\hat{\beta}}_1 - \boldsymbol{\beta}_1)  =  \frac{1}{n} \boldsymbol{X}_1^T \boldsymbol{\epsilon} - \lambda  \operatorname{sgn}(\boldsymbol{\beta}_1)
  \]
  
  which is linear in \(\hat{\beta}\). Solving, we have
  
\begin{equation}\label{linreg.lasso.first.cond.solved}
  \iff  \boldsymbol{\hat{\beta}}_1 - \boldsymbol{\beta}_1  =  ( \boldsymbol{X}_1^T\boldsymbol{X}_1)^{-1} \big( \boldsymbol{X}_1^T \boldsymbol{\epsilon} - n  \lambda  \operatorname{sgn}(\boldsymbol{\beta}_1)\big)   \iff  \boldsymbol{\hat{\beta}}_1   = \boldsymbol{\beta}_1 +   ( n^{-1}\boldsymbol{X}_1^T\boldsymbol{X}_1)^{-1} \big( n^{-1} \boldsymbol{X}_1^T \boldsymbol{\epsilon} -   \lambda  \operatorname{sgn}(\boldsymbol{\beta}_1) \big)
\end{equation}

Looking at the second (boundary) condition (\ref{linreg.lasso.bound.cond}), we have

\begin{equation}\label{linreg.lasso.bound.cond.cont}
\left\lVert \frac{1}{n} \boldsymbol{X}_{2}^T (\boldsymbol{X} \boldsymbol{\hat{\beta}} - \boldsymbol{y}) \right\rVert_\infty \leq \lambda.
\end{equation}

Consider that 

\[
\boldsymbol{X} \boldsymbol{\hat{\beta}}- \boldsymbol{y} = \boldsymbol{X}_1 \boldsymbol{\hat{\beta}}_1 - \boldsymbol{X}_1 \boldsymbol{\beta}_1 - \boldsymbol{\epsilon} = \boldsymbol{X}_1(\boldsymbol{\hat{\beta}}_1 - \boldsymbol{\beta}_1) - \boldsymbol{\epsilon}
\]

Substituting in the result from (\ref{linreg.lasso.first.cond.solved}) yields

\[
\boldsymbol{X} \boldsymbol{\hat{\beta}}- \boldsymbol{y}  = \boldsymbol{X}_1\big[  ( n^{-1} \boldsymbol{X}_1^T\boldsymbol{X}_1)^{-1} \big( n^{-1} \boldsymbol{X}_1^T \boldsymbol{\epsilon} -  \lambda  \operatorname{sgn}(\boldsymbol{\beta}_1)\big)   \big] - \boldsymbol{\epsilon}
\]

which when we plug into (\ref{linreg.lasso.bound.cond.cont}) yields

\[
\left\lVert \frac{1}{n} \boldsymbol{X}_{2}^T \left[\boldsymbol{X}_1  (n^{-1} \boldsymbol{X}_1^T\boldsymbol{X}_1)^{-1} \big( n^{-1} \boldsymbol{X}_1^T \boldsymbol{\epsilon} -  \lambda  \operatorname{sgn}(\boldsymbol{\beta}_1)\big) - \boldsymbol{\epsilon}\right] \right\rVert_\infty \leq \lambda.
\]

\[
\iff \left\lVert \frac{1}{n} \boldsymbol{X}_{2}^T \boldsymbol{X}_1 (n^{-1} \boldsymbol{X}_1^T\boldsymbol{X}_1)^{-1} \big( n^{-1} \boldsymbol{X}_1^T \boldsymbol{\epsilon} -  \lambda  \operatorname{sgn}(\boldsymbol{\beta}_1)\big)    -  \frac{1}{n} \boldsymbol{X}_{2}^T \boldsymbol{\epsilon} \right\rVert_\infty \leq \lambda.
\]

Using the Triangle Inequality, we have

\[
\left\lVert \frac{1}{n} \boldsymbol{X}_{2}^T \boldsymbol{X}_1 (n^{-1} \boldsymbol{X}_1^T\boldsymbol{X}_1)^{-1} \big( n^{-1} \boldsymbol{X}_1^T \boldsymbol{\epsilon} -  \lambda  \operatorname{sgn}(\boldsymbol{\beta}_1)\big)    -  \frac{1}{n} \boldsymbol{X}_{2}^T \boldsymbol{\epsilon} \right\rVert_\infty 
\]

\[
\leq \left\lVert \frac{1}{n} \boldsymbol{X}_{2}^T \boldsymbol{X}_1 (n^{-1} \boldsymbol{X}_1^T\boldsymbol{X}_1)^{-1} \big( n^{-1} \boldsymbol{X}_1^T \boldsymbol{\epsilon} -  \lambda  \operatorname{sgn}(\boldsymbol{\beta}_1)\big)  \right\rVert_\infty + \left\lVert  \frac{1}{n} \boldsymbol{X}_{2}^T \boldsymbol{\epsilon} \right\rVert_\infty 
\]

\begin{equation}\label{linreg.lasso.bound.cond.cont.more}
\leq \left\lVert \frac{1}{n} \boldsymbol{X}_{2}^T  \boldsymbol{X}_1 (n^{-1} \boldsymbol{X}_1^T\boldsymbol{X}_1)^{-1} \right\rVert_\infty \cdot \left\lVert  n^{-1} \boldsymbol{X}_1^T \boldsymbol{\epsilon} -  \lambda  \operatorname{sgn}(\boldsymbol{\beta}_1)    \right\rVert_\infty + \left\lVert  \frac{1}{n} \boldsymbol{X}_{2}^T \boldsymbol{\epsilon} \right\rVert_\infty 
\end{equation}

\[
\vdots
\]

% 5d
\item \textbf{Solution in Section 1.9.1 of Linear Regression notes.} Theorem 4 from \citet{Zhao2006} states that if \(\boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \sigma^2 \boldsymbol{I}_n)\) for some \(\sigma^2 > 0\) and some other regularity conditions hold, then the Strong Irrepresentable Condition implies the lasso has sign consistency (which implies model selection consistency) with high probability for \(\lambda\) satisfying

\[
\lambda(n) \propto n^{(1+c_4)/2}
\]

where \(c_4\) is a constant between 0 and 1 and smaller than \(c_2 - c_1\), where \(c_1\) is a constant governing how large the true model can be with \(0 \leq c_1\) and \(c_2\) is a constant governing the minimum size of the coefficients in the true model with \(c_1 < c_2 \leq 1\).

Recall that the Strong Irrepresentable Condition is the following: if as in part 5(c) \(\boldsymbol{X}_1 \in \mathbb{R}^{n \times q}\) is a matrix containing only the \(q\) columns of \(\boldsymbol{X}\) corresponding to features in the true model and \(\boldsymbol{X}_2 \in \mathbb{R}^{n \times p - q}\) contains only the \(p - q\) columns of \(\boldsymbol{X}\) corresponding to irrelevant features, the Strong Irrepresentable Condition is

\[
\left\lVert \frac{1}{n} \boldsymbol{X}_2^T \boldsymbol{X}_1 \left(\frac{1}{n} \boldsymbol{X}_1^T \boldsymbol{X}_1 \right)^{-1} \operatorname{sgn} \left(\boldsymbol{\beta}_{(1)}^n \right) \right\rVert_\infty \leq \boldsymbol{1} - \boldsymbol{\eta}
\]  

\[
\iff \left\lVert  \left( \boldsymbol{X}_1^T \boldsymbol{X}_1 \right)^{-1}  \boldsymbol{X}_1^T \boldsymbol{X}_2 \right\rVert_\infty \leq \boldsymbol{1} - \boldsymbol{\eta}
\]  

for some \(\boldsymbol{\eta} \in \mathbb{R}^n\) with \(\boldsymbol{\eta} \succeq \boldsymbol{0}\), where \(\boldsymbol{\beta}_{(1)}^n\) are the coefficients of \(\boldsymbol{X}_1\) in the true model. That is, a regression of the variables in \(\boldsymbol{X}_2\) against the variables in \(\boldsymbol{X}_1\) may not have any coefficients that are larger in absolute value than \(1 - \eta\) for some \(\eta > 0\). This implies that the features in \(\boldsymbol{X}_2\) are not too strongly correlated with the features in \(\boldsymbol{X}_1\) (the features of \(\boldsymbol{X}_2\) are ``irrepresentable" by the features in \(\boldsymbol{X}_1\)).

%\(p_n\) is the number of features in the design matrix; the \(n\) in the subscript indicates that \(p_n\) is allowed to grow asymptotically with \(n\).

(Roughly speaking, the required regularity conditions are that the number of features in the design matrix \(p_n\) is not excessively large, the eigenvalues of \(\boldsymbol{X}_1\) are not too small, the coefficients of the relevant features in the true model are not too small, and the model is sufficiently sparse. The conditions of this theorem allow \(p_n\) to grow asymptotically with \(n\); the requirements for lower bounded eigenvalues and coefficients on the true coefficients are consequences of allowing \(p_n\) to grow with \(n\).)

\end{enumerate}

\end{exercise}

\bibliographystyle{abbrvnat}
\bibliography{mybib2fin}
\end{document}