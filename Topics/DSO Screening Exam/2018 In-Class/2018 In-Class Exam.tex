\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage[document]{ragged2e}
\usepackage{textcomp}
% \usepackage{amssymb}
\usepackage{import}
\usepackage{natbib}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\: \hmwkTitle}
%\rhead{\firstxmark}
%\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

%\newcommand{\enterProblemHeader}[1]{
%    \nobreak\extramarks{}{Question \arabic{#1} continued on next page\ldots}\nobreak{}
%    \nobreak\extramarks{Question \arabic{#1} (cont.)}{Question \arabic{#1} continued on next page\ldots}\nobreak{}
%}
%
%\newcommand{\exitProblemHeader}[1]{
%    \nobreak\extramarks{Question \arabic{#1} (cont.)}{Question \arabic{#1} continued on next page\ldots}\nobreak{}
%    \stepcounter{#1}
%    \nobreak\extramarks{Question \arabic{#1}}{}\nobreak{}
%}
%
%\setcounter{secnumdepth}{0}
%\newcounter{partCounter}
%\newcounter{homeworkProblemCounter}
%\setcounter{homeworkProblemCounter}{1}
%\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%%
%% Homework Problem Environment
%%
%% This environment takes an optional argument. When given, it will adjust the
%% problem counter. This is useful for when the problems given for your
%% assignment aren't sequential. See the last 3 problems of this template for an
%% example.
%%
%\newenvironment{homeworkProblem}[1][-1]{
%    \ifnum#1>0
%        \setcounter{homeworkProblemCounter}{#1}
%    \fi
%    \section{Problem \arabic{homeworkProblemCounter}}
%    \setcounter{partCounter}{1}
%    \enterProblemHeader{homeworkProblemCounter}
%}{
%    \exitProblemHeader{homeworkProblemCounter}
%}

\theoremstyle{definition}
\newtheorem{exercise}{Exercise}
\newtheorem{definition}{Definition}
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{corollary}{Corollary}[theorem]

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{2018 In-Class Exam}
%\newcommand{\hmwkDueDate}{Apr. 26, 2019}
\newcommand{\hmwkClass}{DSO}
%\newcommand{\hmwkClassTime}{Section A}
%\newcommand{\hmwkClassInstructor}{S. Heilman}
\newcommand{\hmwkAuthorName}{\textbf{G. Faletto} }

%\renewcommand{\subset}{\subseteq}
\renewcommand{\supset}{\supseteq}
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\abs}[1]{\left|#1\right|}                   % Absolute value notation
\newcommand{\absf}[1]{|#1|}                             % small absolute value signs
\newcommand{\vnorm}[1]{\left|\left|#1\right|\right|}    % norm notation
\newcommand{\vnormf}[1]{||#1||}                         % norm notation, forced to be small
\newcommand{\im}[1]{\mbox{im}#1}                        % Pieces of English for math mode
\newcommand{\tr}[1]{\mbox{tr}#1}
\newcommand{\Proj}[1]{\mbox{Proj}#1}
\newcommand{\Vol}[1]{\mbox{Vol}#1}
\newcommand{\Z}{\mathbb{Z}}                             % Blackboard notation
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\figoneawidth}{.5\textwidth}                % Image formatting parameters
\newcommand{\lbreak}{\\}                                % Linebreak
\newcommand{\italicize}[1]{\textit {#1}}                % formatting commands for bibliography
%\newcommand{\embolden}[1]{\textbf {#1}}
\newcommand{\embolden}[1]{{#1}}
\newcommand{\undline}[1]{\underline {#1}}
\newcommand{\e}{\varepsilon}
\renewcommand{\epsilon}{\varepsilon}
%\renewcommand{:=}{=}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{DSO Screening Exam:\ \hmwkTitle}}\\
%    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
%    \vspace{0.1in}\large{\textit{Instructor: Dr. Steven Heilman\ }}
    \vspace{3in}
}

\author{Gregory Faletto}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[2]{\frac{\mathrm{d} #1}{\mathrm{d} #2}}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{Solution.}}

% Probability commands: Expectation, Variance, Covariance, Bias
%\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\DeclareMathOperator{\Tr}{Tr}

\newcommand{\nPr}[2]{\,_{#1}P_{#2}}
\newcommand{\nCr}[2]{\,_{#1}C_{#2}}
%\binom{n}{r}

% Tilde
\newcommand{\textapprox}{\raisebox{0.5ex}{\texttildelow}}

\begin{document}

\maketitle

\pagebreak


% Problem 1
\begin{exercise}[\textbf{Probability}]

\begin{enumerate}[(a)]

% 1a
\item 

\begin{enumerate}[(i)]

% 1(a)i
\item We have

\[
e^{-y} T(y) \xrightarrow{d} \operatorname{Exponential}(\lambda) \iff \lim_{y \to \infty} \Pr(e^{-y} T(y) \leq t) = 1 - e^{-\lambda t} 
\]

\[
\iff \lim_{y \to \infty} \Pr \left( \frac{  \inf \{x \geq 0: M(x) \geq y\} }{e^y}  > t \right) = e^{-\lambda t} \iff \lim_{y \to \infty} \Pr \left(  e^ { \log \left( \inf \{x \geq 0: M(x) \geq y\} \right)  - y }  > t \right) = e^{-\lambda t}
\]

\[
\iff \lim_{y \to \infty} \Pr \left(  e^ { \log \left( \inf \{x \geq 0: M(x) \geq y\} \right)  - y }  > t \right) = 1 - \lambda t +\frac{\lambda^2t^2}{2!} - \frac{\lambda^3t^3}{3!} + \ldots + \frac{(-\lambda t)^n}{n!} + \ldots 
\]

\[
\vdots
\]

\[
\lim_{y \to \infty} e^{-y}  \inf \{x \geq 0: M(x) \geq y\}  =  \lambda e^{- \lambda t} 
\]

\[
\vdots
\]

So as \(y \to \infty\), \(\Pr \left(  e^ { \log \left( \inf \{x \geq 0: M(x) \geq y\} \right)  - y }  > t \right) \) converges to an \(\operatorname{Exponential}(\lambda)\) random variable. That is, the probability that the ratio of the first hit time for \(y\) (\(T(y)\)) and \(e^y\) exceeds \(t\) has a memoryless distribution in \(t\) as \(y\) grows without bound.

\[
\vdots
\]


% 1(a)ii
\item

\end{enumerate}

% 1b
\item \textbf{Mohammad:}

Using hint:

\[
\int_0^1 t^x  \ dt = \frac{1}{x+1}
\]

\[
 \iff \E \int_0^1 t^x  \ dt = \E \left( \frac{1}{x+1} \right)  \iff \int_0^1f \E ( t^x)  \ dt = \E \left( \frac{1}{x+1} \right)
\]

\[
\vdots
\]

\[
X \sim \operatorname{Bin}(n, p)
\]

Recall that 

\[
\E(X) = np \iff \sum_{x=0}^n x \cdot \binom{n}{x} p^x(1-p)^{n-x} =  (1-p)^n\sum_{x=0}^n x \binom{n}{x} \left( \frac{p}{1-p} \right)^x
\]

\[
=  (1-p)^n\sum_{x=0}^n \frac{n!}{(n-x)!(x-1)!} \left( \frac{p}{1-p} \right)^x  = np.
\]

Then we have

\[
\E \left[ \frac{1}{1 + X} \right] = \sum_{x=0}^\infty \frac{1}{1 + x} \cdot \Pr(X=x) = \sum_{x=0}^n \frac{1}{1 + x} \cdot \binom{n}{x} p^x(1-p)^{n-x} = (1-p)^n\sum_{x=0}^n \frac{1}{1 + x} \cdot \binom{n}{x} \left( \frac{p}{1-p} \right)^x
\]

\[
= (1-p)^n\sum_{x=0}^n  \frac{n!}{(n-x)!(x+1)!}\left( \frac{p}{1-p} \right)^x  = (1-p)^n\sum_{x=0}^n \frac{1}{(x+1)x} \frac{n!}{(n-x)!(x-1)!}\left( \frac{p}{1-p} \right)^x
\]



\[
\vdots
\]

\[
\E \left[ \frac{1}{1 + X} \right] = \int_{0}^\infty \Pr\left( [X+1]^{-1} > x \right) dx = \int_{0}^{1} \Pr\left( \frac{1}{X+1} > x \right)  dx  = \int_{0}^{1} \Pr\left( X < \frac{1}{x} - 1\right)   dx
\]

\[
= \lim_{a \to 0^+} \int_{a}^{1} \Pr \left( X < \lceil x^{-1} - 1\rceil \right) dx
\]

\[
 = \lim_{a \to 0^+} \int_{a}^{(n+1)^{-1}} \Pr \left( X < \lceil x^{-1} - 1\rceil \right) dx + \int_{(n+1)^{-1}}^{1} \Pr \left( X < \lceil x^{-1} - 1\rceil \right) dx 
 \]
 
 \[
 = \frac{1}{n+1} \cdot 1 + \int_{(n+1)^{-1}}^{1} \Pr \left( X < \lceil x^{-1} - 1\rceil \right) dx 
 \]

%\[
%= \lim_{a \to 0^+} \int_{a}^{1}  \left[ \sum_{j=0}^{\lceil x^{-1} - 1\rceil} \Pr\left( X = j  \right) \right]  dx = \lim_{a \to 0^+} \int_{a}^{1}  \left[ \sum_{j=0}^{\lfloor x^{-1} \rfloor} \binom{n}{j} p^j(1-p)^{n-j} \right]  dx
%\]
%
%\[
%= \lim_{a \to 0^+} \int_{a}^{1}  \left[ \sum_{j=0}^{\min \{\lfloor x^{-1} \rfloor, n\}} \binom{n}{j} p^j(1-p)^{n-j} \right]  dx = \frac{1}{n} \cdot 1 + \int_{n^{-1}}^{1}  \left[ \sum_{j=0}^{\lfloor x^{-1} \rfloor} \binom{n}{j} p^j(1-p)^{n-j} \right]  dx
%\]

This is simply the area under \(n\) rectangles with heights \(1, \Pr(X \leq n-1), \Pr(X \leq n- 2), \ldots, \Pr(X =0)\) and bases \((n+1)^{-1}, n^{-1} - (n+1)^{-1}, (n-1)^{-1} - n^{-1},  \ldots, 1/2\). That is, we can write this as

\[
\E \left[ \frac{1}{1 + X} \right] = \frac{1}{n+1} \cdot 1 + \left( \frac{1}{n} - \frac{1}{n+1} \right) \cdot \Pr(X \leq n-1) + \left( \frac{1}{n-1} - \frac{1}{n} \right) \cdot \Pr(X \leq n-2) 
\]

\[
+ \ldots  + \left( \frac{1}{2} - \frac{1}{3} \right)  \cdot \Pr(X\leq 1)  + \frac{1}{2} \cdot \Pr(X=0) 
\]

%= \sum_{x=0}^{n-1} \sum_{j=x+1}^n  \binom{n}{j} p^j(1-p)^{1-j}
%
%\[
% = (1-p) \sum_{x=0}^{n-1} \sum_{j=x+1}^n  \binom{n}{j} \left( \frac{p}{1-p} \right) ^j
%\]

\[
\vdots
\]

Recall that 

\[
\E(X) = np \iff \sum_{x=0}^{n-1} \sum_{j=x+1}^n  \binom{n}{j} p^j(1-p)^{n-j}   =  (1-p) ^n\sum_{x=0}^{n-1} \sum_{j=x+1}^n  \binom{n}{j} \left( \frac{p}{1-p} \right) ^j= np.
\]

\end{enumerate}

\end{exercise}

% Problem 2
\begin{exercise}[\textbf{Mathematical Statistics}]

\begin{enumerate}[(a)]

% 2a
\item Let \(\boxed{T_n (X_1, \ldots, X_n) := \sum_{i=1}^n X_i.}\) 


% 2b
\item Note that if \(T_n(X_1, \ldots, X_n) = y\), then \(\Pr \left[  (X_1, \ldots, X_n)  = (x_1, \ldots, x_n)  \right] = 0\) unless \(\sum_{i=1}^n x_i = y\). Therefore we have

\[
\Pr \left[  (X_1, \ldots, X_n)  = (x_1, \ldots, x_n) \mid T_n (X_1, \ldots, X_n) = y  \right] = \frac{\Pr \left[  (X_1, \ldots, X_n)  = (x_1, \ldots, x_n)  \right] }{\Pr\left(  T_n (X_1, \ldots, X_n) = y \right)}
\]

%
\[
\frac{\prod_{i=1}^n p^{x_i} (1-p)^{1-x_i}}{\binom{n}{y} p^y(1-p)^{n-y}} = \frac{ p^{\sum_{i=1}^n x_i} (1-p)^{n-\sum_{i=1}^n x_i}}{\binom{n}{y} p^y(1-p)^{n-y}}   = \frac{ p^{y} (1-p)^{n-y}}{\binom{n}{y} p^y(1-p)^{n-y}}   = \frac{1}{\binom{n}{y}}
\]

which does not depend on \(p\). That is, the distribution of \(X_1, \ldots, X_n\) conditional on \(T_n(X_1, \ldots, X_n)\) is independent of \(p\). Therefore \(T_n(X_1, \ldots, X_n)\) is sufficient for \(p\).


% 2c
\item Likelihood function:

\[
\mathcal{L}(p) = \prod_{i=1}^n p^{X_i} (1-p)^{1-X_i} 
%= (1-p)^n \prod_{i=1}^n \left(\frac{p}{1-p} \right)^{X_i} = (1-p)^n \left(\frac{p}{1-p} \right)^{\sum_{i=1}^n X_i}
\]

%\[
%\implies \ell(p) = n \log(1-p) + \sum_{i=1}^n X_i \log  \left(\frac{p}{1-p} \right)
%\]

\[
\implies \ell(p) = \sum_{i=1}^n \left[ X_i \log(p) + (1-X_i) \log(1-p) \right] = \log(p)  \sum_{i=1}^n X_i +  \log(1-p)\left(n -  \sum_{i=1}^n  X_i \right) 
\]

\[
\implies \deriv{}{p} \ell(p) =\frac{1}{p}  \sum_{i=1}^n X_i - \frac{1}{1-p}\left(n -  \sum_{i=1}^n  X_i \right) = 0 \implies \frac{1}{p}  \sum_{i=1}^n X_i  = \frac{1}{1-p}\left(n -  \sum_{i=1}^n  X_i \right)
\]

\[
\iff  \sum_{i=1}^n X_i - p  \sum_{i=1}^n X_i= pn - p \sum_{i=1}^n X_i \iff \boxed{ \hat{p} = \frac{1}{n} \sum_{i=1}^n X_i.}
\]

We can find its asymptotic distribution using the Central Limit Theorem:


\

\begin{center}
\noindent\fbox{
\parbox{0.9\textwidth}{
\begin{theorem} \label{asym.clt} \textbf{Central Limit Theorem (Grimmett and Stirzaker theorem 5.10.4.)} Let \(X_1, X_2, \ldots\) be a sequence of independent identically distributed random variables with finite mean \(\mu\) and finite non-zero variance \(\sigma^2\), and let \(S_n = \sum_{i=1}^n X_i\). Then

\[
\frac{S_n - n \mu}{\sqrt{n \sigma^2}} \xrightarrow{d} \mathcal{N}(0,1)
\]
\end{theorem}
}
}
\end{center}
\

Since \(\E(X_i) = p, \Var(X_i) = p(1-p)\), we have

\[
\frac{\sum_{i=1}^n X_i - np}{\sqrt{n p(1-p)}} \xrightarrow{d} \mathcal{N}(0,1) \iff  \sum_{i=1}^n X_i -np \xrightarrow{d} \mathcal{N}(0,np(1-p))
\]

\[
  \iff  \frac{1}{n} \sum_{i=1}^n X_i -p \xrightarrow{d} \mathcal{N}\left(0, \frac{p(1-p)}{n} \right)   \iff  \boxed{ \frac{1}{n} \sum_{i=1}^n X_i  \xrightarrow{d} \mathcal{N}\left(p, \frac{p(1-p)}{n} \right)}
\]

\end{enumerate}

\end{exercise}

% Problem 3
\begin{exercise}[\textbf{Mathematical Statistics}]

\begin{enumerate}[(a)]

% 3a
\item We have

\[
X \mid \mu \sim \mathcal{N}(\mu, \boldsymbol{I}_n)
\]

Let \(X = (X_1, \ldots, X_n)^T\) and let \(\mu = (\mu_1, \ldots, \mu_n)^T\). Notice that

\[
\E(X^TX \mid \mu) = \E \left( X_1^2 + X_2^2 + \ldots + X_n^2 \right) = \sum_{i=1}^n \E (X_i^2) = \sum_{i=1}^n \left( \Var(X_i) + \E(X_i)^2 \right) = \sum_{i=1}^n \left( 1+ \mu_i^2 \right)
\]

\[
= n + \lVert \mu \rVert_2^2 \implies \E(X^TX - n\mid \mu) =  \lVert \mu \rVert_2^2 
\]

Therefore given \(\mu\), \(\boxed{X^TX - n}\) is unbiased for \(\lVert \mu \rVert_2^2\).

% 3b
\item We have

\[
\mu \sim \mathcal{N}(0, k \boldsymbol{I}_n)
\]

\[
\E\left[ \left( \lVert \mu \rVert_2^2 - t(X)  \right)^2 \mid X\right] = \E\left[  \lVert \mu \rVert_2^4 -2 \lVert \mu \rVert_2^2 t(X)   + t(X)^2 \mid X\right] = \E\left[  \lVert \mu \rVert_2^4\mid X\right]  -2t (X) \E \left[ \lVert \mu \rVert_2^2 \mid X\right]    + t(X)^2 
\]

The estimator minimizing this is \(t(X) = \E \left[ \lVert \mu \rVert_2^2 \mid X\right] = \E \left[ \mu^T\mu\mid X\right] \), which we need to find. We have that \(\mu \sim \mathcal{N}(0, k \boldsymbol{I}_n)\), so
%\[
% \Pr(\mu^T\mu = t \mid X) 
% \]

\[
f_{\mu^T\mu}( t ) = f_{\sum_{i=1}^n \mu_i^2 }( t ) = f_{\sum_{i=1}^n \left[ \frac{\mu_i}{k} \right] ^2} \left( \frac{t}{k^2} \right) = f_{\chi_n^2} \left( \frac{t}{k^2} \right) 
\]

where \( f_{\chi_n^2} \) is the density of a \(\chi^2\) random variable with \(n\) degrees of freedom. Also, the set of vectors \(\{\mu' \in \mathbb{R}^n \mid \mu'^T\mu' = \lVert \mu \rVert_2^2\}\) is a hypersphere of radius \(\lVert \mu \rVert_2\) in \(\mathbb{R}^n\), so the conditional density of \(\mu'\) given \(\mu^T\mu\) is uniform over the surface of this hypersphere. Per \citet{Muller1959}, this can be generated by drawing \(n\) standard Gaussian random variables then dividing each by the \(\ell_2\) norm of all of them, then in this case multiplying by the desired \(\ell_2\) norm \(\lVert \mu \rVert_2\). That is,

\[
 f_{\mu \mid \mu^T\mu} (m \mid t) =  f_{\mu \mid \mu^T\mu} \big((m_1, \ldots, m_n) \mid t \big) =  f_{\mu \mid \mu^T\mu} \big((m_1, \ldots, m_n) \mid t \big)
\]

%http://mathworld.wolfram.com/HyperspherePointPicking.html

\[
\implies f_{\mu^T\mu \mid X} (t \mid x) =  \frac{f_{\mu^T\mu , X}(t, x)}{f_X(x)} =  \frac{f_{\mu^T\mu}(t) f_{\mu \mid \mu^T\mu} (m \mid t) f_{X \mid \mu}(x \mid m)}{f_X(x)} 
\]

\[
\implies \E(\mu^T\mu \mid X) = \int_0^\infty t \Pr(\mu^T\mu = t \mid X) dt
\]

\[
\vdots
\]

Given \(\mu\), we have 

\[
X \mid \mu  \sim \mathcal{N} (\mu, \boldsymbol{I}_n) \implies (X - \mu)^T(X- \mu) \mid \mu \sim \chi_n^2 \iff X^TX - 2 \mu^TX + \mu^T\mu \mid \mu \sim \chi_n^2. 
\]

\[
\vdots
\]


Also, we have that \(\mu \sim \mathcal{N}(0, k \boldsymbol{I}_n)\). The joint distribution of \(X\) and \(\mu\) is then

\[
f_{X, \mu}(x, m) = f_{X \mid \mu = m}(x \mid m) f_\mu(m) = 
\]

% 3c
\item

% 3d
\item

\end{enumerate}

\end{exercise}

% Problem 4
\begin{exercise}[\textbf{High-Dimensional Statistics}]

\begin{enumerate}[(a)]

% 4a
\item \textbf{Sparsity in the covariance matrix does not imply sparsity in the precision matrix.} For example, suppose the covariance matrix is the following:

\[
\boldsymbol{\Sigma} := \begin{pmatrix}
(\tau^2  + 1) \boldsymbol{I}_n &  \boldsymbol{I}_n & \cdots & \boldsymbol{I}_n \\
\boldsymbol{I}_n  & (\tau^2 + 1)  \boldsymbol{I}_n  & \cdots &  \boldsymbol{I}_n \\
\vdots & \vdots &  \ddots & \vdots \\
\boldsymbol{I}_n & \boldsymbol{I}_n  &  \cdots & (\tau^2 + 1) \boldsymbol{I}_n
\end{pmatrix},
\]

This matrix is relatively sparse. However, its inverse is dense, with every entry nonzero:

\[
\boldsymbol{\Sigma} = \tau^2 \boldsymbol{I}_{ns} + \boldsymbol{1}_s\boldsymbol{1}_s^T \otimes \boldsymbol{I}_n =  \tau^2 \boldsymbol{I}_{ns} + \left(\boldsymbol{1}_s \otimes \boldsymbol{I}_n\right)\left(\boldsymbol{1}_s \otimes \boldsymbol{I}_n\right)^T
\]

Applying the Sherman-Morrison-Woodbury formula  with \(A = \tau^2 \boldsymbol{I}_{ns}\), \(U = \boldsymbol{1}_s \otimes \boldsymbol{I}_n\), \(C = \boldsymbol{I}_n \), and \(V = (\boldsymbol{1}_s \otimes \boldsymbol{I}_n) ^T\) yields

%\begin{theorem}[\textbf{Woodbury Matrix Identity} (or \textbf{Sherman-Morrison-Woodbury formula})] For \(A \in \mathbb{R}^{n \times n}\), \(U \in \mathbb{R}^{n \times k}\), \(C \in \mathbb{R}^{k \times k}\), and \(V \in \mathbb{R}^{v \times n}\),
%
%\[
%(A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}.
%\]
%
%\end{theorem}

\[
\boldsymbol{\Sigma}^{-1} = \frac{1}{\tau^2} \boldsymbol{I}_{ns} - \frac{1}{\tau^2} (\boldsymbol{1}_s \otimes \boldsymbol{I}_n) \left[ \boldsymbol{I}_n + (\boldsymbol{1}_s \otimes \boldsymbol{I}_n)^T \cdot \frac{1}{\tau^2} (\boldsymbol{1}_s \otimes \boldsymbol{I}_n) \right]^{-1}(\boldsymbol{1}_s \otimes \boldsymbol{I}_n)^T \cdot \frac{1}{\tau^2}
\] 

\[
= \frac{1}{\tau^2} \left( \boldsymbol{I}_{ns} - \left(\boldsymbol{1}_s \otimes \boldsymbol{I}_n\right) \left[ \tau^2 \boldsymbol{I}_n + \boldsymbol{1}_s^T\boldsymbol{1}_s \otimes \boldsymbol{I}_n \right]^{-1}\left(\boldsymbol{1}_s \otimes \boldsymbol{I}_n\right)^T \right)
\]

\[
= \frac{1}{\tau^2} \left( \boldsymbol{I}_{ns} - \left(\boldsymbol{1}_s \otimes \boldsymbol{I}_n\right) \left[ (\tau^2 + s)  \boldsymbol{I}_n \right]^{-1}\left(\boldsymbol{1}_s \otimes \boldsymbol{I}_n\right)^T \right)
\]

\[
= \frac{1}{\tau^2} \boldsymbol{I}_{ns} - \frac{1}{\tau^2(\tau^2 + s)} \boldsymbol{1}_s \boldsymbol{1}_s^T\otimes \boldsymbol{I}_n 
\]

\[
=  \begin{pmatrix}
\left( \frac{1}{\tau^2} - \frac{1}{\tau^2(\tau^2 + s)} \right) \boldsymbol{I}_n & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \cdots & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n \\
- \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \left( \frac{1}{\tau^2} - \frac{1}{\tau^2(\tau^2 + s)} \right) \boldsymbol{I}_n &  - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \cdots & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n \\
- \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n &  - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \left( \frac{1}{\tau^2} - \frac{1}{\tau^2(\tau^2 + s)} \right) \boldsymbol{I}_n  & \cdots & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
- \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n &  - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n  & \cdots  & \left( \frac{1}{\tau^2} - \frac{1}{\tau^2(\tau^2 + s)} \right) \boldsymbol{I}_n
\end{pmatrix}
\]

\[
=  \begin{pmatrix}
\frac{\tau^2+s - 1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \cdots & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n \\
- \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \frac{\tau^2+s-1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n &  - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \cdots & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n \\
- \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n &  - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \frac{\tau^2+s-1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n  & \cdots & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
- \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n &  - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n  & \cdots  & \frac{\tau^2+s-1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n
\end{pmatrix}.
\]


% 4b
\item If \(\omega_{jk}=0\), this means that features \(X_j\) and \(X_k\) are conditionally independent given all of the other features. (This is in contrast to the meaning of \(\sigma_{jk}= 0\), which is that \(X_j\) and \(X_k\) are unconditionally independent.) This interpretation also gives another answer for part (a) of this question: sparsity in the precision matrix does not necessarily imply sparsity in the covariance matrix (and vice versa) because conditional independence does not necessarily imply unconditional independence (and vice versa). 

\

By the same argument as in part (a), if \(\omega_{jk} = 0\) holds it does not necessarily hold that \(\sigma_{jk} =0\), since \(\boldsymbol{\Sigma}\) is the inverse of \(\boldsymbol{\Omega}\).

% 4c
\item \textbf{double-check that this actually works for \(p > n\). Also, consider instead using method from \citet{Fan2008}.} I would estimate the precision matrix using the graphical lasso \citep{Friedman2008}. Let \(\hat{\Sigma}\) be an estimate for the covariance matrix \(\Sigma\). Let \(S\) by the empirical covariance matrix; that is,

\[
S := \frac{1}{n} \sum_{i=1}^n (X^{(i)} - \overline{X})(X^{(i)} - \overline{X})^T
\]

where \(X^{(i)}\) is the \(i\)th row of \(X\) and \(\overline{X} = n^{-1}\sum_{i=1}^n X^{(i)}\). We will make use of the following partitions of \(\hat{\Sigma}\) and \(S\):

\begin{equation}\label{2018.screen.4.c.partitions}
\hat{\Sigma} = \begin{pmatrix} 
\hat{\Sigma}_{11} & \hat{\sigma}_{12} \\
\hat{\sigma}_{12}^T & \hat{\sigma}_{22}
\end{pmatrix}, \qquad S = \begin{pmatrix} 
S_{11} & S_{12} \\
S_{12}^T & S_{22}
\end{pmatrix}, \qquad \hat{\Omega} = \begin{pmatrix} 
\hat{\Omega}_{11} & \hat{\omega}_{12} \\
\hat{\omega}_{12}^T & \hat{\omega}_{22}
\end{pmatrix},
\end{equation}

as well as the constraint

\begin{equation}\label{2018.screen.4.c.constraint}
 \begin{pmatrix} 
\hat{\Sigma}_{11} & \hat{\sigma}_{12} \\
\hat{\sigma}_{12}^T & \hat{\sigma}_{22}
\end{pmatrix}   \begin{pmatrix} 
\hat{\Omega}_{11} & \hat{\omega}_{12} \\
\hat{\omega}_{12}^T & \hat{\omega}_{22}
\end{pmatrix}=\begin{pmatrix}
\boldsymbol{I}_{p-1} & 0 \\
0^T & 1
\end{pmatrix}
\end{equation}

suggested by \(\Sigma \Omega = \boldsymbol{I}_p\). We will optimize the following objective:

\begin{equation}\label{2018.screen.4.c.objective}
\hat{\Omega} := \underset{\Omega \in \mathcal{S}^+}{\arg \max} \left\{ \log \det (\Omega) - \Tr(S \Omega) + \lambda \lVert \Omega \rVert_1 \right\}
\end{equation}

where \( \mathcal{S}^+\) is the set of nonnegative definite \(p \times p\) matrices and \(\lambda > 0\) is a penalty parameter. The proposed procedure to optimize (\ref{2018.screen.4.c.objective}) is as follows:

\begin{enumerate}[1.]

\item Initialize the algorithm with estimate \(\hat{\Sigma} = S + \lambda \boldsymbol{I}_p\). (The diagonal of \(\hat{\Sigma}\) remains unchanged for the rest of the algorithm.)

\item For each \(j = 1, 2, \ldots, p, 1, 2, \ldots, p, \ldots, \) switch the rows and columns of \(\hat{\Sigma}\) so that the row and column corresponding to feature \(j\) come last, as in partition  (\ref{2018.screen.4.c.partitions}). Then solve the lasso problem

\begin{equation}
\hat{\beta} = \underset{\beta \in \mathbb{R}^{p-1}}{\arg \min} \left\{ \frac{1}{2} \lVert \hat{\Sigma}_{11}^{1/2} \beta - \hat{\Sigma}_{11}^{-1/2}s_{12}  \rVert_2^2 + \lambda \lVert \beta \rVert_1 \right\}
\end{equation}

Note that this problem takes as input the inner products \(\hat{\Sigma}_{11}\) and \(s_{12}\). 

\item Fill in the corresponding row and column of \(\hat{\Sigma}\) using \(\hat{\sigma}_{12} = \hat{\Sigma}_{11} \hat{\beta}\). (Again, the diagonal term \(\hat{\sigma}_{22}\) remains as it was after step 1.)

\item Continue until convergence; that is, until the average absolute change in \(\hat{\Sigma}\) is less than \(t \cdot \operatorname{ave} | S^{- \text{diag}} |\), where \( S^{- \text{diag}}\) are the off-diagonal elements of \(S\) and \(t\) is a fixed threshold (\(t = 0.001\) is recommended by \citep{Friedman2008}).

\item Estimate \(\hat{\Omega}\) by using \(\hat{\Sigma}\) to compute \(\hat{\omega}_{22}\) for each feature and filling in the corresponding row of \(\hat{\Omega}\) as in (\ref{2018.screen.4.c.partitions}) using the formulae

\begin{equation}\label{2018.screen.4.c.precision.matrix.formula}
\hat{\omega}_{22} = 1/ \left( \hat{\sigma}_{22} - \hat{\sigma}_{12}^T \hat{\beta}\right), \qquad \hat{\omega}_{12} = - \hat{\beta}\hat{\omega}_{22}.
\end{equation}

\end{enumerate}

Formulae (\ref{2018.screen.4.c.precision.matrix.formula}) are justified as follows: from (\ref{2018.screen.4.c.constraint}) we have the following identities

\[
\hat{\Sigma}_{11} \hat{\omega}_{12} + \hat{\sigma}_{12} \hat{\omega}_{22} = 0, \qquad \hat{\sigma}_{12}^T \hat{\omega}_{12} + \hat{\sigma}_{22} \hat{\omega}_{22} = 1.
\]

These yield

\[
\hat{\omega}_{12} = - \hat{\Sigma}_{11}^{-1} \hat{\sigma}_{12} \hat{\omega}_{22}, \qquad \hat{\omega}_{22} = 1/ \left( \hat{\sigma}_{22} - \hat{\sigma}_{12}^T \hat{\Sigma}_{11}^{-1} \hat{\sigma}_{12} \right).
\]

Then using \(\hat{\sigma}_{12} = \hat{\Sigma}_{11} \hat{\beta} \iff \hat{\beta} =  \hat{\Sigma}_{11} ^{-1} \hat{\sigma}_{12}\), we have (\ref{2018.screen.4.c.precision.matrix.formula}).


% 4d
\item

\end{enumerate}

\end{exercise}

% Problem 5
\begin{exercise}[\textbf{Optimization}]

\begin{enumerate}[(a)]

% 5a
\item We can express the original optimization problem

\begin{equation}\label{2018.screen.5.a.objective}
\begin{aligned}
& \underset{\beta \in \mathbb{R}^p}{\text{minimize}}
& & \frac{1}{2} \lVert y - X \beta \rVert_2^2 + \lambda \lVert \beta \rVert_1
\end{aligned}
\end{equation}

as 

\begin{equation}\label{2018.screen.5.a.objective.alt}
\begin{aligned}
& \underset{\beta \in \mathbb{R}^p, z \in \mathbb{R}^n}{\text{minimize}}
& & \frac{1}{2} \lVert y - z \rVert_2^2 + \lambda \lVert \beta \rVert_1 \\
& \text{subject to}
& & z = X \beta.
\end{aligned}
\end{equation}

We will also refer to another expression of the lasso optimization problem,

\begin{equation}\label{2018.screen.5.a.objective.orig}
\begin{aligned}
& \underset{\beta \in \mathbb{R}^p}{\text{minimize}}
& & \frac{1}{2} \lVert y - X \beta \rVert_2^2 \\
& \text{subject to}
& & \lVert\beta \rVert_1 \leq t
\end{aligned}
\end{equation}

for some \(t >0\). The Lagrangian of (\ref{2018.screen.5.a.objective.alt}) is

\[
\mathcal{L}(\beta, z, \nu) = \frac{1}{2} \lVert y - z \rVert_2^2 + \lambda \lVert \beta \rVert_1 + \nu^T(z - X \beta),
\]

so the Lagrange dual function is

\[
\inf_{\beta, z} \left\{ \mathcal{L}(x, \nu)\right\}  = \inf_{\beta, z} \left\{\frac{1}{2} \lVert y - z \rVert_2^2 + \lambda \lVert \beta \rVert_1 + \nu^T(z - X \beta)  \right\}
\]

\[
= \inf_{\beta, z} \left\{\frac{1}{2} (y-z)^T(y-z) + \nu^T z + \lambda \lVert \beta \rVert_1  - \nu^T X \beta  \right\} 
\]

This minimization is separable:

\begin{equation}\label{2018.screen.5.a.a}
= \inf_{z} \left\{\frac{1}{2} \left(y^Ty - 2 y^Tz + z^Tz \right) + \nu^T z \right\} + \inf_{\beta} \left\{ \lambda \lVert \beta \rVert_1  - \nu^T X \beta  \right\}
\end{equation}

We will handle each part of (\ref{2018.screen.5.a.a}) separately. First, the left side:

\[
 \inf_{z} \left\{\frac{1}{2} \left(y^Ty - 2 y^Tz + z^Tz \right) + \nu^T z \right\} = \inf_{z} \left\{\frac{1}{2}z^Tz  + (\nu - y)^Tz + \frac{1}{2} y^Ty   \right\} 
\]

Since this is a convex quadratic form, differentiate with respect to \(z\) and set equal to zero:

\begin{equation}\label{other.part.result}
z + (\nu - y) = 0 \implies z = y - \nu
\end{equation}

\[
 \implies \inf_{z} \left\{\frac{1}{2}z^Tz  + (\nu - y)^Tz + \frac{1}{2} y^Ty   \right\} =  \frac{1}{2}(y - \nu) ^T(y - \nu)  + (\nu - y)^T(y - \nu) + \frac{1}{2} y^Ty 
\]

\[
=  \frac{1}{2}\left(y^Ty -2 \nu^Ty + \nu^T\nu \right)  + 2\nu^Ty - y^Ty - \nu^T\nu + \frac{1}{2} y^Ty  = -\frac{1}{2}\nu^T\nu   + \nu^Ty = \frac{1}{2} y^Ty - \frac{1}{2}y^Ty  + \nu^Ty - \frac{1}{2} \nu^T\nu 
\]

\[
= \frac{1}{2} y^Ty - \frac{1}{2}(y^Ty - 2\nu^Ty + \nu^T\nu ) = \frac{1}{2} y^Ty - \frac{1}{2}(y - \nu)^T(y - \nu)= \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert y - \nu \rVert_2^2 
\]

Next we will minimize the right side of (\ref{2018.screen.5.a.a}):

\[
 \inf_{\beta} \left\{ \lambda \lVert \beta \rVert_1  - \nu^T X \beta  \right\} =  \inf_{\beta} \left\{ \lambda \sum_{i=1}^p | \beta_i| - \sum_{i=1}^p \begin{bmatrix} \nu^T X \end{bmatrix}_i  \beta_i  \right\}  =  \inf_{\beta} \left\{ \sum_{i=1}^p  \left( \lambda  | \beta_i| -  \begin{bmatrix} \nu^T X \end{bmatrix}_i  \beta_i \right)  \right\} 
 \]
 
% \[
% = \begin{cases}
%   \inf_{\beta} \left\{ \sum_{i=1}^p  \left( \lambda  | \beta_i| -  \begin{bmatrix} \nu^T X \end{bmatrix}_i  \beta_i \right)  \right\} & \beta_i 
%   \end{cases}
% \]
 
 \[
 =  \inf_{\beta} \left\{ \sum_{i=1}^p  \left((-1)^{\mathbb{I}{\{\beta_i < 0\}}} \lambda-  \begin{bmatrix} \nu^T X \end{bmatrix}_i  \right)    \beta_i  \right\}   =  \sum_{i=1}^p  \inf_{\beta_i} \left\{ \left((-1)^{\mathbb{I}{\{\beta_i < 0\}}} \lambda-  \begin{bmatrix} \nu^T X \end{bmatrix}_i  \right)    \beta_i  \right\} 
 \]
 
where \(\mathbb{I}{\{\beta_i < 0\}}\) is an indicator function. Notice that when \(\beta_i\) is negative, if \(\left((-1)^{\mathbb{I}{\{\beta_i < 0\}}} \lambda-  \begin{bmatrix} \nu^T X \end{bmatrix}_i  \right) =  -\left(\lambda+  \begin{bmatrix} \nu^T X \end{bmatrix}_i  \right)\) is positive there is no lower bound on the quantity we are minimizing; otherwise, when \(\beta_i\) is negative the infimum is 0. When \(\beta_i\) is positive, if  \(\left((-1)^{\mathbb{I}{\{\beta_i < 0\}}} \lambda-  \begin{bmatrix} \nu^T X \end{bmatrix}_i  \right) =  \left(\lambda-  \begin{bmatrix} \nu^T X \end{bmatrix}_i  \right)\) is negative there is no lower bound on the quantity we are minimizing; otherwise, when \(\beta_i\) is negative the infimum is 0. That is, the only dual feasible points satisfy for all \(i\)

\[
  -\left(\lambda+  \begin{bmatrix} \nu^T X \end{bmatrix}_i  \right) \leq 0, \qquad \lambda-  \begin{bmatrix} \nu^T X \end{bmatrix}_i   \geq 0 \iff \begin{bmatrix} \nu^TX\end{bmatrix}_i \geq -\lambda, \qquad \begin{bmatrix} \nu^TX\end{bmatrix}_i \leq \lambda
\]

which is equivalent to the condition
\[
\lVert \nu^TX \rVert_\infty \leq \lambda.
\]

Therefore the Lagrange dual function is

%The function we are minimizing is linear in \(\beta\) whenever \(\beta_i < 0 \) for any \(i\), so it decreases without bound if there exists any \(i\) such that \(\beta_i < 0\) and \(\lambda-  \begin{bmatrix} \nu^T X \end{bmatrix}_i  < 0\). That is, the infimum does not exist unless for all \(i \in \{1, \ldots, p\}\)

%\[
%\lambda-  \begin{bmatrix} \nu^T X \end{bmatrix}_i  \geq 0 \iff \lambda \geq  \begin{bmatrix} \nu^T X \end{bmatrix}_i
%\]
 

%The function we are minimizing is convex in \(\beta\). Differentiate and set equal to 0:
%
%\[
%\lambda \begin{bmatrix} \operatorname{sgn}(\beta_i) \end{bmatrix} - \nu^T X = 0 \iff \begin{bmatrix} \operatorname{sgn}(\beta_i) \end{bmatrix}  = \frac{1}{\lambda} \nu^T X 
%\]
%
%where \(\begin{bmatrix} \operatorname{sgn}(\beta_i) \end{bmatrix} \) denotes the vector resulting from operating the \(\operatorname{sgn}(\cdot)\) function elementwise on \(\beta\).
% 
% \[
% \vdots
% \]

\begin{equation}\label{2018.screen.5.a.dual}
 \inf_{\beta, z} \left\{ \mathcal{L}(x, \nu)\right\}  = \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert y - \nu \rVert_2^2 
\end{equation}

subject to the constraint

\[
\lVert \nu^TX \rVert_\infty \leq \lambda.
\]


This quantity represents a lower bound on the minimum value of the original optimization problem for all \(\nu \in \mathbb{R}^p\). The dual problem is to find the best lower bound by maximizing over \(\nu\); that is, the dual problem is

\[
\begin{aligned}
& \underset{\nu \in \mathbb{R}^p}{\text{maximize}}
& & \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert y - \nu \rVert_2^2  \\
& \text{subject to}
& & \lVert \nu^TX \rVert_\infty \leq \lambda.
\end{aligned}
\]

\textbf{still need to finish; actually trivial based on (\ref{other.part.result}):} Lastly, suppose \(\hat{\beta}\) and \(\hat{\nu}\) satisfy

\[
\hat{\beta} = 
\begin{aligned}
& \underset{\beta \in \mathbb{R}^p}{\arg \min}
& & \frac{1}{2} \lVert y - X \beta \rVert_2^2 + \lambda \lVert \beta \rVert_1
\end{aligned},
\]

\[
 \qquad \hat{\nu} = 
\begin{aligned}
& \underset{\nu \in \mathbb{R}^p}{\arg \max}
& & \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert y - \nu \rVert_2^2  \\
& \text{subject to}
& & \lVert \nu^TX \rVert_\infty \leq \lambda
\end{aligned} = 
\begin{aligned}
& \underset{\nu \in \mathbb{R}^p}{\arg \min}
& & -\frac{1}{2} \lVert y \rVert_2^2 + \frac{1}{2} \lVert y - \nu \rVert_2^2  \\
& \text{subject to}
& & \lVert \nu^TX \rVert_\infty \leq \lambda
\end{aligned}
\]

\[
\vdots
\]

% 5b
\item

\begin{enumerate}[(i)]

% 5(b)i
\item \textbf{Not necessarily unique.} Per \citet{Tibshirani2013}, if \(\operatorname{rank}(X) < p\), the lasso solution is not necessarily unique. Intuitively, this is because the columns of \(X\) are linearly dependent, so there may exist more than one linear combination of the columns that minimizes (\ref{2018.screen.5.a.objective}). \textbf{jacob suggestion: counterexample. X is two columns that are equal; then convex combinations of two solutions are equal as long as same sign (can't be opposite sign because then \(\ell_1\) could be smaller by setting one equal to 0.}

% 5(b)ii
\item \textbf{Necessarily unique.} \textbf{flesh out more} By a result from part (a), \(\hat{u} = y - X \hat{\beta}\). By part (iii), even though \(\hat{\beta}\) is not unique, \(X \hat{\beta}\) is (see also Lemma 1 in \citet{Tibshirani2013}). Therefore \(\hat{u}\) is unique.

\textbf{Jacob's solution: strictly convex optimization problem, so argument maximizing is unique. dual is always convex; } 

% 5(b)iii
\item \textbf{Necessarily unique} (except in the trivial case \(\lambda=0\)). Per part 5(b)(iv), \(\lVert \hat{\beta} \rVert_1\) is unique. (\ref{2018.screen.5.a.objective}) is convex, so the minimum \(\frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \hat{\beta} \rVert_1\) is unique. Therefore \( \lVert y - X \hat{\beta} \rVert_2^2\) must be unique.

\textbf{Jacob's solution: if \(\hat{\nu}\) is unique then its \(\ell_2\) norm is unique.}

%\textbf{Necessarily unique.} The optimization problem is convex in \(\beta\), so it has a unique minimum value \(\frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \hat{\beta} \rVert_1\). 

% 5(b)iv
\item \textbf{Necessarily unique} \textbf{change this argument: if you solve 1 and take the norm of that and choose that value for \(t\) then you will get the same solution. because if there existed a better solution then it would have made the objective function 1 better.} (except in the trivial case \(\lambda=0\)). Whenever \(\lambda > 0\), the lasso objective function (\ref{2018.screen.5.a.objective}) is the dual of (\ref{2018.screen.5.a.objective.orig}) with \(t\) less than the \(\ell_1\) norm of the OLS solution (if it exists). (\ref{2018.screen.5.a.objective.orig}) is convex and Slater's condition holds because every point \(\{\beta \in \mathbb{R}^p \mid \lVert \beta \rVert_1 < t\}\) is feasible. Therefore for this dual function, strong duality holds; that is, the optimal values of (\ref{2018.screen.5.a.objective}) and  (\ref{2018.screen.5.a.objective.orig}) are equal. Further, they are optimized over the same variable and they are both convex, so the solution set of (\ref{2018.screen.5.a.objective}) is identical to the solution set of (\ref{2018.screen.5.a.objective.orig}).

Since the objective function of (\ref{2018.screen.5.a.objective.orig}) is continuous and the feasible region \(\lVert \beta \rVert_1 \leq t\) is compact, a minimum is guaranteed to exist. Since (\ref{2018.screen.5.a.objective.orig}) is convex and the global minimum lies outside the region \textbf{for \(t < t_0\)}, the minimum will lie on the boundary; that is, \(\lVert \hat{\beta} \rVert_1 = t\) for (\ref{2018.screen.5.a.objective.orig}) and therefore for (\ref{2018.screen.5.a.objective}). See \citet{Osborne2000} for details.

\end{enumerate}

% 5c
\item 

\begin{enumerate}[(i)]

% 5(c)i
\item Since \(\beta^*\) is clearly feasible for (\ref{2018.screen.5.a.objective}) and \(\hat{\beta}\) achieves the minimum, we have

\[
\frac{1}{2} \lVert y - X \beta^* \rVert_2^2 + \lambda \lVert \beta^* \rVert_1 \geq \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \hat{\beta} \rVert_1 \iff  \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \hat{\beta} \rVert_1 \leq \frac{1}{2} \lVert\epsilon \rVert_2^2 + \lambda \lVert \beta^* \rVert_1
\]


% 5(c)ii
\item From part (a), since the optimal value of the dual (\ref{2018.screen.5.a.dual}) is a lower bound for the optimal value of the primal (\ref{2018.screen.5.a.objective}), we have

\begin{equation}\label{2018.screen.5.c.d}
 \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert y - \hat{\nu} \rVert_2^2  \leq \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \hat{\beta} \rVert_1.
\end{equation}

\textbf{Jacob's solution: plug in \(\epsilon\) for \(\nu\) in the lower bound equation, then you get \(X \beta^*\)}

so we are done if \(\lVert X \beta^* \rVert_2^2 \geq \lVert y - \hat{\nu} \rVert_2^2\). Also by part (a), \(\hat{\nu} = y - X \hat{\beta}\), so \( \lVert y - \hat{\nu} \rVert_2^2 = \lVert X \hat{\beta} \rVert_2^2 \), so we are done if \(\lVert X \beta^* \rVert_2^2 \geq \lVert X \hat{\beta} \rVert_2^2 \). From part (c)(i), we have

\[
 \frac{1}{2}\left[  ( y - X \hat{\beta})^T( y - X \hat{\beta})  - ( y - X \beta^*)^T( y - X \beta^*)  \right] \leq \lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1)
\]

\[
\iff 2 y^T X (\beta^* -  \hat{\beta} ) +  (\hat{\beta})^TX^TX\hat{\beta} - (\beta^*)^TX^TX\beta^* \leq 2\lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1)
\]

\begin{equation}\label{2018.screen.5.c.c}
\iff \lVert X \beta^* \rVert_2^2  -  \lVert X \hat{\beta} \rVert_2^2   \geq  2 y^T X ( \beta^* - \hat{\beta} )  - 2\lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1 ) 
\end{equation}

So (\ref{2018.screen.5.c.c}) is a sufficient condition for the result. Examining the right side of the right side of (\ref{2018.screen.5.c.c}), we have

\begin{equation}\label{2018.screen.5.c.a}
2 y^T X ( \beta^* - \hat{\beta} )  - 2\lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1 ) =  2y^T (y - \epsilon -X \hat{\beta} ) - 2\lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1)  
\end{equation}


%Again, note that \(\lambda > 0\) corresponds to a problem with constraint \(\lVert \beta \rVert_1 \leq t \leq \lVert \beta^*\rVert\) (see the explanation in problem 5(b)(iv)). So \(\lVert \hat{\beta} \rVert_1 = t \leq \lVert \beta^*\rVert\). 

Borrowing notation from \citet{Osborne2000}, note that the subdifferential of (\ref{2018.screen.5.a.objective}) is given by

\[
\partial_\beta \mathcal{L}(\beta, \lambda) = \partial_\beta  \left(  \frac{1}{2}(y - X \beta)^T(y - X \beta) + \lambda \lVert \beta \rVert_1\right) = \partial_\beta  \left(  \frac{1}{2}(y^Ty - 2 y^TX \beta + \beta^T X^T X \beta) + \lambda \lVert \beta \rVert_1\right) 
\]

\[
= -y^TX + X^TX \beta+ \lambda v = -X^T(y - X \beta) + \lambda v
\]

where \(v = (v_1, \dots, v_p)^T\) with \(v_i = \operatorname{sgn}(\beta_i)\) if \(\beta_i \neq 0\) and \(v_i \in [-1, 1]\) if \(\beta_i  =0\). By the convexity of (\ref{2018.screen.5.a.objective}), for \(\hat{\beta}\) minimizing (\ref{2018.screen.5.a.objective}) it must hold that 

\[
0 = -X^T(y - X \beta) + \lambda v \iff  \lambda v^T \hat{\beta} = (y - X \hat{\beta}) ^TX \hat{\beta} \iff \lambda = \frac{(y - X \hat{\beta})^TX \hat{\beta}}{\lVert \hat{\beta} \rVert_1}
\]

\begin{equation}\label{2018.screen.5.c.b}
\iff \lVert \hat{\beta} \rVert_1 = \frac{(y - X \hat{\beta})^TX \hat{\beta}}{\lambda}
\end{equation}

where we used \(v^T \hat{\beta} = \lVert \hat{\beta} \rVert_1 \). Substituting (\ref{2018.screen.5.c.b}) into (\ref{2018.screen.5.c.a}), we have
%
%Substituting this identity and (\ref{2018.screen.5.c.b}) into (\ref{2018.screen.5.c.d}), we have that a sufficient condition to prove our result is 
%
%\[
% \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert y - \hat{\nu} \rVert_2^2  \leq \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + (y - X \hat{\beta})^TX \hat{\beta}
% \]
% 
% \[
%\iff  \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert X \hat{\beta} \rVert_2^2  \leq \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + (y - X \hat{\beta})^TX \hat{\beta}
% \]
% 
%  \[
%\iff  \frac{1}{2} y^Ty - \frac{1}{2} ( \hat{\beta})^T X^TX \hat{\beta}  \leq \frac{1}{2} \left(y^Ty - 2 y^T X \hat{\beta} + (\hat{\beta})^TX^TX\hat{\beta} \right) +y^T X \hat{\beta} - (\hat{\beta})^TX^T X \hat{\beta}
% \]
% 
%   \[
%\iff 0 \leq \frac{1}{2} \left(- 2 y^T X \hat{\beta}  \right) +y^T X \hat{\beta}  = 0
% \]
 

%\[
%\vdots
%\]



\[
2y^T (y - \epsilon -X \hat{\beta} ) -  2\lambda \left(\lVert \beta^* \rVert_1 - \frac{(y - X \hat{\beta})^TX \hat{\beta}}{\lambda} \right)  
\]

\[
=2 y^T y - 2y^T \epsilon - 2y^TX \hat{\beta}  -2 \lambda\lVert \beta^* \rVert_1  + 2y^TX \hat{\beta}  -2  \hat{\beta}^T X^T X \hat{\beta} = 2y^T (y -  \epsilon)  -2 \lambda\lVert \beta^* \rVert_1   -  2\hat{\beta}^T X^T X \hat{\beta}
\]

\[
= 2y^T X \beta^* - 2\lambda\lVert \beta^* \rVert_1   -  2\lVert X \hat{\beta} \rVert_2^2  =  2(X \beta^* + \epsilon)^T X \beta^*  - 2 \lambda\lVert \beta^* \rVert_1   - 2\lVert X \hat{\beta} \rVert_2^2
\]

\[
= 2 \lVert X \beta^* \rVert_2^2 + 2\epsilon^T X \beta^* - 2\lambda\lVert \beta^* \rVert_1   - 2 \lVert X \hat{\beta} \rVert_2^2  = 2 \left( \lVert X \beta^* \rVert_2^2  - \lVert X \hat{\beta} \rVert_2^2\right)  - 2 \left( \lambda\lVert \beta^* \rVert_1  -   \epsilon^T X \beta^* \right)
\]

\[
= 2 \left( \lVert X \beta^* \rVert_2^2  - \lVert X \hat{\beta} \rVert_2^2\right)  - 2 \sum_{i=1}^p \left((-1)^{\mathbb{I}{\{\beta_i < 0\}}} \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i \right) \beta_i^* 
\]

Because \(\lambda \geq \lVert X^T \epsilon \rVert_\infty\) (that is, for all \(i \in \{1, \ldots, p\}\), \(\lambda -  \left| \begin{bmatrix} X^T \epsilon \end{bmatrix}_i \right| \geq 0 \) ) we have that for all \(i \in \{1, \ldots, p\}\),


%\(\epsilon^T X \preceq \lambda \boldsymbol{1}^T \)

\[
 \left((-1)^{\mathbb{I}{\{\beta_i < 0\}}} \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i \right) \beta_i^*  \geq 0
 \]
 
 by the following argument. If \(\beta_i^* = 0\), the result is trivial. If \(\beta_i^* > 0\), we have
 
 \[
  (-1)^{\mathbb{I}{\{\beta_i < 0\}}} \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i  =   \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i  \geq \lambda -  \left| \begin{bmatrix} X^T \epsilon \end{bmatrix}_i \right|  \geq 0
  \]
  
  \[
   \implies  \left((-1)^{\mathbb{I}{\{\beta_i < 0\}}} \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i \right) \beta_i^*  \geq 0.
  \]
  
  Lastly, if \(\beta_i^* < 0\), we have
  
   \[
  (-1)^{\mathbb{I}{\{\beta_i < 0\}}} \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i  =   - \left( \lambda +   \begin{bmatrix} \epsilon^T X \end{bmatrix}_i \right)  \leq - \left( \lambda -  \left| \begin{bmatrix} X^T \epsilon \end{bmatrix}_i \right|  \right) \leq 0
  \]
  
  \[
   \implies  \left((-1)^{\mathbb{I}{\{\beta_i < 0\}}} \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i \right) \beta_i^*  \geq 0.
  \]
  
  Therefore we have
  
  \[
2 \left( \lVert X \beta^* \rVert_2^2  - \lVert X \hat{\beta} \rVert_2^2\right)  - 2 \sum_{i=1}^p \left((-1)^{\mathbb{I}{\{\beta_i < 0\}}} \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i \right) \beta_i^*  \leq 2 \left( \lVert X \beta^* \rVert_2^2  - \lVert X \hat{\beta} \rVert_2^2\right) 
\]

\[
\vdots
\]

\[
\lVert X \beta^* \rVert_2^2  -  \lVert X \hat{\beta} \rVert_2^2   \geq  2 y^T X ( \beta^* - \hat{\beta} )  - 2\lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1 ) 
\]

\[
\iff \lVert X \beta^* \rVert_2^2  -  \lVert X \hat{\beta} \rVert_2^2   \geq  2 (X \beta^* + \epsilon)^T X \beta^* - 2 y^T X\hat{\beta}   - 2\lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1 ) 
\]

\[
\iff \lVert X \beta^* \rVert_2^2  -  \lVert X \hat{\beta} \rVert_2^2   \geq   2\lVert X \beta^* \rVert_2^2 + 2 \epsilon^T X \beta^* - 2 y^T X\hat{\beta}   - 2\lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1 ) 
\]

%\[
%\epsilon^T X \preceq \lambda \boldsymbol{1}^T  \iff  \ldots \iff  \sum_{i=1}^p \left((-1)^{\mathbb{I}{\{\beta_i < 0\}}} \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i \right) \beta_i^* 
%\]
%
%\[
%\iff | \epsilon^T X \beta^* | \leq | \lambda \boldsymbol{1}^T \beta^* |= \lambda \lVert \beta^* \rVert_1.
%\]


% Therefore \(\lambda\lVert \beta^* \rVert_1  -  \epsilon^T X \beta^* \geq 0\), so we have that \(\lVert X \beta^* \rVert_2^2 \geq  \lVert X \hat{\beta} \rVert_2^2\) is a sufficient condition to prove the result.

%which proves the result by (\ref{2018.screen.5.c.d}).

%\[
%\vdots
%\]
%
%Further, if \(\hat{\beta} \neq 0\) then \(\lVert v \rVert_\infty = 1\), so from (\ref{2018.screen.5.c.b}) we have
%
%\[
%\lambda = \frac{(y - X \hat{\beta})^TX \hat{\beta}}{\lVert \hat{\beta} \rVert_1} = \lVert X^T(y - X \hat{\beta}) \rVert_\infty.
%\]
%
%\[
%\vdots
%\]

% \(\lVert y - 

% 5(c)iii
\item \textbf{haven't finished, but this is just a bonus question} We already have from part (c)(i)

\[
\frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \hat{\beta} \rVert_1 \leq \frac{1}{2} \lVert\epsilon \rVert_2^2 + \lambda \lVert \beta^* \rVert_1
\]

Note that

\[
\frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \hat{\beta} \rVert_1 \leq \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \beta^* \rVert_1 
\]

% 5(c)iv
\item  \textbf{haven't finished, but this is just a bonus question} 

\end{enumerate}

% 5d

\item 

\begin{proof}

\

\begin{center}
\noindent\fbox{
\parbox{0.9\textwidth}{

\begin{definition}[\textbf{Convex function in \(\mathbb{R}^n\)}]\label{cvx.defn.convex.multivar} Let $f:\mathbb{R}^n\to\mathbb{R}$.  We say that $f$ is \textbf{convex} if, for any $x,y\in\mathbb{R}^n$ and for any $t\in[0,1]$, we have

\begin{equation}\label{cvx.541a.hw6.5a}
f(tx+(1-t)y)\leq tf(x)+(1-t)f(y).
\end{equation}

\end{definition}

}
}
\end{center}

\

%If \(\theta \in \mathbb{R}\), we can simply write
%
%\[
%f(tx+(1-t)y) = \left( \lVert tx+(1-t)y \rVert_1 \right)^2 = \left(tx+(1-t)y \right)^2 = t^2x^2 + (1-t)^2y^2 + 2 t(1-t) xy 
%\]
%
%%Since \(t \in [0,1]\) and \(1-t \in [0,1]\), \(t \leq \sqrt{t} \) and \(1-t \leq \sqrt{1-t}\). 
%%
%%\[
%%\leq \ldots  =  \left( \sqrt{t} x\right)^2 +  \left( \sqrt{1-t}  y \right)^2  = t \left( \lVert x \rVert_1 \right)^2 + (1-t) \left(\lVert y \rVert_1 \right)^2 =  tf(x)+(1-t)f(y).
%%\]
%
%Further,
%
%\[
% tf(x)+(1-t)f(y) = t \left( \lVert x \rVert_1 \right)^2 + (1-t) \left(\lVert y \rVert_1 \right)^2  = tx^2 + (1-t)y^2.
%\]
%
%Taking the difference of these yields
%
%\[
% tf(x)+(1-t)f(y)  - f(tx+(1-t)y) = tx^2 + (1-t)y^2 -  \left( t^2x^2 + (1-t)^2y^2 + 2 t(1-t) xy \right) 
%\]
%
%\[
%= (t - t^2) x^2 + [ (1 - t) - (1-t)^2] y^2 - 2 t(1-t) xy = (t - t^2) x^2 + [ (1 - t) - (1-2t + t^2)] y^2 - 2 t(1-t) xy
%\]
%
%\[
%= (t - t^2) x^2 + ( t - t^2) y^2 - 2 (t-t^2) xy = t(1-t)(x^2 + y^2 - 2xy) = t(1-t)(x - y)^2 \geq 0 
%\]
%
%\[
%\iff  tf(x)+(1-t)f(y) \geq f(tx+(1-t)y)
%\]
%
%since \(t \geq 0, 1-t \geq 0\), and \((x-y)^2 \geq 0\) for all \(x, y \in \mathbb{R}\). So \(\left( \lVert \theta \rVert_1 \right)^2\) is convex if \(\theta \in \mathbb{R}\). If \(\theta \in \mathbb{R}^n\), \(n  \geq 2\), 

Note that

\begin{equation}\label{2018.screen.5.d.a}
\lVert tx+(1-t)y \rVert_1 \leq \lVert tx \rVert_1  + \lVert (1-t)y \rVert_1 = t \lVert x \rVert_1  + (1-t) \lVert y \rVert_1 
\end{equation}

where the first step follows by the Triangle Inequality (which all norms satisfy, including the \(\ell_1\) norm) and the second step follows by the homogeneity property of norms. Therefore \(\lVert \theta \rVert_1\) is convex. Next, by (\ref{2018.screen.5.d.a}) and the monotonicity of \(g(\theta) = \theta^2\) when \(\theta \geq 0\),

%Note that (\ref{2018.screen.5.d.a}) holds with equality if \(x \succeq \boldsymbol{0}_n\) and \(y \succeq \boldsymbol{0}_n\) or \(x \preceq \boldsymbol{0}_n\) and \(y \preceq \boldsymbol{0}_n\). If either of those conditions are met, we have

\[
f(tx+(1-t)y) = \left( \lVert tx+(1-t)y \rVert_1 \right)^2 \leq \left(  t \lVert x \rVert_1  + (1-t) \lVert y \rVert_1  \right)^2 
\]

\[
= t^2 \lVert x \rVert_1^2 + (1-t)^2 \lVert y \rVert_1^2 +2t(1-t) \lVert x \rVert_1 \lVert y \rVert_1 
\]

and

\[
tf(x) + (1-t)f(y) = t \lVert x \rVert_1^2 + (1-t) \lVert y \rVert_1^2
\]

Taking the difference of these yields

\[
tf(x) + (1-t)f(y)  - f(tx+(1-t)y)  \geq t \lVert x \rVert_1^2 + (1-t) \lVert y \rVert_1^2 - \left(  t^2 \lVert x \rVert_1^2 + (1-t)^2 \lVert y \rVert_1^2 +2t(1-t) \lVert x \rVert_1 \lVert y \rVert_1  \right)
\]

\[
=( t - t^2) \lVert x \rVert_1^2 + [(1-t) - (1-t)^2] \lVert y \rVert_1^2 - 2t(1-t)\lVert x \rVert_1 \lVert y \rVert_1 
\]

\[
=(t - t^2)\left( \lVert x \rVert_1^2 + \lVert y \rVert_1^2 - 2\lVert x \rVert_1 \lVert y \rVert_1 \right) = t(1-t)(\lVert x \rVert_1 - \lVert y \rVert_1)^2 \geq 0
\]

\[
\iff  tf(x) + (1-t)f(y)  \geq f(tx+(1-t)y)  
\]

which proves convexity.

\end{proof}

\end{enumerate}

\end{exercise}

\bibliographystyle{abbrvnat}
\bibliography{mybib2fin}
\end{document}