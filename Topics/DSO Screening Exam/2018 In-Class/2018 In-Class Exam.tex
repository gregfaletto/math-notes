\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage[document]{ragged2e}
\usepackage{textcomp}
% \usepackage{amssymb}
\usepackage{import}
\usepackage{natbib}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\: \hmwkTitle}
%\rhead{\firstxmark}
%\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

%\newcommand{\enterProblemHeader}[1]{
%    \nobreak\extramarks{}{Question \arabic{#1} continued on next page\ldots}\nobreak{}
%    \nobreak\extramarks{Question \arabic{#1} (cont.)}{Question \arabic{#1} continued on next page\ldots}\nobreak{}
%}
%
%\newcommand{\exitProblemHeader}[1]{
%    \nobreak\extramarks{Question \arabic{#1} (cont.)}{Question \arabic{#1} continued on next page\ldots}\nobreak{}
%    \stepcounter{#1}
%    \nobreak\extramarks{Question \arabic{#1}}{}\nobreak{}
%}
%
%\setcounter{secnumdepth}{0}
%\newcounter{partCounter}
%\newcounter{homeworkProblemCounter}
%\setcounter{homeworkProblemCounter}{1}
%\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%%
%% Homework Problem Environment
%%
%% This environment takes an optional argument. When given, it will adjust the
%% problem counter. This is useful for when the problems given for your
%% assignment aren't sequential. See the last 3 problems of this template for an
%% example.
%%
%\newenvironment{homeworkProblem}[1][-1]{
%    \ifnum#1>0
%        \setcounter{homeworkProblemCounter}{#1}
%    \fi
%    \section{Problem \arabic{homeworkProblemCounter}}
%    \setcounter{partCounter}{1}
%    \enterProblemHeader{homeworkProblemCounter}
%}{
%    \exitProblemHeader{homeworkProblemCounter}
%}

\theoremstyle{definition}
\newtheorem{exercise}{Exercise}
\newtheorem{definition}{Definition}
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{corollary}{Corollary}[theorem]
\theoremstyle{definition}
\newtheorem{proposition}[theorem]{Proposition}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{2018 In-Class Exam}
%\newcommand{\hmwkDueDate}{Apr. 26, 2019}
\newcommand{\hmwkClass}{DSO}
%\newcommand{\hmwkClassTime}{Section A}
%\newcommand{\hmwkClassInstructor}{S. Heilman}
\newcommand{\hmwkAuthorName}{\textbf{G. Faletto} }

%\renewcommand{\subset}{\subseteq}
\renewcommand{\supset}{\supseteq}
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\abs}[1]{\left|#1\right|}                   % Absolute value notation
\newcommand{\absf}[1]{|#1|}                             % small absolute value signs
\newcommand{\vnorm}[1]{\left|\left|#1\right|\right|}    % norm notation
\newcommand{\vnormf}[1]{||#1||}                         % norm notation, forced to be small
\newcommand{\im}[1]{\mbox{im}#1}                        % Pieces of English for math mode
\newcommand{\tr}[1]{\mbox{tr}#1}
\newcommand{\Proj}[1]{\mbox{Proj}#1}
\newcommand{\Vol}[1]{\mbox{Vol}#1}
\newcommand{\Z}{\mathbb{Z}}                             % Blackboard notation
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\figoneawidth}{.5\textwidth}                % Image formatting parameters
\newcommand{\lbreak}{\\}                                % Linebreak
\newcommand{\italicize}[1]{\textit {#1}}                % formatting commands for bibliography
%\newcommand{\embolden}[1]{\textbf {#1}}
\newcommand{\embolden}[1]{{#1}}
\newcommand{\undline}[1]{\underline {#1}}
\newcommand{\e}{\varepsilon}
\renewcommand{\epsilon}{\varepsilon}
%\renewcommand{:=}{=}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{DSO Screening Exam:\ \hmwkTitle}}\\
%    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
%    \vspace{0.1in}\large{\textit{Instructor: Dr. Steven Heilman\ }}
    \vspace{3in}
}

\author{Gregory Faletto}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[2]{\frac{\mathrm{d} #1}{\mathrm{d} #2}}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{Solution.}}

% Probability commands: Expectation, Variance, Covariance, Bias
%\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\DeclareMathOperator{\Tr}{Tr}

\newcommand{\nPr}[2]{\,_{#1}P_{#2}}
\newcommand{\nCr}[2]{\,_{#1}C_{#2}}
%\binom{n}{r}

% Tilde
\newcommand{\textapprox}{\raisebox{0.5ex}{\texttildelow}}

\begin{document}

\maketitle

\pagebreak


% Problem 1
\begin{exercise}[\textbf{Probability}]

\begin{enumerate}[(a)]

% 1a
\item 

\begin{enumerate}[(i)]

% 1(a)i
\item 

%We have

%\[
%e^{-y} T(y) \xrightarrow{d} \operatorname{Exponential}(\lambda) \iff \lim_{y \to \infty} \Pr(e^{-y} T(y) \leq t) = 1 - e^{-\lambda t} 
%\]
%
%\[
%\iff \lim_{y \to \infty} \Pr \left( \frac{  \inf \{x \geq 0: M(x) \geq y\} }{e^y}  > t \right) = e^{-\lambda t} \iff \lim_{y \to \infty} \Pr \left(  e^ { \log \left( \inf \{x \geq 0: M(x) \geq y\} \right)  - y }  > t \right) = e^{-\lambda t}
%\]
%
%\[
%\iff \lim_{y \to \infty} \Pr \left(  e^ { \log \left( \inf \{x \geq 0: M(x) \geq y\} \right)  - y }  > t \right) = 1 - \lambda t +\frac{\lambda^2t^2}{2!} - \frac{\lambda^3t^3}{3!} + \ldots + \frac{(-\lambda t)^n}{n!} + \ldots 
%\]
%
%\[
%\vdots
%\]
%
%\[
%\lim_{y \to \infty} e^{-y}  \inf \{x \geq 0: M(x) \geq y\}  =  \lambda e^{- \lambda t} 
%\]
%
%\[
%\vdots
%\]
%
%So as \(y \to \infty\), \(\Pr \left(  e^ { \log \left( \inf \{x \geq 0: M(x) \geq y\} \right)  - y }  > t \right) \) converges to an \(\operatorname{Exponential}(\lambda)\) random variable. That is, the probability that the ratio of the first hit time for \(y\) (\(T(y)\)) and \(e^y\) exceeds \(t\) has a memoryless distribution in \(t\) as \(y\) grows without bound.
%
%\[
%\vdots
%\]

We have

\[
T(y) = \inf \{x \geq 0 : M(x) \geq y \}; \qquad \lim_{y \to \infty} \Pr \left( e^{-y} T(y) \leq a \right) = 1 - e^{-\lambda a} \qquad \forall a \geq 0.
\]

Note that

\[
 \lim_{y \to \infty} \Pr \left( e^{-y} T(y) \leq a \right) = \lim_{y \to \infty} \Pr(T(y) \leq a e^y) 
\]

Because \(T(y) = \inf \{x \geq 0 : M(x) \geq y \}\) and \(M(\cdot)\) is montonically increasing, we have the inequality \(M(z) \geq y\) for all \(z \geq x\). Therefore \(T(y) \leq ae^y \iff M(ae^y) \geq y\). Let \(z = ae^y \implies y = \log(z/a)\); then we have

\[
\lim_{y \to \infty} \Pr(T(y) \leq a e^y)  = \lim_{y \to \infty} \Pr( M(ae^y) \geq y)  = \lim_{z \to \infty} \Pr( M(z) \geq \log(z) - \log(a)) 
\]

Let \(b = \log a\) to get 

\[
= \lim_{z \to \infty} \Pr( M(z) - \log(z) \geq   b) = 1 - e^{-\lambda a} \implies  \lim_{z \to \infty} \Pr( M(z) - \log(z) \geq  b)  = 1 - e^{-\lambda e^{-b}}
\]

\[
 \iff \lim_{z \to \infty} \Pr( M(z) - \log(z) \leq  b)  = e^{-\lambda e^{-b}}
\]

% 1(a)ii
\item The distribution function is \(F(x) = e^{-\lambda e^{-x}}\), a Gumbel distribution with parameter \(\lambda\).

\end{enumerate}

% 1b
\item \textbf{Mohammad:}

Using hint:

\[
\int_0^1 t^x  \ dt = \left[ \frac{t^{x+1}}{x+1} \right]_0^1 =  \frac{1}{x+1}
\]

\[
 \iff \E \left[ \int_0^1 t^X  \ dt \right] = \E \left( \frac{1}{X+1} \right)  \iff \int_0^1 \E ( t^X)  \ dt = \E \left( \frac{1}{X+1} \right)
\]

\[
\iff \int_0^1 \sum_{i=0}^n t^i \Pr(X = i) \ dt = \E \left( \frac{1}{X+1} \right) \iff \int_0^1 \sum_{i=0}^n t^i \binom{n}{i} p^i (1-p)^{n-i} \ dt = \E \left( \frac{1}{X+1} \right)
\]

\[
 \iff (1-p)^n \int_0^1 \sum_{i=0}^n \binom{n}{i} \left( \frac{tp}{1-p} \right) ^i  \ dt = \E \left( \frac{1}{X+1} \right)
\]

\[
 \iff (1-p)^n \int_0^1 \left( 1 + \frac{tp}{1-p} \right)^n  \ dt = \E \left( \frac{1}{X+1} \right)
\]

\[
 \iff  \int_0^1 \left( 1 - p + tp  \right)^n  \ dt = \E \left( \frac{1}{X+1} \right)
\]

Let \(u = 1 - p + tp \implies du = p \ dt\). Then we can write

\[
\frac{1}{p} \int_{1-p}^1 u^n  \ du = \E \left( \frac{1}{X+1} \right) \iff \frac{1}{p} \left[ \frac{u^{n+1}}{n+1} \right]_{1-p}^1= \E \left( \frac{1}{X+1} \right)  \iff \E \left( \frac{1}{X+1} \right) = \frac{1}{p} \left[ \frac{1 - (1-p)^{n+1}}{n+1} \right]
\]

Now we consider \(\E \left[ \frac{1}{X+2} \right] \). 

\[
\int_0^1 t^{x+1}  \ dt = \left[ \frac{t^{x+2}}{x+2} \right]_0^1 =  \frac{1}{x+2}
\]

\[
 \iff \E \left[ \int_0^1 t^{X+1}  \ dt \right] = \E \left( \frac{1}{X+2} \right)  \iff \int_0^1 \E ( t^{X+1})  \ dt = \E \left( \frac{1}{X+2} \right)
\]

\[
\iff \int_0^1 \sum_{i=0}^n t^{i+1} \Pr(X = i) \ dt = \E \left( \frac{1}{X+2} \right) \iff \int_0^1 \sum_{i=0}^n t^{i+1} \binom{n}{i} p^i (1-p)^{n-i} \ dt = \E \left( \frac{1}{X+2} \right)
\]

\[
 \iff (1-p)^n \int_0^1 t\sum_{i=0}^n \binom{n}{i} \left( \frac{tp}{1-p} \right) ^i  \ dt = \E \left( \frac{1}{X+2} \right)
\]

\[
 \iff (1-p)^n \int_0^1t \left( 1 + \frac{tp}{1-p} \right)^n  \ dt = \E \left( \frac{1}{X+2} \right)
\]

\[
 \iff  \int_0^1 t\left( 1 - p + tp  \right)^n  \ dt = \E \left( \frac{1}{X+2} \right)
\]

Let \(u = 1 - p + tp \implies du = p \ dt\) and \(t = (u + p - 1)/p\). Then we can write

\[
\frac{1}{p^2} \int_{1-p}^1 u^n (u + p - 1) \ du = \E \left( \frac{1}{X+2} \right) \iff  \frac{1}{p^2} \int_{1-p}^1 [u^{n+1} + (p - 1)u^n] \ du = \E \left( \frac{1}{X+2} \right) 
\]

\[
\iff  \frac{1}{p^2} \left[\frac{u^{n+2}}{n+2} + (p - 1)\frac{u^{n+1}}{n+1} \right]_{1-p}^1 = \E \left( \frac{1}{X+2} \right) 
\]

\[
\iff   \E \left( \frac{1}{X+2} \right)  = \frac{1}{p^2} \left[\frac{1 - (1-p)^{n+2}}{n+2} - (1 - p)\frac{1 - (1-p)^{n+1}}{n+1} \right]
\]

%
%\[
%\vdots
%\]
%
%\[
%X \sim \operatorname{Bin}(n, p)
%\]
%
%Recall that 
%
%\[
%\E(X) = np \iff \sum_{x=0}^n x \cdot \binom{n}{x} p^x(1-p)^{n-x} =  (1-p)^n\sum_{x=0}^n x \binom{n}{x} \left( \frac{p}{1-p} \right)^x
%\]
%
%\[
%=  (1-p)^n\sum_{x=0}^n \frac{n!}{(n-x)!(x-1)!} \left( \frac{p}{1-p} \right)^x  = np.
%\]
%
%Then we have
%
%\[
%\E \left[ \frac{1}{1 + X} \right] = \sum_{x=0}^\infty \frac{1}{1 + x} \cdot \Pr(X=x) = \sum_{x=0}^n \frac{1}{1 + x} \cdot \binom{n}{x} p^x(1-p)^{n-x} = (1-p)^n\sum_{x=0}^n \frac{1}{1 + x} \cdot \binom{n}{x} \left( \frac{p}{1-p} \right)^x
%\]
%
%\[
%= (1-p)^n\sum_{x=0}^n  \frac{n!}{(n-x)!(x+1)!}\left( \frac{p}{1-p} \right)^x  = (1-p)^n\sum_{x=0}^n \frac{1}{(x+1)x} \frac{n!}{(n-x)!(x-1)!}\left( \frac{p}{1-p} \right)^x
%\]
%
%
%
%\[
%\vdots
%\]
%
%\[
%\E \left[ \frac{1}{1 + X} \right] = \int_{0}^\infty \Pr\left( [X+1]^{-1} > x \right) dx = \int_{0}^{1} \Pr\left( \frac{1}{X+1} > x \right)  dx  = \int_{0}^{1} \Pr\left( X < \frac{1}{x} - 1\right)   dx
%\]
%
%\[
%= \lim_{a \to 0^+} \int_{a}^{1} \Pr \left( X < \lceil x^{-1} - 1\rceil \right) dx
%\]
%
%\[
% = \lim_{a \to 0^+} \int_{a}^{(n+1)^{-1}} \Pr \left( X < \lceil x^{-1} - 1\rceil \right) dx + \int_{(n+1)^{-1}}^{1} \Pr \left( X < \lceil x^{-1} - 1\rceil \right) dx 
% \]
% 
% \[
% = \frac{1}{n+1} \cdot 1 + \int_{(n+1)^{-1}}^{1} \Pr \left( X < \lceil x^{-1} - 1\rceil \right) dx 
% \]
%
%%\[
%%= \lim_{a \to 0^+} \int_{a}^{1}  \left[ \sum_{j=0}^{\lceil x^{-1} - 1\rceil} \Pr\left( X = j  \right) \right]  dx = \lim_{a \to 0^+} \int_{a}^{1}  \left[ \sum_{j=0}^{\lfloor x^{-1} \rfloor} \binom{n}{j} p^j(1-p)^{n-j} \right]  dx
%%\]
%%
%%\[
%%= \lim_{a \to 0^+} \int_{a}^{1}  \left[ \sum_{j=0}^{\min \{\lfloor x^{-1} \rfloor, n\}} \binom{n}{j} p^j(1-p)^{n-j} \right]  dx = \frac{1}{n} \cdot 1 + \int_{n^{-1}}^{1}  \left[ \sum_{j=0}^{\lfloor x^{-1} \rfloor} \binom{n}{j} p^j(1-p)^{n-j} \right]  dx
%%\]
%
%This is simply the area under \(n\) rectangles with heights \(1, \Pr(X \leq n-1), \Pr(X \leq n- 2), \ldots, \Pr(X =0)\) and bases \((n+1)^{-1}, n^{-1} - (n+1)^{-1}, (n-1)^{-1} - n^{-1},  \ldots, 1/2\). That is, we can write this as
%
%\[
%\E \left[ \frac{1}{1 + X} \right] = \frac{1}{n+1} \cdot 1 + \left( \frac{1}{n} - \frac{1}{n+1} \right) \cdot \Pr(X \leq n-1) + \left( \frac{1}{n-1} - \frac{1}{n} \right) \cdot \Pr(X \leq n-2) 
%\]
%
%\[
%+ \ldots  + \left( \frac{1}{2} - \frac{1}{3} \right)  \cdot \Pr(X\leq 1)  + \frac{1}{2} \cdot \Pr(X=0) 
%\]
%
%%= \sum_{x=0}^{n-1} \sum_{j=x+1}^n  \binom{n}{j} p^j(1-p)^{1-j}
%%
%%\[
%% = (1-p) \sum_{x=0}^{n-1} \sum_{j=x+1}^n  \binom{n}{j} \left( \frac{p}{1-p} \right) ^j
%%\]
%
%\[
%\vdots
%\]
%
%Recall that 
%
%\[
%\E(X) = np \iff \sum_{x=0}^{n-1} \sum_{j=x+1}^n  \binom{n}{j} p^j(1-p)^{n-j}   =  (1-p) ^n\sum_{x=0}^{n-1} \sum_{j=x+1}^n  \binom{n}{j} \left( \frac{p}{1-p} \right) ^j= np.
%\]

\end{enumerate}

\end{exercise}

% Problem 2
\begin{exercise}[\textbf{Mathematical Statistics}]

\begin{enumerate}[(a)]

% 2a
\item Let \(\boxed{T_n (X_1, \ldots, X_n) := \sum_{i=1}^n X_i.}\) 


% 2b
\item Note that if \(T_n(X_1, \ldots, X_n) = y\), then \(\Pr \left[  (X_1, \ldots, X_n)  = (x_1, \ldots, x_n)  \right] = 0\) unless \(\sum_{i=1}^n x_i = y\). Therefore we have

\[
\Pr \left[  (X_1, \ldots, X_n)  = (x_1, \ldots, x_n) \mid T_n (X_1, \ldots, X_n) = y  \right] = \frac{\Pr \left[  (X_1, \ldots, X_n)  = (x_1, \ldots, x_n)  \right] }{\Pr\left(  T_n (X_1, \ldots, X_n) = y \right)}
\]

%
\[
\frac{\prod_{i=1}^n p^{x_i} (1-p)^{1-x_i}}{\binom{n}{y} p^y(1-p)^{n-y}} = \frac{ p^{\sum_{i=1}^n x_i} (1-p)^{n-\sum_{i=1}^n x_i}}{\binom{n}{y} p^y(1-p)^{n-y}}   = \frac{ p^{y} (1-p)^{n-y}}{\binom{n}{y} p^y(1-p)^{n-y}}   = \frac{1}{\binom{n}{y}}
\]

which does not depend on \(p\). That is, the distribution of \(X_1, \ldots, X_n\) conditional on \(T_n(X_1, \ldots, X_n)\) is independent of \(p\). Therefore \(T_n(X_1, \ldots, X_n)\) is sufficient for \(p\).


% 2c
\item Likelihood function:

\[
\mathcal{L}(p) = \prod_{i=1}^n p^{X_i} (1-p)^{1-X_i} 
%= (1-p)^n \prod_{i=1}^n \left(\frac{p}{1-p} \right)^{X_i} = (1-p)^n \left(\frac{p}{1-p} \right)^{\sum_{i=1}^n X_i}
\]

%\[
%\implies \ell(p) = n \log(1-p) + \sum_{i=1}^n X_i \log  \left(\frac{p}{1-p} \right)
%\]

\[
\implies \ell(p) = \sum_{i=1}^n \left[ X_i \log(p) + (1-X_i) \log(1-p) \right] = \log(p)  \sum_{i=1}^n X_i +  \log(1-p)\left(n -  \sum_{i=1}^n  X_i \right) 
\]

\[
\implies \deriv{}{p} \ell(p) =\frac{1}{p}  \sum_{i=1}^n X_i - \frac{1}{1-p}\left(n -  \sum_{i=1}^n  X_i \right) = 0 \implies \frac{1}{p}  \sum_{i=1}^n X_i  = \frac{1}{1-p}\left(n -  \sum_{i=1}^n  X_i \right)
\]

\[
\iff  \sum_{i=1}^n X_i - p  \sum_{i=1}^n X_i= pn - p \sum_{i=1}^n X_i \iff \boxed{ \hat{p} = \frac{1}{n} \sum_{i=1}^n X_i.}
\]

We can find its asymptotic distribution using the Central Limit Theorem:


\

\begin{center}
\noindent\fbox{
\parbox{0.9\textwidth}{
\begin{theorem} \label{asym.clt} \textbf{Central Limit Theorem (Grimmett and Stirzaker theorem 5.10.4.)} Let \(X_1, X_2, \ldots\) be a sequence of independent identically distributed random variables with finite mean \(\mu\) and finite non-zero variance \(\sigma^2\), and let \(S_n = \sum_{i=1}^n X_i\). Then

\[
\frac{S_n - n \mu}{\sqrt{n \sigma^2}} \xrightarrow{d} \mathcal{N}(0,1)
\]
\end{theorem}
}
}
\end{center}
\

Since \(\E(X_i) = p, \Var(X_i) = p(1-p)\), we have

\[
\frac{\sum_{i=1}^n X_i - np}{\sqrt{n p(1-p)}} \xrightarrow{d} \mathcal{N}(0,1) \iff  \sum_{i=1}^n X_i -np \xrightarrow{d} \mathcal{N}(0,np(1-p))
\]

\[
  \iff  \frac{1}{n} \sum_{i=1}^n X_i -p \xrightarrow{d} \mathcal{N}\left(0, \frac{p(1-p)}{n} \right)   \iff  \boxed{ \frac{1}{n} \sum_{i=1}^n X_i  \xrightarrow{d} \mathcal{N}\left(p, \frac{p(1-p)}{n} \right)}
\]

\end{enumerate}

\end{exercise}

% Problem 3
\begin{exercise}[\textbf{Mathematical Statistics}]

\begin{enumerate}[(a)]

% 3a
\item We have

\[
X \mid \mu \sim \mathcal{N}(\mu, \boldsymbol{I}_n)
\]

Let \(X = (X_1, \ldots, X_n)^T\) and let \(\mu = (\mu_1, \ldots, \mu_n)^T\). Notice that

\[
\E(\boldsymbol{X}^T\boldsymbol{X}) = \E[ \E(\boldsymbol{X}^T\boldsymbol{X} \mid \mu)] =  \E \left[ \E \left( X_1^2 + X_2^2 + \ldots + X_n^2 \right)  \right] = \E \left[ \sum_{i=1}^n \E (X_i^2)  \right] 
\]

\[
= \E \left[ \sum_{i=1}^n \Var(X_i) + \E(X_i)^2  \right]  =  \E \left[ \sum_{i=1}^n 1+ \mu_i^2 \right] = n + \lVert \mu \rVert_2^2 
\]

\[
\implies \E(\boldsymbol{X}^T\boldsymbol{X} - n) =  \lVert \mu \rVert_2^2 
\]

Therefore \(\boxed{ \hat{\delta}_{\text{unbiased}} = \boldsymbol{X}^T\boldsymbol{X} - n}\) is unbiased for \(\lVert \mu \rVert_2^2\).

% 3b
\item We will begin by finding the posterior distribution of \(\mu\). The prior distribution of \(\mu\) is

\[
f(\boldsymbol{\mu}) = (2 \pi)^{-n/2} | k\boldsymbol{I}_n|^{-1/2} \cdot \exp \left(- \frac{1}{2} \boldsymbol{\mu} ^T ( k\boldsymbol{I}_n)^{-1} \boldsymbol{\mu} \right) = \frac{1}{\sqrt{(2 \pi k)^n}} \exp \left(- \frac{1}{2k}\boldsymbol{\mu} ^T  \boldsymbol{\mu} \right).
\]


The likelihood is

\[
f_{\boldsymbol{X} \mid \boldsymbol{\mu}}(\boldsymbol{x} \mid \boldsymbol{\mu}) = (2 \pi)^{-n/2} | \boldsymbol{I}_n|^{-1/2} \cdot \exp \left(- \frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu}) ^T ( \boldsymbol{I}_n)^{-1} (\boldsymbol{x} - \boldsymbol{\mu} ) \right)
\]

\[
= (2 \pi)^{-n/2} \exp \left(- \frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu}) ^T  (\boldsymbol{x} - \boldsymbol{\mu} ) \right).
\]

So the unconditional distribution of \(\boldsymbol{X}\) is

\[
f_{\boldsymbol{X}}(\boldsymbol{x})  = \int_{\mathbb{R}^n} f_{\boldsymbol{X} \mid \boldsymbol{\mu}}(\boldsymbol{x} \mid \boldsymbol{\mu}) f(\boldsymbol{\mu}) \ d \boldsymbol{\mu} 
\]

\[
= \int_{\mathbb{R}^n}  (2 \pi)^{-n/2} \exp \left(- \frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu}) ^T  (\boldsymbol{x} - \boldsymbol{\mu} ) \right)  \cdot \frac{1}{\sqrt{(2 \pi k)^n}} \exp \left(- \frac{1}{2k}\boldsymbol{\mu} ^T  \boldsymbol{\mu} \right) \ d \boldsymbol{\mu}
\]

\[
= \frac{1}{(2 \pi \sqrt{k})^n  } \int_{\mathbb{R}^n}  \exp \left(- \frac{1}{2}\left[  \boldsymbol{\mu} ^T\boldsymbol{\mu}  + \frac{1}{k}\boldsymbol{\mu} ^T  \boldsymbol{\mu}  - 2 \boldsymbol{x} ^T\boldsymbol{\mu}   + \boldsymbol{x}^T \boldsymbol{x}  \right]  \right)  \ d \boldsymbol{\mu}
\]

\[
= \frac{1}{(2 \pi \sqrt{k})^n  } \int_{\mathbb{R}^n}  \exp \left(-  \frac{k+1}{2k} \left[  \boldsymbol{\mu} ^T\boldsymbol{\mu}   -  \frac{2k}{k+1} \boldsymbol{x} ^T\boldsymbol{\mu}   \right] - \frac{1}{2}  \boldsymbol{x}^T \boldsymbol{x}   \right)  \ d \boldsymbol{\mu}
\]

\[
= \frac{1}{(2 \pi \sqrt{k})^n  } \int_{\mathbb{R}^n}  \exp \left(-  \frac{k+1}{2k} \left[  \boldsymbol{\mu} ^T\boldsymbol{\mu}   -  \frac{2k}{k+1} \boldsymbol{x} ^T\boldsymbol{\mu} + \left(\frac{k}{k+1} \right)^2 \boldsymbol{x}^T \boldsymbol{x}    \right] - \left(-  \frac{k+1}{2k} \right) \left(\frac{k}{k+1} \right)^2 \boldsymbol{x}^T \boldsymbol{x} - \frac{1}{2}  \boldsymbol{x}^T \boldsymbol{x}   \right)  \ d \boldsymbol{\mu}
\]

\[
= \frac{1}{(2 \pi \sqrt{k})^n  } \int_{\mathbb{R}^n}  \exp \left(-  \frac{k+1}{2k} \left[  \left( \boldsymbol{\mu} -  \frac{k}{k+1}  \boldsymbol{x} \right) ^T\left( \boldsymbol{\mu} -  \frac{k}{k+1} \boldsymbol{x} \right) \right] - \frac{1}{2} \left[ -\left(\frac{k}{k+1} \right) \boldsymbol{x}^T \boldsymbol{x}  + \frac{k+1}{k+1}  \boldsymbol{x}^T \boldsymbol{x}   \right] \right)  \ d \boldsymbol{\mu}
\]

\[
= \frac{1}{(2 \pi \sqrt{k})^n  } \int_{\mathbb{R}^n}  \exp \left(-  \frac{k+1}{2k} \left[  \left( \boldsymbol{\mu} -  \frac{k}{k+1}  \boldsymbol{x} \right) ^T\left( \boldsymbol{\mu} -  \frac{k}{k+1} \boldsymbol{x} \right) \right] \right) \exp \left(- \frac{1}{2} \frac{1}{k+1}  \boldsymbol{x}^T \boldsymbol{x}   \right)  \ d \boldsymbol{\mu}
\]

\[
= \frac{1}{\sqrt{(2 \pi k )^n}} \exp \left(- \frac{1}{2} \frac{1}{k+1} \boldsymbol{x}^T \boldsymbol{x}   \right)\left( \frac{k}{k+1} \right)^{n/2}
\]

\[
\cdot \int_{\mathbb{R}^n}  \frac{1}{\sqrt{(2 \pi)^n}}  \cdot \left( \frac{k}{k+1} \right)^{-n/2}   \exp \left(-  \frac{1}{2}   \left( \boldsymbol{\mu} -  \frac{k}{k+1}  \boldsymbol{x} \right) ^T \left( \frac{k}{k+1}\boldsymbol{I}_n \right)^{-1} \left( \boldsymbol{\mu} -  \frac{k}{k+1} \boldsymbol{x} \right)  \right)   \ d \boldsymbol{\mu}
\]

The second row is the integral over \(\mathbb{R}^n\) of an \(n\)-dimensional multivariate Gaussian distribution with mean \(k/(k+1) \boldsymbol{x}\) and covariance \(k/(k+1) \boldsymbol{I}_n\), so it equals 1. Then we are left with

\[
f_{\boldsymbol{X}}(\boldsymbol{x})  =\frac{1}{\sqrt{(2 \pi )^n}}  \frac{1}{k^{n/2}}\exp \left(- \frac{1}{2} \frac{1}{k+1} \boldsymbol{x}^T \boldsymbol{x}   \right)\left( \frac{k}{k+1} \right)^{n/2}
\]

\begin{equation}\label{2018.screen.3.d.marginal.x.gaussian}
=\frac{1}{\sqrt{(2 \pi )^n}} \left[ \left( k+1 \right)^n \right]^{-1/2} \exp \left(- \frac{1}{2} \boldsymbol{x}^T \left( (k+1) \boldsymbol{I}_n \right)^{-1} \boldsymbol{x}   \right)
\end{equation}

which is the density of an \(n\)-dimensional multivariate Gaussian random variable with mean \(\boldsymbol{0}\) and covariance \((k+1)\boldsymbol{I}_n\). Therefore the posterior distribution of \(\boldsymbol{\mu}\) is

\[
f_{\boldsymbol{\mu} \mid \boldsymbol{X}}(\boldsymbol{\mu} \mid \boldsymbol{x}) = \frac{f_{\boldsymbol{X} \mid \boldsymbol{\mu}}(\boldsymbol{x} \mid \boldsymbol{\mu}) f(\boldsymbol{\mu})}{f_{\boldsymbol{X}}(\boldsymbol{x})}
\]

\[
= \left[   \frac{1}{(2 \pi \sqrt{k})^n  }  \exp \left(- \frac{1}{2}\left[  \boldsymbol{\mu} ^T\boldsymbol{\mu}  + \frac{1}{k}\boldsymbol{\mu} ^T  \boldsymbol{\mu}  - 2 \boldsymbol{x} ^T\boldsymbol{\mu}   + \boldsymbol{x}^T \boldsymbol{x}  \right]  \right)  \Bigg] \middle/  \Bigg[ \frac{1}{\sqrt{(2 \pi )^n}} \left[ \left( k+1 \right)^n \right]^{-1/2} \exp \left(- \frac{1}{2} \frac{1}{k+1} \boldsymbol{x}^T \boldsymbol{x}   \right) \right]
\]

\[
=  \frac{1}{\sqrt{(2\pi)^n}} \left( \frac{k+1}{k} \right)^{n/2} \exp  \left(-  \frac{k+1}{2k} \left[  \left( \boldsymbol{\mu} -  \frac{k}{k+1}  \boldsymbol{x} \right) ^T\left( \boldsymbol{\mu} -  \frac{k}{k+1} \boldsymbol{x} \right) \right] - \frac{1}{2} \frac{1}{k+1}  \boldsymbol{x}^T \boldsymbol{x}  + \frac{1}{2} \frac{1}{k+1} \boldsymbol{x}^T \boldsymbol{x}\right)  
\]

\[
=  \frac{1}{\sqrt{(2\pi)^n}} \left( \frac{k+1}{k} \right)^{n/2} \exp  \left(-  \frac{k+1}{2k}  \left( \boldsymbol{\mu} -  \frac{k}{k+1}  \boldsymbol{x} \right) ^T\left( \boldsymbol{\mu} -  \frac{k}{k+1} \boldsymbol{x} \right)\right)  
\]

\[
=
 \frac{1}{\sqrt{(2 \pi)^n}}  \cdot \left( \frac{k}{k+1} \right)^{-n/2}   \exp \left(-  \frac{1}{2}   \left( \boldsymbol{\mu} -  \frac{k}{k+1}  \boldsymbol{x} \right) ^T \left( \frac{k}{k+1}\boldsymbol{I}_n \right)^{-1} \left( \boldsymbol{\mu} -  \frac{k}{k+1} \boldsymbol{x} \right)  \right)
 \]
 
which is an \(n\)-dimensional multivariate Gaussian distribution with mean \(k/(k+1) \boldsymbol{x}\) and covariance \(k/(k+1) \boldsymbol{I}_n\). That is, conditional on \(\boldsymbol{X}\), \(\boldsymbol{\mu} = (\mu_1, \ldots, \mu_n)\), where

\[
\mu_i \mid \boldsymbol{X}  \overset{i.i.d.}{\sim} \mathcal{N} \left( \frac{k}{k+1} X_i, \frac{k}{k+1}\right).
\]

\[
\iff \left( \mu_i - \frac{k}{k+1}  X_i \right) \frac{k+1}{k}  \mid \boldsymbol{X}\overset{i.i.d.}{\sim} \mathcal{N} \left(0, 1\right) \iff \frac{k+1}{k} \mu_i -   X_i  \mid \boldsymbol{X} \overset{i.i.d.}{\sim} \mathcal{N} \left(0, 1\right)
\]

\begin{equation}
\implies  \E \left( \left[ \frac{k+1}{k} \mu_i -   X_i \right]^2  \mid \boldsymbol{X}  \right) = 1  \iff   \E \left( \left[ \frac{k+1}{k} \right]^2 \mu_i  ^2  + X_i^2 - 2 \frac{k+1}{k} \mu_i X_i  \mid \boldsymbol{X}  \right) = 1 
\end{equation}

So,

\[
\hat{\delta}_{\text{proper}} = \E \left( \lVert \boldsymbol{\mu} \rVert_2^2 \mid \boldsymbol{X} \right) = \E \left( \sum_{i=1}^n \mu_i^2 \mid \boldsymbol{X}  \right) = \left[ \frac{k}{k+1} \right]^2 \E \left( \sum_{i=1}^n \left[ \frac{k+1}{k} \right]^2 \mu_i^2 \mid \boldsymbol{X}  \right)
\]

\[
= \left[ \frac{k}{k+1} \right]^2 \E \left( \sum_{i=1}^n \left[ \frac{k+1}{k} \right]^2 \mu_i  ^2  + X_i^2 - 2 \frac{k+1}{k} \mu_i X_i  \mid \boldsymbol{X}  \right)  -  \left[ \frac{k}{k+1} \right]^2 \E \left( \sum_{i=1}^n X_i^2 - 2 \frac{k+1}{k} \mu_i X_i  \mid \boldsymbol{X}  \right) 
\] 

\[
= \left[ \frac{k}{k+1} \right]^2  - \left[ \frac{k}{k+1} \right]^2 \sum_{i=1}^n X_i^2 + 2 \frac{k+1}{k} \left[ \frac{k}{k+1} \right]^2   \sum_{i=1}^n X_i   \E \left( \mu_i \mid \boldsymbol{X}  \right) 
\] 

\[
= \left[ \frac{k}{k+1} \right]^2  - \left[ \frac{k}{k+1} \right]^2 \sum_{i=1}^n X_i^2 + 2 \frac{k}{k+1} \sum_{i=1}^n    X_i   \cdot \frac{k}{k+1} X_i
\] 

\[
= \left[ \frac{k}{k+1} \right]^2   + 2 \left[ \frac{k}{k+1}\right]^2    \sum_{i=1}^n  X_i ^2  - \left[ \frac{k}{k+1} \right]^2 \sum_{i=1}^n X_i^2 = \left[ \frac{k}{k+1} \right]^2 \left(1   +     \boldsymbol{X}^T \boldsymbol{X} \right).  
\] 


% 3c
\item We will again begin by finding the posterior distribution of \(\mu\). The (improper) prior distribution of \(\mu\) is constant; that is, for some \(c\in \mathbb{R}\),

\[
f(\boldsymbol{\mu}) = c, \qquad \forall \boldsymbol{\mu} \in \mathbb{R}^n.
\]

The likelihood is

\[
f_{\boldsymbol{X} \mid \boldsymbol{\mu}}(\boldsymbol{x} \mid \boldsymbol{\mu}) = (2 \pi)^{-n/2} | \boldsymbol{I}_n|^{-1/2} \cdot \exp \left(- \frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu}) ^T ( \boldsymbol{I}_n)^{-1} (\boldsymbol{x} - \boldsymbol{\mu} ) \right)
\]

\[
= (2 \pi)^{-n/2} \exp \left(- \frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu}) ^T  (\boldsymbol{x} - \boldsymbol{\mu} ) \right).
\]

So the unconditional distribution of \(\boldsymbol{X}\) is

\[
f_{\boldsymbol{X}}(\boldsymbol{x})  = \int_{\mathbb{R}^n} f_{\boldsymbol{X} \mid \boldsymbol{\mu}}(\boldsymbol{x} \mid \boldsymbol{\mu}) f(\boldsymbol{\mu}) \ d \boldsymbol{\mu} = c \int_{\mathbb{R}^n}  (2 \pi)^{-n/2} \exp \left(- \frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu}) ^T  (\boldsymbol{x} - \boldsymbol{\mu} ) \right)  \ d \boldsymbol{\mu}
\]

\[
= c \int_{\mathbb{R}^n}  (2 \pi)^{-n/2} \exp \left(- \frac{1}{2}(  \boldsymbol{\mu} - \boldsymbol{x}) ^T  (\boldsymbol{\mu} - \boldsymbol{x}) \right)  \ d \boldsymbol{\mu}
\]

The expression inside the integral is the density of a Gaussian random variable with mean \(\boldsymbol{x}\) and covariance \(\boldsymbol{I}_n\), so the integral evaluates to 1. Therefore the unconditional distribution of \(\boldsymbol{X}\) is also flat. Therefore the posterior distribution of \(\boldsymbol{\mu}\) is the same as the likelihood:

\[
f_{\boldsymbol{\mu} \mid \boldsymbol{X}}(\boldsymbol{\mu} \mid \boldsymbol{x}) = \frac{f_{\boldsymbol{X} \mid \boldsymbol{\mu}}(\boldsymbol{x} \mid \boldsymbol{\mu}) f(\boldsymbol{\mu})}{f_{\boldsymbol{X}}(\boldsymbol{x})} = (2 \pi)^{-n/2} \exp \left(- \frac{1}{2}(\boldsymbol{\mu} - \boldsymbol{x} ) ^T  ( \boldsymbol{\mu} - \boldsymbol{x}) \right);
\]

that is, conditional on \(\boldsymbol{X}\), \(\boldsymbol{\mu}\) is normally distributed with mean \(\boldsymbol{X}\) and covariance \(\boldsymbol{I}_n\). So  conditional on \(\boldsymbol{X}\), \(\boldsymbol{\mu} = (\mu_1, \ldots, \mu_n)\), where

\[
\mu_i  \mid \boldsymbol{X} \overset{i.i.d.}{\sim} \mathcal{N} \left(  X_i, 1 \right).
\]

\begin{equation}
 \iff  \mu_i -  X_i  \mid \boldsymbol{X} \overset{i.i.d.}{\sim} \mathcal{N} \left(0, 1\right)  \implies  \E \left( \left[ \mu_i -   X_i \right]^2  \mid \boldsymbol{X}  \right) = 1 \iff   \E \left(  \mu_i  ^2  + X_i^2 -  2\mu_i X_i  \mid \boldsymbol{X}  \right) = 1 
\end{equation}

So,

\[
\hat{\delta}_{\text{flat}} = \E \left( \lVert \boldsymbol{\mu} \rVert_2^2 \mid \boldsymbol{X} \right) = \E \left( \sum_{i=1}^n \mu_i^2 \mid \boldsymbol{X}  \right) = \E \left( \sum_{i=1}^n  \mu_i  ^2  + X_i^2 -  2\mu_i X_i  \mid \boldsymbol{X}  \right) -  \E \left( \sum_{i=1}^n  X_i^2 - 2 \mu_i X_i  \mid \boldsymbol{X}  \right)
\]

\[
= 1 -  \sum_{i=1}^n  X_i^2  + 2 \sum_{i=1}^n  X_i  \E \left( \mu_i  \mid \boldsymbol{X}  \right)  = 1 -  \sum_{i=1}^n  X_i^2  + 2 \sum_{i=1}^n  X_i^2  = 1 + \boldsymbol{X}^T\boldsymbol{X}.  
\]

So,

\begin{equation}\label{2018.screen.3.c.flat.unbiased.difference}
\hat{\delta}_{\text{flat}}  - \hat{\delta}_{\text{unbiased}} = 1 + \boldsymbol{X}^T\boldsymbol{X} - \left( \boldsymbol{X}^T\boldsymbol{X} - n \right) = 1 + n.
\end{equation}


% 3d
\item If the true distribution of \(\boldsymbol{\mu}\) is the prior from part (b), then the marginal distribution of \(\boldsymbol{X}\) is (\ref{2018.screen.3.d.marginal.x.gaussian}):

\[
=\frac{1}{\sqrt{(2 \pi )^n}} \left[ \left( k+1 \right)^n \right]^{-1/2} \exp \left(- \frac{1}{2} \boldsymbol{x}^T \left( (k+1) \boldsymbol{I}_n \right)^{-1} \boldsymbol{x}   \right);
\]

that is, \(\boldsymbol{X} \sim \mathcal{N} (\boldsymbol{0}, (k+1)\boldsymbol{I}_n)\). (Note that this means \( (k+1)^{-1} X_i \) is standard Gaussian and i.i.d. for all \(i \in \{1, \ldots, n\}\).) Per the suggestion, let 

\[
\hat{D} = \hat{\delta}_{\text{proper}}  - \hat{\delta}_{\text{unbiased}} =  \left[ \frac{k}{k+1} \right]^2 \left(1   +     \boldsymbol{X}^T \boldsymbol{X} \right) - \left( \boldsymbol{X}^T\boldsymbol{X} - n \right) = \frac{k^2 - (k^2 + 2k + 1)}{(k+1)^2}  \boldsymbol{X}^T\boldsymbol{X} + \left[ \frac{k}{k+1} \right]^2 + n
\]

\begin{equation}
= -\frac{2k + 1}{(k+1)^2}  \boldsymbol{X}^T\boldsymbol{X} + \left[ \frac{k}{k+1} \right]^2 + n
\end{equation}



Since from  (\ref{2018.screen.3.c.flat.unbiased.difference}) we have

\[
\E \left( \hat{\delta}_{\text{proper}}  - \hat{\delta}_{\text{unbiased}} \right)^2 = n^2 + 2n + 1,
\]

we seek

\[
\E \left( \hat{\delta}_{\text{flat}}  - \hat{\delta}_{\text{unbiased}} \right)^2= \E( \hat{D}^2) = \Var(\hat{D}) + \left[\E(\hat{D}) \right]^2.
\]

Note that since \((k+1)^{-1} X_i \overset{i.i.d.}{\sim} \mathcal{N}(0,1)\) for all \(i \in \{1, \ldots, n\}\), 

\[
\sum_{i=1}^n \left( (k+1)^{-1} X_i \right)^2 \sim \chi_n^2,
\]

so

\[
\E \left[ \sum_{i=1}^n \left( \frac{1}{k+1} X_i \right)^2  \right] = n \iff \frac{1}{(k+1)^2} \E \left[ \sum_{i=1}^n X_i ^2  \right] = n \iff  \E \left[ \sum_{i=1}^n X_i ^2  \right] = n(k+1)^2,
\]

and

\[
\Var \left[ \sum_{i=1}^n \left( \frac{1}{k+1} X_i \right)^2  \right] = 2n \iff \frac{1}{(k+1)^4} \Var \left[ \sum_{i=1}^n X_i ^2  \right] = 2n \iff  \Var \left[ \sum_{i=1}^n X_i ^2  \right] = 2n(k+1)^4.
\]

Under the unconditional distribution of \(\boldsymbol{X}\),

\[
\E \left(\hat{D} \right) = \E \left( -\frac{2k + 1}{(k+1)^2}  \boldsymbol{X}^T\boldsymbol{X} + \left[ \frac{k}{k+1} \right]^2 + n \right)  =  - \frac{2k +1}{(k+1)^2}\E \left(  \boldsymbol{X}^T\boldsymbol{X}  \right) + \left[ \frac{k}{k+1} \right]^2 + n
\]

\[
=  - \frac{2k +1}{(k+1)^2}\E \left(  \sum_{i=1}^n X_i^2  \right) + \left[ \frac{k}{k+1} \right]^2 + n =  - \frac{2k +1}{(k+1)^2} n(k+1)^2 + \left[ \frac{k}{k+1} \right]^2 + n
\]

\[
=  -n \frac{(2k +1)(k^2 + 2k + 1) + k^2}{(k+1)^2}   + n =  n \left[ \frac{k^2 + 2k + 1}{(k+1)^2} - \frac{2k^3 + 4k^2 + 2k + k^2 + 2k + 1 + k^2}{(k+1)^2} \right]
\]

\[
=  n \left[  \frac{-2k^3 -5k^2-2k }{(k+1)^2} \right] =  -n \left[  \frac{2k^3  + 5k^2 + 2k }{(k+1)^2} \right]
\]


%\[
%=n \cdot \frac{(k^2 - k - 1)(k^2 + 2k + 1)}{(k+1)^2}  + \left[ \frac{k}{k+1} \right]^2 + n
%\]

%\[
%\E \left(\hat{D} \right) =  - \frac{2k +1}{(k+1)^2} \cdot (k+1)^2 + \left[ \frac{k}{k+1} \right]^2 + n = \frac{(k^2 - k - 1)(k^2 + 2k + 1)}{(k+1)^2}  + \left[ \frac{k}{k+1} \right]^2 + n
%\]
%\[
%= k^2 - k - 1  + \left[ \frac{k}{k+1} \right]^2 + n
%\]


%\[
%= \frac{k^4 + 2k^3 + k^2 - k^3 -2k^2 -k - k^2 -2k - 1 + k^2}{(k+1)^2}  + n = \frac{k^4 + k^3  - k^2    -3k - 1 }{(k+1)^2}  + n
%\]

Next,

\[
\Var (\hat{D}) = \Var \left( - \frac{2k +1}{(k+1)^2}  \boldsymbol{X}^T\boldsymbol{X} + \left[ \frac{k}{k+1} \right]^2 + n  \right) = \frac{(2k +1)^2}{(k+1)^4}   \Var \left( \sum_{i=1}^n  X_i ^2   \right)
\]

\[
= \frac{(2k +1)^2}{(k+1)^4}   2n(k+1)^4 = 2n (2k+1)^2
\]


So

\[
\E \left( \hat{\delta}_{\text{flat}}  - \hat{\delta}_{\text{unbiased}} \right)^2 =  2n (2k+1)^2 + \left(-n \left[  \frac{2k^3  + 5k^2 + 2k }{(k+1)^2} \right] \right)^2 = n^2  \left[  \frac{2k^3  + 5k^2 + 2k }{(k+1)^2} \right] ^2 + n \cdot 2(2k+1)^2
\]

\[
\approx 4k^2 n^2 + 8k^2 n,
\]

so in general when \(k \geq 1\) and \(n\) is large,, \(\E \left( \hat{\delta}_{\text{flat}}  - \hat{\delta}_{\text{unbiased}} \right)^2 > \E \left( \hat{\delta}_{\text{proper}}  - \hat{\delta}_{\text{unbiased}} \right)^2 \); that is, the flat prior Bayes estimator is further from the unbiased estimator than the proper prior Bayes estimator.

%\[
%\approx k^2 + n,
%\]
%
%so in general roughly when \(k \geq 1\) this quantity is higher in expectation than \(\hat{\delta}_{\text{flat}}  - \hat{\delta}_{\text{unbiased}} \), (\ref{2018.screen.3.c.flat.unbiased.difference}). 

% \textbf{Try using information here: section 3.4, page 62} \begin{verbatim}http://www.stat.cmu.edu/~brian/463-663/week09/Chapter%2003.pdf\end{verbatim} 
%
%\textbf{maybe this is what I'm supposed to do for part (d)?} Because the prior distribution for \(\mu\) is Gaussian and the conditional distribution of \(X\) given \(\mu\) is Gaussian, that means the unconditional distribution for \(X\) is Gaussian. That is, we can use the formula for conditioning one set of Gaussian random variables on another to learn about the unconditional distribution of \(X\).
%
%\begin{center}
%\noindent\fbox{
%\parbox{0.9\textwidth}{
%\begin{proposition}\label{prob.cond.multivar.norm.dist} Suppose 
%
%\[
%\boldsymbol{X} = \begin{bmatrix} \boldsymbol{X}_1 \\ \boldsymbol{X}_2 \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix} \boldsymbol{\mu}_1 \\ \boldsymbol{\mu}_2 \end{bmatrix}, \begin{bmatrix}  \boldsymbol{\Sigma}_{11} & \boldsymbol{\Sigma}_{12} \\ \boldsymbol{\Sigma}_{12}^T & \boldsymbol{\Sigma}_{22} \end{bmatrix} \right)
%\]
%
%where \(\boldsymbol{\mu}_1 \in \mathbb{R}^q\), \(\boldsymbol{\mu}_2 \in \mathbb{R}^{p - q}\), \(\boldsymbol{\Sigma}_{11} \in \mathbb{R}^{q \times q}\), \( \boldsymbol{\Sigma}_{12} \in \mathbb{R}^{(p-q) \times q}\), and \( \boldsymbol{\Sigma}_{22} \in \mathbb{R}^{(p-q) \times (p-q)}\). Then
%
%\begin{equation}\label{prob.mult.gaussian.cond.dist.gen}
%\{\boldsymbol{X}_1 \mid \boldsymbol{X}_2\} \sim \mathcal{N} \left(  \boldsymbol{\mu}_1 +  \boldsymbol{\Sigma}_{12} \boldsymbol{\Sigma}_{22}^{-1} \left(\boldsymbol{X}_2 - \boldsymbol{\mu}_2\right), \boldsymbol{\Sigma}_{11} - \boldsymbol{\Sigma}_{12} \boldsymbol{\Sigma}_{22}^{-1} \boldsymbol{\Sigma}_{12}^T \right)
%\end{equation}
%
%\end{proposition}
%}
%}
%\end{center}
%
%\
%
%In this case, we know
%
%\[
%\mu \sim \mathcal{N}(0, k \boldsymbol{I}_n), \qquad X \mid \mu \sim \mathcal{N}(\mu,  \boldsymbol{I}_n)
%\]
%
%which implies that if \(X \sim \mathcal{N}(\boldsymbol{\mu}_1 , \boldsymbol{\Sigma}_{11})\), 
%
%\[
%\mu =  \boldsymbol{\mu}_1 +  \boldsymbol{\Sigma}_{12} \left( k \boldsymbol{I}_n \right)^{-1} \left(\mu - 0\right) \iff  \boldsymbol{\mu}_1 = \mu - \frac{1}{k} \boldsymbol{\Sigma}_{12}\mu = \left( \boldsymbol{I}_n -  \frac{1}{k} \boldsymbol{\Sigma}_{12} \right) \mu 
%\]
%
%and
%
%\[
%\boldsymbol{I}_n = \boldsymbol{\Sigma}_{11} - \boldsymbol{\Sigma}_{12}\left( k \boldsymbol{I}_n \right)^{-1} \boldsymbol{\Sigma}_{12}^T \iff  \boldsymbol{\Sigma}_{11} = \boldsymbol{I}_n  + \frac{1}{k} \boldsymbol{\Sigma}_{12} ^2
%\]
%
%(since \(\boldsymbol{\Sigma}_{12}\) is symmetric).
%
%\[
%\E\left[ \left( \lVert \mu \rVert_2^2 - t(X)  \right)^2 \mid X\right] = \E\left[  \lVert \mu \rVert_2^4 -2 \lVert \mu \rVert_2^2 t(X)   + t(X)^2 \mid X\right] = \E\left[  \lVert \mu \rVert_2^4\mid X\right]  -2t (X) \E \left[ \lVert \mu \rVert_2^2 \mid X\right]    + t(X)^2 
%\]
%
%The estimator minimizing this is \(t(X) = \E \left[ \lVert \mu \rVert_2^2 \mid X\right] = \E \left[ \mu^T\mu\mid X\right] \), which we need to find. We have that \(\mu \sim \mathcal{N}(0, k \boldsymbol{I}_n)\), so
%%\[
%% \Pr(\mu^T\mu = t \mid X) 
%% \]
%
%\[
%f_{\mu^T\mu}( t ) = f_{\sum_{i=1}^n \mu_i^2 }( t ) = f_{\sum_{i=1}^n \left[ \frac{\mu_i}{k} \right] ^2} \left( \frac{t}{k^2} \right) = f_{\chi_n^2} \left( \frac{t}{k^2} \right) 
%\]
%
%where \( f_{\chi_n^2} \) is the density of a \(\chi^2\) random variable with \(n\) degrees of freedom. Also, the set of vectors \(\{\mu' \in \mathbb{R}^n \mid \mu'^T\mu' = \lVert \mu \rVert_2^2\}\) is a hypersphere of radius \(\lVert \mu \rVert_2\) in \(\mathbb{R}^n\), so the conditional density of \(\mu'\) given \(\mu^T\mu\) is uniform over the surface of this hypersphere. Per \citet{Muller1959}, this can be generated by drawing \(n\) standard Gaussian random variables then dividing each by the \(\ell_2\) norm of all of them, then in this case multiplying by the desired \(\ell_2\) norm \(\lVert \mu \rVert_2\). That is,
%
%\[
% f_{\mu \mid \mu^T\mu} (m \mid t) =  f_{\mu \mid \mu^T\mu} \big((m_1, \ldots, m_n) \mid t \big) =  f_{\mu \mid \mu^T\mu} \big((m_1, \ldots, m_n) \mid t \big)
%\]
%
%%http://mathworld.wolfram.com/HyperspherePointPicking.html
%
%\[
%\implies f_{\mu^T\mu \mid X} (t \mid x) =  \frac{f_{\mu^T\mu , X}(t, x)}{f_X(x)} =  \frac{f_{\mu^T\mu}(t) f_{\mu \mid \mu^T\mu} (m \mid t) f_{X \mid \mu}(x \mid m)}{f_X(x)} 
%\]
%
%\[
%\implies \E(\mu^T\mu \mid X) = \int_0^\infty t \Pr(\mu^T\mu = t \mid X) dt
%\]
%
%\[
%\vdots
%\]
%
%Given \(\mu\), we have 
%
%\[
%X \mid \mu  \sim \mathcal{N} (\mu, \boldsymbol{I}_n) \implies (X - \mu)^T(X- \mu) \mid \mu \sim \chi_n^2 \iff X^TX - 2 \mu^TX + \mu^T\mu \mid \mu \sim \chi_n^2. 
%\]
%
%\[
%\vdots
%\]
%
%
%Also, we have that \(\mu \sim \mathcal{N}(0, k \boldsymbol{I}_n)\). The joint distribution of \(X\) and \(\mu\) is then
%
%\[
%f_{X, \mu}(x, m) = f_{X \mid \mu = m}(x \mid m) f_\mu(m) = 
%\]
%
%

\end{enumerate}

\end{exercise}

% Problem 4
\begin{exercise}[\textbf{High-Dimensional Statistics}]

\begin{enumerate}[(a)]

% 4a
\item \textbf{Sparsity in the covariance matrix does not imply sparsity in the precision matrix.} For a simple example, consider a sequence of \(p\) Gaussian random variables generated in the following way:

\[
X_1 \sim \mathcal{N}(0, 1), \qquad X_i = X_{i-1} + Z, i = 2, \ldots, p
\]

where \(Z \sim \mathcal{N}(0,1)\). Then for all \(i \neq j \in \{1, \ldots, p\}\), \(\Cov(X_i, X_j) \neq 0\), so the covariance matrix \(\boldsymbol{\Sigma} \in \mathbb{R}^{p \times p}\) is dense with no zero entries. However, for any \(i \neq j \in \{1, \ldots, p\}\), \(\Cov(X_i, X_j \mid \boldsymbol{X}_{-i-j}\) (where \(\boldsymbol{X}_{-i-j}\) contains all of the \(X_k\) except \(X_i and X_j\); that is, \(\Cov(X_i, X_j \mid \boldsymbol{X}_{-i-j}\) is the covariance of \(X_i\) and \(X_j\) conditional on all of the other \(X_k\)) is zero for all \(j\) except \(i-1\) and \(i+1\). Since the precision matrix contains in entry \(\sigma_{ij}\) the covariance of \(X_i\) and \(X_j\) conditional on all the other \(p-2\) variables (see the discussion in part (b)), the precision matrix is sparse, with mostly 0 entries except for the diagonal and a band of nonzero entries on each side of the diagonal.

Of course, since the precision matrix and covariance matrix are inverses, had the covariance matrix been sparse except the diagonal and a band of nonzero entries on each side of the diagonal, in general the precision matrix would be dense.

For a more complicated example, suppose the covariance matrix is the following:

\[
\boldsymbol{\Sigma} := \begin{pmatrix}
(\tau^2  + 1) \boldsymbol{I}_n &  \boldsymbol{I}_n & \cdots & \boldsymbol{I}_n \\
\boldsymbol{I}_n  & (\tau^2 + 1)  \boldsymbol{I}_n  & \cdots &  \boldsymbol{I}_n \\
\vdots & \vdots &  \ddots & \vdots \\
\boldsymbol{I}_n & \boldsymbol{I}_n  &  \cdots & (\tau^2 + 1) \boldsymbol{I}_n
\end{pmatrix},
\]

This matrix is relatively sparse. However, its inverse is dense, with every entry nonzero:

\[
\boldsymbol{\Sigma} = \tau^2 \boldsymbol{I}_{ns} + \boldsymbol{1}_s\boldsymbol{1}_s^T \otimes \boldsymbol{I}_n =  \tau^2 \boldsymbol{I}_{ns} + \left(\boldsymbol{1}_s \otimes \boldsymbol{I}_n\right)\left(\boldsymbol{1}_s \otimes \boldsymbol{I}_n\right)^T
\]

Applying the Sherman-Morrison-Woodbury formula  with \(A = \tau^2 \boldsymbol{I}_{ns}\), \(U = \boldsymbol{1}_s \otimes \boldsymbol{I}_n\), \(C = \boldsymbol{I}_n \), and \(V = (\boldsymbol{1}_s \otimes \boldsymbol{I}_n) ^T\) yields

%\begin{theorem}[\textbf{Woodbury Matrix Identity} (or \textbf{Sherman-Morrison-Woodbury formula})] For \(A \in \mathbb{R}^{n \times n}\), \(U \in \mathbb{R}^{n \times k}\), \(C \in \mathbb{R}^{k \times k}\), and \(V \in \mathbb{R}^{v \times n}\),
%
%\[
%(A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}.
%\]
%
%\end{theorem}

\[
\boldsymbol{\Sigma}^{-1} = \frac{1}{\tau^2} \boldsymbol{I}_{ns} - \frac{1}{\tau^2} (\boldsymbol{1}_s \otimes \boldsymbol{I}_n) \left[ \boldsymbol{I}_n + (\boldsymbol{1}_s \otimes \boldsymbol{I}_n)^T \cdot \frac{1}{\tau^2} (\boldsymbol{1}_s \otimes \boldsymbol{I}_n) \right]^{-1}(\boldsymbol{1}_s \otimes \boldsymbol{I}_n)^T \cdot \frac{1}{\tau^2}
\] 

\[
= \frac{1}{\tau^2} \left( \boldsymbol{I}_{ns} - \left(\boldsymbol{1}_s \otimes \boldsymbol{I}_n\right) \left[ \tau^2 \boldsymbol{I}_n + \boldsymbol{1}_s^T\boldsymbol{1}_s \otimes \boldsymbol{I}_n \right]^{-1}\left(\boldsymbol{1}_s \otimes \boldsymbol{I}_n\right)^T \right)
\]

\[
= \frac{1}{\tau^2} \left( \boldsymbol{I}_{ns} - \left(\boldsymbol{1}_s \otimes \boldsymbol{I}_n\right) \left[ (\tau^2 + s)  \boldsymbol{I}_n \right]^{-1}\left(\boldsymbol{1}_s \otimes \boldsymbol{I}_n\right)^T \right)
\]

\[
= \frac{1}{\tau^2} \boldsymbol{I}_{ns} - \frac{1}{\tau^2(\tau^2 + s)} \boldsymbol{1}_s \boldsymbol{1}_s^T\otimes \boldsymbol{I}_n 
\]

\[
=  \begin{pmatrix}
\left( \frac{1}{\tau^2} - \frac{1}{\tau^2(\tau^2 + s)} \right) \boldsymbol{I}_n & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \cdots & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n \\
- \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \left( \frac{1}{\tau^2} - \frac{1}{\tau^2(\tau^2 + s)} \right) \boldsymbol{I}_n &  - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \cdots & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n \\
- \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n &  - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \left( \frac{1}{\tau^2} - \frac{1}{\tau^2(\tau^2 + s)} \right) \boldsymbol{I}_n  & \cdots & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
- \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n &  - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n  & \cdots  & \left( \frac{1}{\tau^2} - \frac{1}{\tau^2(\tau^2 + s)} \right) \boldsymbol{I}_n
\end{pmatrix}
\]

\[
=  \begin{pmatrix}
\frac{\tau^2+s - 1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \cdots & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n \\
- \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \frac{\tau^2+s-1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n &  - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \cdots & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n \\
- \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n &  - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & \frac{\tau^2+s-1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n  & \cdots & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
- \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n &  - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n & - \frac{1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n  & \cdots  & \frac{\tau^2+s-1}{\tau^2(\tau^2 +s)} \boldsymbol{I}_n
\end{pmatrix}.
\]


% 4b
\item If \(\omega_{jk}=0\), this means that features \(X_j\) and \(X_k\) are conditionally independent given all of the other features. (This is in contrast to the meaning of \(\sigma_{jk}= 0\), which is that \(X_j\) and \(X_k\) are unconditionally independent.) This interpretation also gives another answer for part (a) of this question: sparsity in the precision matrix does not necessarily imply sparsity in the covariance matrix (and vice versa) because conditional independence does not necessarily imply unconditional independence (and vice versa). 

\

By the same argument as in part (a), if \(\omega_{jk} = 0\) holds it does not necessarily hold that \(\sigma_{jk} =0\), since \(\boldsymbol{\Sigma}\) is the inverse of \(\boldsymbol{\Omega}\).

% 4c
\item \textbf{consider instead using method from \citet{Fan2008}.} I would estimate the precision matrix using the graphical lasso \citep{Friedman2008}. Let \(\boldsymbol{\hat{\Sigma}}\) be an estimate for the covariance matrix \(\Sigma\). Let \(S\) by the empirical covariance matrix; that is,

\[
S := \frac{1}{n} \sum_{i=1}^n (X^{(i)} - \overline{X})(X^{(i)} - \overline{X})^T
\]

where \(X^{(i)}\) is the \(i\)th row of \(X\) and \(\overline{X} = n^{-1}\sum_{i=1}^n X^{(i)}\). In our case, we assume the mean vector is known to be \(\boldsymbol{0}\), so we can instead use 

\[
S := \frac{1}{n} \sum_{i=1}^n X^{(i)}\left(X^{(i)}\right)^T.
\]

To find the function to optimize, we will find the likelihood function. Note that the density for a multivariate \(p\)-dimensional Gaussian distribution with known mean \(\boldsymbol{0}\) is

\[
f_{\boldsymbol{X}}(x_1, \ldots, x_p) = (2 \pi)^{-p/2} | \boldsymbol{\Sigma}|^{-1/2} \cdot \exp \left(- \frac{1}{2}\boldsymbol{x} ^T \boldsymbol{\Sigma}^{-1} \boldsymbol{x} \right).
\]

The likelihood function for \(n\) observations from this distribution is given by

\[
\mathcal{L}(\boldsymbol{\Sigma}^{-1}) = \prod_{i=1}^n f_{\boldsymbol{X_i}}(x_{1i}, \ldots, x_{pi}) = (2 \pi)^{-np/2} | \boldsymbol{\Sigma}|^{-n/2} \cdot \exp \left(- \frac{1}{2} \sum_{i=1}^n\left(\boldsymbol{X}^{(i)}\right)^T \boldsymbol{\Sigma}^{-1} \boldsymbol{X}^{(i)}\right)
\]

\[
= (2 \pi)^{-np/2} | \boldsymbol{\Sigma}|^{-n/2} \cdot \exp \left(- \frac{1}{2} \sum_{i=1}^n \Tr \left[ \left(\boldsymbol{X}^{(i)}\right)^T \boldsymbol{\Sigma}^{-1} \boldsymbol{X}^{(i)} \right]\right) \]

\[
= (2 \pi)^{-np/2} | \boldsymbol{\Sigma}|^{-n/2} \cdot \exp \left(- \frac{1}{2} \sum_{i=1}^n \Tr \left[ \boldsymbol{X}^{(i)} \left(\boldsymbol{X}^{(i)}\right)^T \boldsymbol{\Sigma}^{-1}  \right]\right)
\]

\[
= (2 \pi)^{-np/2} | \boldsymbol{\Sigma}|^{-n/2} \cdot \exp \left(- \frac{1}{2} \Tr \left[ \sum_{i=1}^n  \boldsymbol{X}^{(i)} \left(\boldsymbol{X}^{(i)}\right)^T \boldsymbol{\Sigma}^{-1}  \right]\right)
\]

\[
= (2 \pi)^{-np/2} | \boldsymbol{\Sigma}|^{-n/2} \cdot \exp \left(- \frac{1}{2} \Tr \left[ nS \boldsymbol{\Sigma}^{-1}  \right]\right) = (2 \pi)^{-np/2} \left| \boldsymbol{\Sigma}^{-1} \right|^{n/2} \cdot \exp \left(- \frac{n}{2} \Tr \left[ S \boldsymbol{\Sigma}^{-1}  \right]\right)
\]

Take the logarithm of this expression to get the log likelihood function:

\[
\log \mathcal{L}(\boldsymbol{\Sigma}^{-1})  = - \frac{np}{2} \log (2 \pi) + \frac{n}{2}  \log \left| \boldsymbol{\Sigma}^{-1} \right| - \frac{n}{2} \Tr \left( S \boldsymbol{\Sigma}^{-1}  \right)\
\]

Since we are only concerned with the arguments maximizing the log likelihood function, we can disregard the first constant term and the multiplicative constants, leaving a simpler expression maximized by the same matrix

\[
\log \mathcal{L}(\boldsymbol{\Sigma}^{-1})  \propto \log \left| \boldsymbol{\Sigma}^{-1} \right| - \Tr \left( S \boldsymbol{\Sigma}^{-1}  \right)\
\]

Lastly, to impose sparsity we will add an \(\ell_1\) penalty. We will optimize the \(\ell_1\)-penalized log likelihood function

\begin{equation}\label{2018.screen.4.c.objective}
\hat{\Omega} := \underset{\Omega \in \mathcal{S}^+}{\arg \max} \left\{ \log \left| \Omega \right| - \Tr(S \Omega) + \lambda \lVert \Omega \rVert_1 \right\}
\end{equation}

where \( \mathcal{S}^+\) is the set of nonnegative definite \(p \times p\) matrices and \(\lambda > 0\) is a penalty parameter. Next, we will discuss the algorithm to optimize this function. We will make use of the following partitions of \(\boldsymbol{\hat{\Sigma}}\) and \(S\):

\begin{equation}\label{2018.screen.4.c.partitions}
\boldsymbol{\hat{\Sigma}} = \begin{pmatrix} 
\hat{\Sigma}_{11} & \hat{\Sigma}_{12} \\
\hat{\Sigma}_{12}^T & \hat{\sigma}_{22}
\end{pmatrix}, \qquad S = \begin{pmatrix} 
S_{11} & S_{12} \\
S_{12}^T & S_{22}
\end{pmatrix}, \qquad \hat{\Omega} = \begin{pmatrix} 
\hat{\Omega}_{11} & \hat{\omega}_{12} \\
\hat{\omega}_{12}^T & \hat{\omega}_{22}
\end{pmatrix},
\end{equation}

as well as the constraint

\begin{equation}\label{2018.screen.4.c.constraint}
 \begin{pmatrix} 
\hat{\Sigma}_{11} & \hat{\sigma}_{12} \\
\hat{\sigma}_{12}^T & \hat{\sigma}_{22}
\end{pmatrix}   \begin{pmatrix} 
\hat{\Omega}_{11} & \hat{\omega}_{12} \\
\hat{\omega}_{12}^T & \hat{\omega}_{22}
\end{pmatrix}=\begin{pmatrix}
\boldsymbol{I}_{p-1} & 0 \\
0^T & 1
\end{pmatrix}
\end{equation}

suggested by the identity \(\Sigma \Omega = \boldsymbol{I}_p\). The proposed procedure to optimize (\ref{2018.screen.4.c.objective}) is the following coordinate descent algorithm:

\begin{enumerate}[1.]

\item Initialize the algorithm with estimate \(\boldsymbol{\hat{\Sigma}} = S + \lambda \boldsymbol{I}_p\). (The diagonal of \(\boldsymbol{\hat{\Sigma}}\) remains unchanged for the rest of the algorithm.)

\item For each \(j = 1, 2, \ldots, p, 1, 2, \ldots, p, \ldots, \) switch the rows and columns of \(\boldsymbol{\hat{\Sigma}}\) so that the row and column corresponding to feature \(j\) come last, as in partition  (\ref{2018.screen.4.c.partitions}). Then solve the lasso problem

\begin{equation}\label{2008.screen.4.c.lasso.prob}
\hat{\beta} = \underset{\beta \in \mathbb{R}^{p-1}}{\arg \min} \left\{ \frac{1}{2} \left\lVert \hat{\Sigma}_{11}^{1/2} \beta - \hat{\Sigma}_{11}^{-1/2}s_{12}  \right\rVert_2^2 + \lambda \lVert \beta \rVert_1 \right\}
\end{equation}

%Note that this problem takes as input the inner products \(\hat{\Sigma}_{11}\) and \(s_{12}\). 

\item Fill in the corresponding row and column of \(\boldsymbol{\hat{\Sigma}}\) using \(\hat{\sigma}_{12} = \hat{\Sigma}_{11} \hat{\beta}\). (Again, the diagonal term \(\hat{\sigma}_{22}\) remains as it was after step 1.)

\item Continue until convergence; that is, until the average absolute change in \(\boldsymbol{\hat{\Sigma}}\) is less than \(t \cdot \operatorname{ave} | S^{- \text{diag}} |\), where \( S^{- \text{diag}}\) are the off-diagonal elements of \(S\) and \(t\) is a fixed threshold (\(t = 0.001\) is recommended by \citet{Friedman2008}).

\item Estimate \(\hat{\Omega}\) by using \(\boldsymbol{\hat{\Sigma}}\) to compute \(\hat{\omega}_{22}\) for each feature and filling in the corresponding row of \(\hat{\Omega}\) as in (\ref{2018.screen.4.c.partitions}) using the formulae

\begin{equation}\label{2018.screen.4.c.precision.matrix.formula}
\hat{\omega}_{22} = 1/ \left( \hat{\sigma}_{22} - \hat{\sigma}_{12}^T \hat{\beta}\right), \qquad \hat{\omega}_{12} = - \hat{\beta}\hat{\omega}_{22}.
\end{equation}

\end{enumerate}


\begin{remark}
Formula (\ref{2008.screen.4.c.lasso.prob}) is justified as follows: \citet{Banerjee2008} show that \(\hat{\Sigma}_{12}\) satisfies

\begin{equation}\label{2018.screen.4.c.lasso.primal}
\hat{\Sigma}_{12} = \underset{y}{\arg \min} \left\{y^T \boldsymbol{\hat{\Sigma}}^{-1}y: \lVert y - S_{12} \rVert_\infty \leq p \right\}.
\end{equation}

Using strong duality, they then show that the dual problem (\ref{2008.screen.4.c.lasso.prob}) is equivalent; specifically, if \(\hat{\beta}\) solves (\ref{2008.screen.4.c.lasso.prob}) then \(\hat{\sigma}_{12} = \hat{\Sigma}_{11}\hat{\beta}\) solves (\ref{2018.screen.4.c.lasso.primal}).
\end{remark}

\begin{remark}
Formulae (\ref{2018.screen.4.c.precision.matrix.formula}) are justified as follows: from (\ref{2018.screen.4.c.constraint}) we have the following identities

\[
\hat{\Sigma}_{11} \hat{\omega}_{12} + \hat{\sigma}_{12} \hat{\omega}_{22} = 0, \qquad \hat{\sigma}_{12}^T \hat{\omega}_{12} + \hat{\sigma}_{22} \hat{\omega}_{22} = 1.
\]

These yield

\[
\hat{\omega}_{12} = - \hat{\Sigma}_{11}^{-1} \hat{\sigma}_{12} \hat{\omega}_{22}, \qquad \hat{\omega}_{22} = 1/ \left( \hat{\sigma}_{22} - \hat{\sigma}_{12}^T \hat{\Sigma}_{11}^{-1} \hat{\sigma}_{12} \right).
\]

Then using \(\hat{\sigma}_{12} = \hat{\Sigma}_{11} \hat{\beta} \iff \hat{\beta} =  \hat{\Sigma}_{11} ^{-1} \hat{\sigma}_{12}\), we have (\ref{2018.screen.4.c.precision.matrix.formula}).
\end{remark}

% 4d
\item

\end{enumerate}

\end{exercise}

% Problem 5
\begin{exercise}[\textbf{Optimization}]

\begin{enumerate}[(a)]

% 5a
\item We can express the original optimization problem

\begin{equation}\label{2018.screen.5.a.objective}
\begin{aligned}
& \underset{\beta \in \mathbb{R}^p}{\text{minimize}}
& & \frac{1}{2} \lVert y - X \beta \rVert_2^2 + \lambda \lVert \beta \rVert_1
\end{aligned}
\end{equation}

as 

\begin{equation}\label{2018.screen.5.a.objective.alt}
\begin{aligned}
& \underset{\beta \in \mathbb{R}^p, z \in \mathbb{R}^n}{\text{minimize}}
& & \frac{1}{2} \lVert y - z \rVert_2^2 + \lambda \lVert \beta \rVert_1 \\
& \text{subject to}
& & z = X \beta.
\end{aligned}
\end{equation}

We will also refer to another expression of the lasso optimization problem,

\begin{equation}\label{2018.screen.5.a.objective.orig}
\begin{aligned}
& \underset{\beta \in \mathbb{R}^p}{\text{minimize}}
& & \frac{1}{2} \lVert y - X \beta \rVert_2^2 \\
& \text{subject to}
& & \lVert\beta \rVert_1 \leq t
\end{aligned}
\end{equation}

for some \(t >0\). The Lagrangian of (\ref{2018.screen.5.a.objective.alt}) is

\[
\mathcal{L}(\beta, z, u) = \frac{1}{2} \lVert y - z \rVert_2^2 + \lambda \lVert \beta \rVert_1 + u^T(z - X \beta),
\]

so the Lagrange dual function is

\[
\inf_{\beta, z} \left\{ \mathcal{L}(x, u)\right\}  = \inf_{\beta, z} \left\{\frac{1}{2} \lVert y - z \rVert_2^2 + \lambda \lVert \beta \rVert_1 + u^T(z - X \beta)  \right\}
\]

\[
= \inf_{\beta, z} \left\{\frac{1}{2} (y-z)^T(y-z) + u^T z + \lambda \lVert \beta \rVert_1  - u^T X \beta  \right\} 
\]

This minimization is separable:

\begin{equation}\label{2018.screen.5.a.a}
= \inf_{z} \left\{\frac{1}{2} \left(y^Ty - 2 y^Tz + z^Tz \right) + u^T z \right\} + \inf_{\beta} \left\{ \lambda \lVert \beta \rVert_1  - u^T X \beta  \right\}
\end{equation}

We will handle each part of (\ref{2018.screen.5.a.a}) separately. First, the left side:

\[
 \inf_{z} \left\{\frac{1}{2} \left(y^Ty - 2 y^Tz + z^Tz \right) + u^T z \right\} = \inf_{z} \left\{\frac{1}{2}z^Tz  + (u - y)^Tz + \frac{1}{2} y^Ty   \right\} 
\]

Since this is a convex quadratic form, differentiate with respect to \(z\) and set equal to zero:

\begin{equation}\label{other.part.result}
z + (u - y) = 0 \implies z = y - u
\end{equation}

\[
 \implies \inf_{z} \left\{\frac{1}{2}z^Tz  + (u - y)^Tz + \frac{1}{2} y^Ty   \right\} =  \frac{1}{2}(y - u) ^T(y - u)  + (u - y)^T(y - u) + \frac{1}{2} y^Ty 
\]

\[
=  \frac{1}{2}\left(y^Ty -2 u^Ty + u^Tu \right)  + 2u^Ty - y^Ty - u^Tu + \frac{1}{2} y^Ty  = -\frac{1}{2}u^Tu   + u^Ty = \frac{1}{2} y^Ty - \frac{1}{2}y^Ty  + u^Ty - \frac{1}{2} u^Tu 
\]

\[
= \frac{1}{2} y^Ty - \frac{1}{2}(y^Ty - 2u^Ty + u^Tu ) = \frac{1}{2} y^Ty - \frac{1}{2}(y - u)^T(y - u)= \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert y - u \rVert_2^2 
\]

Next we will minimize the right side of (\ref{2018.screen.5.a.a}):

\[
 \inf_{\beta} \left\{ \lambda \lVert \beta \rVert_1  - u^T X \beta  \right\} =  \inf_{\beta} \left\{ \lambda \sum_{i=1}^p | \beta_i| - \sum_{i=1}^p \begin{bmatrix} u^T X \end{bmatrix}_i  \beta_i  \right\}  =  \inf_{\beta} \left\{ \sum_{i=1}^p  \left( \lambda  | \beta_i| -  \begin{bmatrix} u^T X \end{bmatrix}_i  \beta_i \right)  \right\} 
 \]
 
% \[
% = \begin{cases}
%   \inf_{\beta} \left\{ \sum_{i=1}^p  \left( \lambda  | \beta_i| -  \begin{bmatrix} u^T X \end{bmatrix}_i  \beta_i \right)  \right\} & \beta_i 
%   \end{cases}
% \]
 
 \[
 =  \inf_{\beta} \left\{ \sum_{i=1}^p  \left(\operatorname{sgn}(\beta_i) \lambda-  \begin{bmatrix} u^T X \end{bmatrix}_i  \right)    \beta_i  \right\}   =  \sum_{i=1}^p  \inf_{\beta_i} \left\{ \left(\operatorname{sgn}(\beta_i) \lambda-  \begin{bmatrix} u^T X \end{bmatrix}_i  \right)    \beta_i  \right\} .
 \]
 
Notice that when \(\beta_i\) is negative, if \(\left(\operatorname{sgn}(\beta_i) \lambda-  \begin{bmatrix} u^T X \end{bmatrix}_i  \right) =  -\left(\lambda+  \begin{bmatrix} u^T X \end{bmatrix}_i  \right)\) is positive there is no lower bound on the quantity we are minimizing; otherwise, when \(\beta_i\) is negative the infimum is 0. When \(\beta_i\) is positive, if  \(\left(\operatorname{sgn}(\beta_i) \lambda-  \begin{bmatrix} u^T X \end{bmatrix}_i  \right) =  \left(\lambda-  \begin{bmatrix} u^T X \end{bmatrix}_i  \right)\) is negative there is no lower bound on the quantity we are minimizing; otherwise, when \(\beta_i\) is negative the infimum is 0. That is, the only dual feasible points satisfy for all \(i\)

\[
  -\left(\lambda+  \begin{bmatrix} u^T X \end{bmatrix}_i  \right) \leq 0, \qquad \lambda-  \begin{bmatrix} u^T X \end{bmatrix}_i   \geq 0 \iff \begin{bmatrix} u^TX\end{bmatrix}_i \geq -\lambda, \qquad \begin{bmatrix} u^TX\end{bmatrix}_i \leq \lambda
\]

which is equivalent to the condition
\[
\lVert u^TX \rVert_\infty \leq \lambda.
\]

Therefore the Lagrange dual function is

%The function we are minimizing is linear in \(\beta\) whenever \(\beta_i < 0 \) for any \(i\), so it decreases without bound if there exists any \(i\) such that \(\beta_i < 0\) and \(\lambda-  \begin{bmatrix} u^T X \end{bmatrix}_i  < 0\). That is, the infimum does not exist unless for all \(i \in \{1, \ldots, p\}\)

%\[
%\lambda-  \begin{bmatrix} u^T X \end{bmatrix}_i  \geq 0 \iff \lambda \geq  \begin{bmatrix} u^T X \end{bmatrix}_i
%\]
 

%The function we are minimizing is convex in \(\beta\). Differentiate and set equal to 0:
%
%\[
%\lambda \begin{bmatrix} \operatorname{sgn}(\beta_i) \end{bmatrix} - u^T X = 0 \iff \begin{bmatrix} \operatorname{sgn}(\beta_i) \end{bmatrix}  = \frac{1}{\lambda} u^T X 
%\]
%
%where \(\begin{bmatrix} \operatorname{sgn}(\beta_i) \end{bmatrix} \) denotes the vector resulting from operating the \(\operatorname{sgn}(\cdot)\) function elementwise on \(\beta\).
% 
% \[
% \vdots
% \]

\begin{equation}\label{2018.screen.5.a.dual}
 \inf_{\beta, z} \left\{ \mathcal{L}(x, u)\right\}  = \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert y - u \rVert_2^2 
\end{equation}

subject to the constraint \(\lVert u^TX \rVert_\infty \leq \lambda\). This quantity represents a lower bound on the minimum value of the original optimization problem for all \(u \in \mathbb{R}^p\). The dual problem is to find the best lower bound by maximizing over \(u\); that is, the dual problem is

\begin{equation}\label{2018.screen.5.a.dual.ans}
\begin{aligned}
& \underset{u \in \mathbb{R}^p}{\text{maximize}}
& & \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert y - u \rVert_2^2  \\
& \text{subject to}
& & \lVert u^TX \rVert_\infty \leq \lambda.
\end{aligned}
\end{equation}

Lastly, suppose \(\hat{\beta}\) and \(\hat{u}\) satisfy

\[
\hat{\beta} = 
\begin{aligned}
& \underset{\beta \in \mathbb{R}^p}{\arg \min}
& & \frac{1}{2} \lVert y - X \beta \rVert_2^2 + \lambda \lVert \beta \rVert_1
\end{aligned},
\]

\[
 \qquad \hat{u} = 
\begin{aligned}
& \underset{u \in \mathbb{R}^p}{\arg \max}
& & \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert y - u \rVert_2^2  \\
& \text{subject to}
& & \lVert u^TX \rVert_\infty \leq \lambda
\end{aligned} = 
\begin{aligned}
& \underset{u \in \mathbb{R}^p}{\arg \min}
& & -\frac{1}{2} \lVert y \rVert_2^2 + \frac{1}{2} \lVert y - u \rVert_2^2  \\
& \text{subject to}
& & \lVert u^TX \rVert_\infty \leq \lambda
\end{aligned}
\]

Then by (\ref{other.part.result}) and strong duality we have \(\hat{u} = y - X \hat{\beta}\).

% 5b
\item

\begin{enumerate}[(i)]

% 5(b)i
\item \textbf{Not necessarily unique.} Per \citet{Tibshirani2013}, if \(\operatorname{rank}(X) < p\), the lasso solution is not necessarily unique. Intuitively, this is because the columns of \(X\) are linearly dependent, so there may exist more than one linear combination of the columns that minimizes (\ref{2018.screen.5.a.objective}). \textbf{Jacob's suggestion: counterexample. X is two columns that are equal; then convex combinations of two solutions are equal as long as same sign (can't be opposite sign because then \(\ell_1\) could be smaller by setting one equal to 0.}

% 5(b)ii
\item \textbf{Necessarily unique.} The dual problem (\ref{2018.screen.5.a.dual.ans}) is strictly concave, so the value \(\hat{u}\) that maximizes it is unique.

%  \textbf{flesh out more} By a result from part (a), \(\hat{u} = y - X \hat{\beta}\). By part (iii), even though \(\hat{\beta}\) is not unique, \(X \hat{\beta}\) is (see also Lemma 1 in \citet{Tibshirani2013}). Therefore \(\hat{u}\) is unique.

%\textbf{Jacob's solution: strictly convex optimization problem, so argument maximizing is unique. dual is always convex; } 

% 5(b)iii
\item \textbf{Necessarily unique} (except in the trivial case \(\lambda=0\)). Per part 5(b)(iv), \(\lVert \hat{\beta} \rVert_1\) is unique. (\ref{2018.screen.5.a.objective}) is convex, so the minimum \(\frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \hat{\beta} \rVert_1\) is unique. Therefore \( \lVert y - X \hat{\beta} \rVert_2^2\) must be unique.

\textbf{Jacob's solution:} Since \(\hat{u}\) is unique and by (\ref{other.part.result}) \(\hat{u} = y - X \hat{\beta}\), we must have that \(\lVert \hat{u} \rVert = \lVert y - X \hat{\beta} \rVert\) is unique.

%\textbf{Necessarily unique.} The optimization problem is convex in \(\beta\), so it has a unique minimum value \(\frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \hat{\beta} \rVert_1\). 

% 5(b)iv
\item \textbf{Necessarily unique} (except in the trivial case \(\lambda=0\)). Whenever \(\lambda > 0\), the lasso objective function (\ref{2018.screen.5.a.objective}) is the Lagrangian of (\ref{2018.screen.5.a.objective.orig}). We will prove a useful lemma about the relationship between these functions.

\begin{lemma}\label{2018.screen.5.b.iv.relation}
For a given \(\lambda >0\), let \(\hat{\beta}\) minimize (\ref{2018.screen.5.a.objective}). Then there is exactly one \(t = \lVert \hat{\beta} \rVert_1\) such that any \(\hat{\beta}\) minimizing (\ref{2018.screen.5.a.objective}) also minimizes (\ref{2018.screen.5.a.objective.orig}).

\end{lemma}

\begin{proof}
This must be true by contradiction. First of all, since the objective function of (\ref{2018.screen.5.a.objective.orig}) is continuous and the feasible region \(\lVert \beta \rVert_1 \leq t\) is compact, a minimum of (\ref{2018.screen.5.a.objective.orig}) is guaranteed to exist. Now suppose \(\hat{\beta}\) minimizes (\ref{2018.screen.5.a.objective}) for a fixed \(\lambda\), with \(\lVert \hat{\beta} \rVert_1 = t\), but there is a different solution \(\hat{\beta}^*\) that is feasible for (\ref{2018.screen.5.a.objective.orig}) and achieves a lower value. That is,

\[
\frac{1}{2} \lVert y - X \hat{\beta}^* \rVert_2^2  < \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2
\]

and \(\lVert\hat{\beta}^*\rVert_1 \leq \lVert\hat{\beta}\rVert_1 = t\). Since \(\lambda > 0\), \(\lVert\hat{\beta}\rVert_1 < \lVert \hat{\beta}_{global} \rVert_1\), where \(\hat{\beta}_{global}\) is a global minimum for \(\frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2\). Since (\ref{2018.screen.5.a.objective.orig}) is convex and all global minima lie outside the feasible region, \(\hat{\beta}^*\) lies on the boundary; that is, \(\lVert \hat{\beta}^* \rVert_1 = \lVert\hat{\beta}\rVert_1 = t\). But then

\[
\frac{1}{2} \lVert y - X \hat{\beta}^* \rVert_2^2  < \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 \iff \frac{1}{2} \lVert y - X \hat{\beta}^* \rVert_2^2 + \lambda \lVert\hat{\beta}^* \rVert_1 < \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 +  \lambda \lVert\hat{\beta} \rVert_1
\]

which contradicts the fact that \(\hat{\beta}\) minimizes (\ref{2018.screen.5.a.objective}). 

\end{proof}

Now the result follows in a simple way:

\begin{proposition}
Let \(\mathcal{B}\) be the set of all \(\hat{\beta}\) that minimize (\ref{2018.screen.5.a.objective}) for some fixed \(\lambda > 0\). Then for any two \(\hat{\beta}_1, \hat{\beta}_2 \in \mathcal{B}\), \(\lVert \hat{\beta}_1 \rVert_1 = \lVert \hat{\beta}_2 \rVert_1\). That is, \(\lVert \hat{\beta} \rVert_1\) is unique.
\end{proposition}

\begin{proof}
Suppose \(\hat{\beta}_1\) and \(\hat{\beta}_2\) both minimize (\ref{2018.screen.5.a.objective}), and (without loss of generality) \(\lVert \hat{\beta}_1 \rVert_1 < \lVert \hat{\beta}_2 \rVert_1\). By Lemma \ref{2018.screen.5.b.iv.relation}, these values both minimize (\ref{2018.screen.5.a.objective.orig}) with \(t =  \lVert \hat{\beta}_2 \rVert_1\) (we cannot choose \(t =  \lVert \hat{\beta}_1 \rVert_1\) because \(\hat{\beta}_1\) is not feasible for that problem). Because the global minimum of (\ref{2018.screen.5.a.objective.orig}) lies outside the feasible region and  (\ref{2018.screen.5.a.objective.orig}) is convex, all solutions to (\ref{2018.screen.5.a.objective.orig}) lie on the boundary of the feasible region. But \(\lVert\hat{\beta}_1 \rVert_1 < \lVert \hat{\beta}_2 \rVert_1\), so \(\hat{\beta}_1\) is not on the boundary of the feasible region, contradiction. Therefore \(\lVert \hat{\beta}_1 \rVert_1 = \lVert \hat{\beta}_2 \rVert_1\) for all solutions \(\hat{\beta}_1\), \(\hat{\beta}_2\) to (\ref{2018.screen.5.a.objective}); that is,  \(\lVert \hat{\beta} \rVert_1\) is unique.
\end{proof}

(See \citet{Osborne2000} for more details.)

%for (\ref{2018.screen.5.a.objective.orig}) and therefore for (\ref{2018.screen.5.a.objective}). 


%

% \textbf{change this argument: if you solve 1 and take the norm of that and choose that value for \(t\) then you will get the same solution. because if there existed a better solution then it would have made the objective function 1 better.}
 
% Further, they are optimized over the same variable and they are both convex, so the solution set of (\ref{2018.screen.5.a.objective}) is identical to the solution set of (\ref{2018.screen.5.a.objective.orig}).

%(\ref{2018.screen.5.a.objective.orig}) is convex and Slater's condition holds because every point \(\{\beta \in \mathbb{R}^p \mid \lVert \beta \rVert_1 < t\}\) is feasible. Therefore for this dual function, strong duality holds; that is, the optimal values of (\ref{2018.screen.5.a.objective}) and  (\ref{2018.screen.5.a.objective.orig}) are equal. 

\end{enumerate}

% 5c
\item 

\begin{enumerate}[(i)]

% 5(c)i
\item Since \(\beta^*\) is clearly feasible for (\ref{2018.screen.5.a.objective}) and \(\hat{\beta}\) achieves the minimum, we have

\[
\frac{1}{2} \lVert y - X \beta^* \rVert_2^2 + \lambda \lVert \beta^* \rVert_1 \geq \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \hat{\beta} \rVert_1 \iff  \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \hat{\beta} \rVert_1 \leq \frac{1}{2} \lVert\epsilon \rVert_2^2 + \lambda \lVert \beta^* \rVert_1
\]


% 5(c)ii
\item

% \textbf{Jacob's solution: plug in \(\epsilon\) for \(u\) in the lower bound equation, then you get \(X \beta^*\)}

We know that the expression in the dual problem (\ref{2018.screen.5.a.dual.ans}) is a lower bound for the solution of the primal problem (\ref{2018.screen.5.a.objective}) for any \(u\) feasible for (\ref{2018.screen.5.a.dual.ans}) (that is, any \(u\) satisfying \(\lVert u^TX \rVert_\infty \leq \lambda\)). Therefore we have

\[
\frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \beta \rVert_1 \geq  \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert y - u \rVert_2^2.
\]

Since by assumption \(\lambda \geq \lVert X^T \epsilon \rVert_\infty\), \(\epsilon\) is feasible for (\ref{2018.screen.5.a.dual.ans}). Therefore we have

\begin{equation}\label{2018.screen.5.c.ii.result}
\frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \beta \rVert_1 \geq  \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert y - \epsilon \rVert_2^2 =  \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert X \beta^* \rVert_2^2
\end{equation}

as desired.

%\[
%\vdots
%\]
%
%%From part (a), since the optimal value of the dual (\ref{2018.screen.5.a.dual}) is a lower bound for the optimal value of the primal (\ref{2018.screen.5.a.objective}), we have
%%
%%\begin{equation}\label{2018.screen.5.c.d}
%% \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert y - \hat{u} \rVert_2^2  \leq \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \hat{\beta} \rVert_1.
%%\end{equation}
%%
%%
%%
%%so we are done if \(\lVert X \beta^* \rVert_2^2 \geq \lVert y - \hat{u} \rVert_2^2\). Also by part (a), \(\hat{u} = y - X \hat{\beta}\), so \( \lVert y - \hat{u} \rVert_2^2 = \lVert X \hat{\beta} \rVert_2^2 \), so we are done if \(\lVert X \beta^* \rVert_2^2 \geq \lVert X \hat{\beta} \rVert_2^2 \). 
%
%%From part (c)(i), we have
%%
%%\[
%% \frac{1}{2}\left[  ( y - X \hat{\beta})^T( y - X \hat{\beta})  - ( y - X \beta^*)^T( y - X \beta^*)  \right] \leq \lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1)
%%\]
%%
%%\[
%%\iff 2 y^T X (\beta^* -  \hat{\beta} ) +  (\hat{\beta})^TX^TX\hat{\beta} - (\beta^*)^TX^TX\beta^* \leq 2\lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1)
%%\]
%%
%%\begin{equation}\label{2018.screen.5.c.c}
%%\iff \lVert X \beta^* \rVert_2^2  -  \lVert X \hat{\beta} \rVert_2^2   \geq  2 y^T X ( \beta^* - \hat{\beta} )  - 2\lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1 ) 
%%\end{equation}
%%
%%So (\ref{2018.screen.5.c.c}) is a sufficient condition for the result. Examining the right side of the right side of (\ref{2018.screen.5.c.c}), we have
%%
%%\begin{equation}\label{2018.screen.5.c.a}
%%2 y^T X ( \beta^* - \hat{\beta} )  - 2\lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1 ) =  2y^T (y - \epsilon -X \hat{\beta} ) - 2\lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1)  
%%\end{equation}
%%
%%
%%%Again, note that \(\lambda > 0\) corresponds to a problem with constraint \(\lVert \beta \rVert_1 \leq t \leq \lVert \beta^*\rVert\) (see the explanation in problem 5(b)(iv)). So \(\lVert \hat{\beta} \rVert_1 = t \leq \lVert \beta^*\rVert\). 
%
%Borrowing notation from \citet{Osborne2000}, note that the subdifferential of (\ref{2018.screen.5.a.objective}) is given by
%
%\[
%\partial_\beta \mathcal{L}(\beta, \lambda) = \partial_\beta  \left(  \frac{1}{2}(y - X \beta)^T(y - X \beta) + \lambda \lVert \beta \rVert_1\right) = \partial_\beta  \left(  \frac{1}{2}(y^Ty - 2 y^TX \beta + \beta^T X^T X \beta) + \lambda \lVert \beta \rVert_1\right) 
%\]
%
%\[
%= -y^TX + X^TX \beta+ \lambda v = -X^T(y - X \beta) + \lambda v
%\]
%
%where \(v = (v_1, \dots, v_p)^T\) with \(v_i = \operatorname{sgn}(\beta_i)\) if \(\beta_i \neq 0\) and \(v_i \in [-1, 1]\) if \(\beta_i  =0\). By the convexity of (\ref{2018.screen.5.a.objective}), for \(\hat{\beta}\) minimizing (\ref{2018.screen.5.a.objective}) it must hold that 
%
%\[
%0 = -X^T(y - X \beta) + \lambda v \iff  \lambda v^T \hat{\beta} = (y - X \hat{\beta}) ^TX \hat{\beta} \iff \lambda = \frac{(y - X \hat{\beta})^TX \hat{\beta}}{\lVert \hat{\beta} \rVert_1}
%\]
%
%\begin{equation}\label{2018.screen.5.c.b}
%\iff \lVert \hat{\beta} \rVert_1 = \frac{(y - X \hat{\beta})^TX \hat{\beta}}{\lambda}
%\end{equation}
%
%where we used \(v^T \hat{\beta} = \lVert \hat{\beta} \rVert_1 \). Substituting (\ref{2018.screen.5.c.b}) into (\ref{2018.screen.5.c.a}), we have
%%
%%Substituting this identity and (\ref{2018.screen.5.c.b}) into (\ref{2018.screen.5.c.d}), we have that a sufficient condition to prove our result is 
%%
%%\[
%% \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert y - \hat{u} \rVert_2^2  \leq \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + (y - X \hat{\beta})^TX \hat{\beta}
%% \]
%% 
%% \[
%%\iff  \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert X \hat{\beta} \rVert_2^2  \leq \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + (y - X \hat{\beta})^TX \hat{\beta}
%% \]
%% 
%%  \[
%%\iff  \frac{1}{2} y^Ty - \frac{1}{2} ( \hat{\beta})^T X^TX \hat{\beta}  \leq \frac{1}{2} \left(y^Ty - 2 y^T X \hat{\beta} + (\hat{\beta})^TX^TX\hat{\beta} \right) +y^T X \hat{\beta} - (\hat{\beta})^TX^T X \hat{\beta}
%% \]
%% 
%%   \[
%%\iff 0 \leq \frac{1}{2} \left(- 2 y^T X \hat{\beta}  \right) +y^T X \hat{\beta}  = 0
%% \]
% 
%
%%\[
%%\vdots
%%\]
%
%
%
%\[
%2y^T (y - \epsilon -X \hat{\beta} ) -  2\lambda \left(\lVert \beta^* \rVert_1 - \frac{(y - X \hat{\beta})^TX \hat{\beta}}{\lambda} \right)  
%\]
%
%\[
%=2 y^T y - 2y^T \epsilon - 2y^TX \hat{\beta}  -2 \lambda\lVert \beta^* \rVert_1  + 2y^TX \hat{\beta}  -2  \hat{\beta}^T X^T X \hat{\beta} = 2y^T (y -  \epsilon)  -2 \lambda\lVert \beta^* \rVert_1   -  2\hat{\beta}^T X^T X \hat{\beta}
%\]
%
%\[
%= 2y^T X \beta^* - 2\lambda\lVert \beta^* \rVert_1   -  2\lVert X \hat{\beta} \rVert_2^2  =  2(X \beta^* + \epsilon)^T X \beta^*  - 2 \lambda\lVert \beta^* \rVert_1   - 2\lVert X \hat{\beta} \rVert_2^2
%\]
%
%\[
%= 2 \lVert X \beta^* \rVert_2^2 + 2\epsilon^T X \beta^* - 2\lambda\lVert \beta^* \rVert_1   - 2 \lVert X \hat{\beta} \rVert_2^2  = 2 \left( \lVert X \beta^* \rVert_2^2  - \lVert X \hat{\beta} \rVert_2^2\right)  - 2 \left( \lambda\lVert \beta^* \rVert_1  -   \epsilon^T X \beta^* \right)
%\]
%
%\[
%= 2 \left( \lVert X \beta^* \rVert_2^2  - \lVert X \hat{\beta} \rVert_2^2\right)  - 2 \sum_{i=1}^p \left(\operatorname{sgn}(\beta_i) \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i \right) \beta_i^* 
%\]
%
%Because \(\lambda \geq \lVert X^T \epsilon \rVert_\infty\) (that is, for all \(i \in \{1, \ldots, p\}\), \(\lambda -  \left| \begin{bmatrix} X^T \epsilon \end{bmatrix}_i \right| \geq 0 \) ) we have that for all \(i \in \{1, \ldots, p\}\),
%
%
%%\(\epsilon^T X \preceq \lambda \boldsymbol{1}^T \)
%
%\[
% \left(\operatorname{sgn}(\beta_i) \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i \right) \beta_i^*  \geq 0
% \]
% 
% by the following argument. If \(\beta_i^* = 0\), the result is trivial. If \(\beta_i^* > 0\), we have
% 
% \[
%  \operatorname{sgn}(\beta_i) \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i  =   \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i  \geq \lambda -  \left| \begin{bmatrix} X^T \epsilon \end{bmatrix}_i \right|  \geq 0
%  \]
%  
%  \[
%   \implies  \left(\operatorname{sgn}(\beta_i) \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i \right) \beta_i^*  \geq 0.
%  \]
%  
%  Lastly, if \(\beta_i^* < 0\), we have
%  
%   \[
%  \operatorname{sgn}(\beta_i) \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i  =   - \left( \lambda +   \begin{bmatrix} \epsilon^T X \end{bmatrix}_i \right)  \leq - \left( \lambda -  \left| \begin{bmatrix} X^T \epsilon \end{bmatrix}_i \right|  \right) \leq 0
%  \]
%  
%  \[
%   \implies  \left(\operatorname{sgn}(\beta_i) \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i \right) \beta_i^*  \geq 0.
%  \]
%  
%  Therefore we have
%  
%  \[
%2 \left( \lVert X \beta^* \rVert_2^2  - \lVert X \hat{\beta} \rVert_2^2\right)  - 2 \sum_{i=1}^p \left(\operatorname{sgn}(\beta_i) \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i \right) \beta_i^*  \leq 2 \left( \lVert X \beta^* \rVert_2^2  - \lVert X \hat{\beta} \rVert_2^2\right) 
%\]
%
%\[
%\vdots
%\]
%
%\[
%\lVert X \beta^* \rVert_2^2  -  \lVert X \hat{\beta} \rVert_2^2   \geq  2 y^T X ( \beta^* - \hat{\beta} )  - 2\lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1 ) 
%\]
%
%\[
%\iff \lVert X \beta^* \rVert_2^2  -  \lVert X \hat{\beta} \rVert_2^2   \geq  2 (X \beta^* + \epsilon)^T X \beta^* - 2 y^T X\hat{\beta}   - 2\lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1 ) 
%\]
%
%\[
%\iff \lVert X \beta^* \rVert_2^2  -  \lVert X \hat{\beta} \rVert_2^2   \geq   2\lVert X \beta^* \rVert_2^2 + 2 \epsilon^T X \beta^* - 2 y^T X\hat{\beta}   - 2\lambda (\lVert \beta^* \rVert_1 - \lVert \hat{\beta} \rVert_1 ) 
%\]
%
%%\[
%%\epsilon^T X \preceq \lambda \boldsymbol{1}^T  \iff  \ldots \iff  \sum_{i=1}^p \left(\operatorname{sgn}(\beta_i) \lambda-  \begin{bmatrix} \epsilon^T X \end{bmatrix}_i \right) \beta_i^* 
%%\]
%%
%%\[
%%\iff | \epsilon^T X \beta^* | \leq | \lambda \boldsymbol{1}^T \beta^* |= \lambda \lVert \beta^* \rVert_1.
%%\]
%
%
%% Therefore \(\lambda\lVert \beta^* \rVert_1  -  \epsilon^T X \beta^* \geq 0\), so we have that \(\lVert X \beta^* \rVert_2^2 \geq  \lVert X \hat{\beta} \rVert_2^2\) is a sufficient condition to prove the result.
%
%%which proves the result by (\ref{2018.screen.5.c.d}).
%
%%\[
%%\vdots
%%\]
%%
%%Further, if \(\hat{\beta} \neq 0\) then \(\lVert v \rVert_\infty = 1\), so from (\ref{2018.screen.5.c.b}) we have
%%
%%\[
%%\lambda = \frac{(y - X \hat{\beta})^TX \hat{\beta}}{\lVert \hat{\beta} \rVert_1} = \lVert X^T(y - X \hat{\beta}) \rVert_\infty.
%%\]
%%
%%\[
%%\vdots
%%\]
%
%% \(\lVert y - 

% 5(c)iii
\item 

%We already have from part (c)(i)
%
%%\[
%%\frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \hat{\beta} \rVert_1 \leq \frac{1}{2} \lVert\epsilon \rVert_2^2 + \lambda \lVert \beta^* \rVert_1
%%\]
%%
%%Note that
%%
%%\[
%%\frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \hat{\beta} \rVert_1 \leq \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \beta^* \rVert_1 
%%\]
%%
%%\[
%%\vdots
%%\]
%
%From part c(ii) we have
%
%\[
%\frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \hat{\beta} \rVert_1 \geq \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert X \beta^* \rVert_2^2 
%\]

We can rewrite the right side of (\ref{2018.screen.5.c.ii.result}) as 

\begin{equation}\label{2018.screen.5.c.ii.result.rewritten}
\frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert X \beta^* \rVert_2^2 = \frac{1}{2} \lVert X \beta^* \rVert_2^2 + \frac{1}{2} \lVert \epsilon \rVert_2^2 + \epsilon^T X \beta^* - \frac{1}{2} \lVert X \beta^* \rVert_2^2  =  \frac{1}{2} \lVert \epsilon \rVert_2^2 + \epsilon^T X \beta^*. 
\end{equation}

By assumption, we have 
\[
\lambda \geq \lVert X^T \epsilon \rVert_\infty \iff \lambda \boldsymbol{1}  - X^T \epsilon \succeq 0 \implies \lambda \boldsymbol{1} \beta^*  - X^T \epsilon \beta^* \succeq 0 
\]

\[
\iff - \lambda \lVert \beta^* \rVert_1 \leq \epsilon^T X \beta^* \leq \lambda \lVert \beta^* \rVert_1.
\]

By H\"{o}lder's Inequality, we have for any two vectors \(u, v \in \mathbb{R}^n\), \( | u^Tv | \leq \lVert u \rVert_\infty \lVert v \rVert_1\). Therefore

\[
| \epsilon^T X \beta^*| = | (X^T \epsilon)^T \beta^* | \leq \lVert X^T \epsilon \rVert_\infty \lVert \beta^* \rVert_1 \leq \lambda \lVert \beta^* \rVert_1
\]

where the last step used the assumption \(\lVert X^T \epsilon \rVert_\infty \leq \lambda\). So we have 

\[
 \frac{1}{2} \lVert \epsilon \rVert_2^2 +\lambda \lVert \beta^* \rVert_1 \leq \frac{1}{2} \lVert \epsilon \rVert_2^2 + \epsilon^T X \beta^* .
\]

Substituting in to (\ref{2018.screen.5.c.ii.result}) and using the identity in (\ref{2018.screen.5.c.ii.result.rewritten}) yields

\[
 \frac{1}{2} \lVert \epsilon \rVert_2^2 +\lambda \lVert \beta^* \rVert_1 \leq \frac{1}{2} \lVert \epsilon \rVert_2^2 + \epsilon^T X \beta^* = \frac{1}{2} \lVert y \rVert_2^2 - \frac{1}{2} \lVert X \beta^* \rVert_2^2  \leq \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \beta \rVert_1 
\]

as desired.

% 5(c)iv
\item  We see from parts (i) and (iii) that

\[
\frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \beta \rVert_1 - \lambda \lVert \beta^* \rVert_1  \leq \frac{1}{2} \lVert \epsilon \rVert_2^2 \leq  \frac{1}{2} \lVert y - X \hat{\beta} \rVert_2^2 + \lambda \lVert \beta \rVert_1 + \lambda \lVert \beta^* \rVert_1 
\]

\[
\iff \frac{1}{n} \lVert y - X \hat{\beta} \rVert_2^2 + \frac{2}{n} \lambda \lVert \beta \rVert_1 - \frac{2}{n}\lambda \lVert \beta^* \rVert_1  \leq \frac{1}{n} \lVert \epsilon \rVert_2^2 \leq  \frac{1}{n} \lVert y - X \hat{\beta} \rVert_2^2 + \frac{2}{n}\lambda \lVert \beta \rVert_1 +\frac{2}{n} \lambda \lVert \beta^* \rVert_1 
\]

that is, we can lower bound and upper bound \(\frac{1}{n} \lVert \epsilon \rVert_2^2 \) by taking the quantity \(\frac{1}{n} \lVert y - X \hat{\beta} \rVert_2^2 + \frac{2}{n} \lambda \lVert \beta \rVert_1\) and adding or subtracting \(\frac{2}{n}\lambda \lVert \beta^* \rVert_1\). Therefore it seems that the quantity in the middle of this interval,  \(\frac{1}{n} \lVert y - X \hat{\beta} \rVert_2^2 + \frac{2}{n} \lambda \lVert \beta \rVert_1\), is a reasonable estimator for \(\sigma^2 = \E \left[ n^{-1} \lVert \epsilon \rVert_2^2 \right]\).

\end{enumerate}

% 5d

\item 


\begin{proof}

\

\begin{center}
\noindent\fbox{
\parbox{0.9\textwidth}{

\begin{definition}[\textbf{Convex function in \(\mathbb{R}^n\)}]\label{cvx.defn.convex.multivar} Let $f:\mathbb{R}^n\to\mathbb{R}$.  We say that $f$ is \textbf{convex} if, for any $x,y\in\mathbb{R}^n$ and for any $t\in[0,1]$, we have

\begin{equation}\label{cvx.541a.hw6.5a}
f(tx+(1-t)y)\leq tf(x)+(1-t)f(y).
\end{equation}

\end{definition}

}
}
\end{center}

\

%If \(\theta \in \mathbb{R}\), we can simply write
%
%\[
%f(tx+(1-t)y) = \left( \lVert tx+(1-t)y \rVert_1 \right)^2 = \left(tx+(1-t)y \right)^2 = t^2x^2 + (1-t)^2y^2 + 2 t(1-t) xy 
%\]
%
%%Since \(t \in [0,1]\) and \(1-t \in [0,1]\), \(t \leq \sqrt{t} \) and \(1-t \leq \sqrt{1-t}\). 
%%
%%\[
%%\leq \ldots  =  \left( \sqrt{t} x\right)^2 +  \left( \sqrt{1-t}  y \right)^2  = t \left( \lVert x \rVert_1 \right)^2 + (1-t) \left(\lVert y \rVert_1 \right)^2 =  tf(x)+(1-t)f(y).
%%\]
%
%Further,
%
%\[
% tf(x)+(1-t)f(y) = t \left( \lVert x \rVert_1 \right)^2 + (1-t) \left(\lVert y \rVert_1 \right)^2  = tx^2 + (1-t)y^2.
%\]
%
%Taking the difference of these yields
%
%\[
% tf(x)+(1-t)f(y)  - f(tx+(1-t)y) = tx^2 + (1-t)y^2 -  \left( t^2x^2 + (1-t)^2y^2 + 2 t(1-t) xy \right) 
%\]
%
%\[
%= (t - t^2) x^2 + [ (1 - t) - (1-t)^2] y^2 - 2 t(1-t) xy = (t - t^2) x^2 + [ (1 - t) - (1-2t + t^2)] y^2 - 2 t(1-t) xy
%\]
%
%\[
%= (t - t^2) x^2 + ( t - t^2) y^2 - 2 (t-t^2) xy = t(1-t)(x^2 + y^2 - 2xy) = t(1-t)(x - y)^2 \geq 0 
%\]
%
%\[
%\iff  tf(x)+(1-t)f(y) \geq f(tx+(1-t)y)
%\]
%
%since \(t \geq 0, 1-t \geq 0\), and \((x-y)^2 \geq 0\) for all \(x, y \in \mathbb{R}\). So \(\left( \lVert \theta \rVert_1 \right)^2\) is convex if \(\theta \in \mathbb{R}\). If \(\theta \in \mathbb{R}^n\), \(n  \geq 2\), 

Note that

\begin{equation}\label{2018.screen.5.d.a}
\lVert tx+(1-t)y \rVert_1 \leq \lVert tx \rVert_1  + \lVert (1-t)y \rVert_1 = t \lVert x \rVert_1  + (1-t) \lVert y \rVert_1 
\end{equation}

where the first step follows by the Triangle Inequality (which all norms satisfy, including the \(\ell_1\) norm) and the second step follows by the homogeneity property of norms. Therefore \(\lVert \theta \rVert_1\) is convex. Next, by (\ref{2018.screen.5.d.a}) and the monotonicity of \(g(\theta) = \theta^2\) when \(\theta \geq 0\),

%Note that (\ref{2018.screen.5.d.a}) holds with equality if \(x \succeq \boldsymbol{0}_n\) and \(y \succeq \boldsymbol{0}_n\) or \(x \preceq \boldsymbol{0}_n\) and \(y \preceq \boldsymbol{0}_n\). If either of those conditions are met, we have

\[
f(tx+(1-t)y) = \left( \lVert tx+(1-t)y \rVert_1 \right)^2 \leq \left(  t \lVert x \rVert_1  + (1-t) \lVert y \rVert_1  \right)^2 
\]

\[
= t^2 \lVert x \rVert_1^2 + (1-t)^2 \lVert y \rVert_1^2 +2t(1-t) \lVert x \rVert_1 \lVert y \rVert_1 
\]

and

\[
tf(x) + (1-t)f(y) = t \lVert x \rVert_1^2 + (1-t) \lVert y \rVert_1^2
\]

Taking the difference of these yields

\[
tf(x) + (1-t)f(y)  - f(tx+(1-t)y)  \geq t \lVert x \rVert_1^2 + (1-t) \lVert y \rVert_1^2 - \left(  t^2 \lVert x \rVert_1^2 + (1-t)^2 \lVert y \rVert_1^2 +2t(1-t) \lVert x \rVert_1 \lVert y \rVert_1  \right)
\]

\[
=( t - t^2) \lVert x \rVert_1^2 + [(1-t) - (1-t)^2] \lVert y \rVert_1^2 - 2t(1-t)\lVert x \rVert_1 \lVert y \rVert_1 
\]

\[
=(t - t^2)\left( \lVert x \rVert_1^2 + \lVert y \rVert_1^2 - 2\lVert x \rVert_1 \lVert y \rVert_1 \right) = t(1-t)(\lVert x \rVert_1 - \lVert y \rVert_1)^2 \geq 0
\]

\[
\iff  tf(x) + (1-t)f(y)  \geq f(tx+(1-t)y)  
\]

which proves convexity.

\end{proof}

\end{enumerate}

\end{exercise}

\bibliographystyle{abbrvnat}
\bibliography{mybib2fin}
\end{document}