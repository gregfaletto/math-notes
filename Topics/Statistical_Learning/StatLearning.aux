\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{v1_bertsekas2012dynamic}
\citation{v1_bertsekas2012dynamic}
\citation{v1_bertsekas2012dynamic}
\citation{v1_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v1_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{Mathieu2019}
\citation{Chi2020}
\citation{Chi2020}
\citation{Chi2020}
\citation{v1_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Statistical Learning}{7}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Segmented regression, local regression, splines}{7}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Local Regression}{7}{subsection.1.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Curse of Dimensionality (brief discussion)}{8}{subsection.1.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Dimension Reduction methods}{8}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}High dimensional correlations}{8}{subsection.1.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Principal components regression}{9}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Partial least squares}{9}{subsection.1.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Dimension reduction by random matrix}{10}{subsection.1.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Goodness of fit, residuals, residual diagnostics, leverage}{10}{section.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Residual diagnostics}{11}{subsection.1.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}DSO 607}{11}{section.1.4}}
\citation{Akaike1973}
\citation{Akaike1973}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Akaike Information Criterion (AIC)}{12}{subsection.1.4.1}}
\newlabel{linreg.aic.bic}{{1.4.1}{12}{Akaike Information Criterion (AIC)}{subsection.1.4.1}{}}
\citation{Schwarz1978}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Bayesian Information Criterion (BIC)}{13}{subsection.1.4.2}}
\newlabel{linreg.bic}{{1.4.2}{13}{Bayesian Information Criterion (BIC)}{subsection.1.4.2}{}}
\newlabel{linreg.aic.ftest.proof.c}{{1.1}{14}{Bayesian Information Criterion (BIC)}{equation.1.4.1}{}}
\newlabel{linreg.aic.ftest.proof.a}{{1.2}{15}{Bayesian Information Criterion (BIC)}{equation.1.4.2}{}}
\newlabel{linreg.aic.ftest.proof.b}{{1.3}{15}{Bayesian Information Criterion (BIC)}{equation.1.4.3}{}}
\newlabel{linreg.aic.ftest.proof.d}{{1.4}{16}{Bayesian Information Criterion (BIC)}{equation.1.4.4}{}}
\newlabel{linreg.aic.ftest.proof.e}{{1.5}{16}{Bayesian Information Criterion (BIC)}{equation.1.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Ridge Regression}{16}{section.1.5}}
\newlabel{math541a.hw7.3.eq1}{{1.6}{17}{Ridge Regression}{equation.1.5.6}{}}
\newlabel{linreg.dso.607.hw2}{{1.5.0.2}{17}{\textbf {DSO 607 Homework Problem}}{theorem.1.5.0.2}{}}
\newlabel{linreg.gsba.604.ridge.b}{{1.7}{18}{Ridge Regression}{equation.1.5.7}{}}
\newlabel{linreg.gsba.604.ridge.a}{{1.8}{18}{Ridge Regression}{equation.1.5.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Lasso}{19}{section.1.6}}
\citation{Antoniadis2001}
\newlabel{math541a.hw7.3.eq2}{{1.9}{20}{\textbf {Math 541A Homework Problem}}{equation.1.6.9}{}}
\citation{Donoho1994}
\newlabel{math541a.hw7.3a}{{1.10}{21}{Lasso}{equation.1.6.10}{}}
\newlabel{math541a.hw7.3b}{{1.11}{21}{Lasso}{equation.1.6.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Soft Thresholding}{21}{subsection.1.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Lasso theory}{22}{subsection.1.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Level sets of least squares loss function with feasible sets for (a) lasso and (b) ridge regression in the case of \(\beta \in \mathbb  {R}^2\).}}{23}{figure.1.1}}
\newlabel{linreg.lasso.level.set.figure}{{1.1}{23}{Level sets of least squares loss function with feasible sets for (a) lasso and (b) ridge regression in the case of \(\beta \in \mathbb {R}^2\)}{figure.1.1}{}}
\newlabel{linreg.prop.2018.screen.5}{{1.6.2.1}{23}{\textbf {2018 DSO Statistics Group In-Class Screening Exam, Question 5}}{theorem.1.6.2.1}{}}
\newlabel{2018.screen.5.a.objective}{{1.12}{23}{\textbf {2018 DSO Statistics Group In-Class Screening Exam, Question 5}}{equation.1.6.12}{}}
\newlabel{2018.screen.5.a.objective.alt}{{1.13}{24}{}{equation.1.6.13}{}}
\newlabel{2018.screen.5.a.objective.orig}{{1.14}{24}{}{equation.1.6.14}{}}
\newlabel{2018.screen.5.b.iv.relation}{{1.6.2.2}{24}{}{theorem.1.6.2.2}{}}
\citation{Osborne2000}
\newlabel{linreg.2018.screen.5.b.iv.prop}{{1.6.2.3}{25}{}{theorem.1.6.2.3}{}}
\newlabel{2018.screen.5.a.a}{{1.15}{25}{Lasso theory}{equation.1.6.15}{}}
\newlabel{2018.screen.5.a.other.part.result}{{1.16}{25}{Lasso theory}{equation.1.6.16}{}}
\newlabel{2018.screen.5.a.dual}{{1.17}{26}{Lasso theory}{equation.1.6.17}{}}
\newlabel{2018.screen.5.a.dual.ans}{{1.18}{26}{Lasso theory}{equation.1.6.18}{}}
\citation{Tibshirani2013}
\newlabel{2018.screen.5.c.ii.result}{{1.19}{27}{Lasso theory}{equation.1.6.19}{}}
\citation{Breiman1995}
\newlabel{2018.screen.5.c.ii.result.rewritten}{{1.20}{28}{Lasso theory}{equation.1.6.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.3}Non-Negative Garotte}{28}{subsection.1.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.4}LARS---Preliminaries and Intuition}{29}{subsection.1.6.4}}
\newlabel{lars.prelims}{{1.6.4}{29}{LARS---Preliminaries and Intuition}{subsection.1.6.4}{}}
\newlabel{linreg.lasso.constrained}{{1.21}{29}{LARS---Preliminaries and Intuition}{equation.1.6.21}{}}
\newlabel{linreg.lasso.lagrangian}{{1.22}{29}{LARS---Preliminaries and Intuition}{equation.1.6.22}{}}
\newlabel{linreg.lasso.lagrangian.gradient}{{1.23}{29}{LARS---Preliminaries and Intuition}{equation.1.6.23}{}}
\newlabel{linreg.lasso.soln.cond}{{1.24}{30}{LARS---Preliminaries and Intuition}{equation.1.6.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.5}LARS}{30}{subsection.1.6.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces  LARS figure in 2d case.}}{31}{figure.1.2}}
\newlabel{fig:lars_2d}{{1.2}{31}{LARS figure in 2d case}{figure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces  LARS figure in 3d case.}}{31}{figure.1.3}}
\newlabel{fig:lars_3d}{{1.3}{31}{LARS figure in 3d case}{figure.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Loss Functions}{32}{section.1.7}}
\newlabel{linreg.sec.loss.fxs}{{1.7}{32}{Loss Functions}{section.1.7}{}}
\newlabel{exercise6.5}{{1.7.0.1}{32}{\textbf {Loss: quadratic}}{theorem.1.7.0.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}Feature Selection properties}{34}{subsection.1.7.1}}
\newlabel{linreg.lasso.first.cond}{{1.25}{34}{Feature Selection properties}{equation.1.7.25}{}}
\newlabel{linreg.lasso.bound.cond}{{1.26}{35}{Feature Selection properties}{equation.1.7.26}{}}
\newlabel{linreg.lasso.first.cond.solved}{{1.27}{35}{Feature Selection properties}{equation.1.7.27}{}}
\newlabel{linreg.lasso.bound.cond.cont}{{1.28}{35}{Feature Selection properties}{equation.1.7.28}{}}
\newlabel{linreg.lasso.bound.cond.cont.more}{{1.29}{36}{Feature Selection properties}{equation.1.7.29}{}}
\newlabel{linreg.lasso.irrep}{{1.30}{36}{Feature Selection properties}{equation.1.7.30}{}}
\citation{James2009}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Dantzig Selector}{37}{section.1.8}}
\newlabel{linreg.dantzig.lp}{{1.31}{37}{Dantzig Selector}{equation.1.8.31}{}}
\newlabel{linreg.lasso.dantzig.thm1}{{1.8.0.1}{37}{}{theorem.1.8.0.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Coordinate Descent}{38}{section.1.9}}
\@writefile{toc}{\contentsline {section}{\numberline {1.10}Total Variational Distance}{38}{section.1.10}}
\@writefile{toc}{\contentsline {section}{\numberline {1.11}Non-parametric regression}{38}{section.1.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.11.1}Generalized additive models}{38}{subsection.1.11.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.12}Mixture regression}{39}{section.1.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Mixture model example.}}{39}{figure.1.4}}
\newlabel{linreg.fig.mix.model.ex}{{1.4}{39}{Mixture model example}{figure.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.13}Missing observations}{40}{section.1.13}}
\@writefile{toc}{\contentsline {section}{\numberline {1.14}Generalized linear models}{40}{section.1.14}}
\newlabel{linreg.glm}{{1.14}{40}{Generalized linear models}{section.1.14}{}}
\newlabel{linreg.eqn.likelihood}{{1.32}{41}{Generalized linear models}{equation.1.14.32}{}}
\newlabel{linreg.mean.param}{{1.33}{41}{Generalized linear models}{equation.1.14.33}{}}
\newlabel{linreg.fish.inf}{{1.34}{42}{Generalized linear models}{equation.1.14.34}{}}
\newlabel{linreg.fish.inf.mean.unb}{{1.35}{42}{Generalized linear models}{equation.1.14.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.14.1}Regression models}{43}{subsection.1.14.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.14.2}Applications---Categorical Data}{43}{subsection.1.14.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.14.3}Applications---Continuous Data}{44}{subsection.1.14.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.15}Mixed Effects Models}{44}{section.1.15}}
\@writefile{toc}{\contentsline {section}{\numberline {1.16}Miscellaneous Topics}{44}{section.1.16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.16.1}Multinomial Response}{44}{subsection.1.16.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.16.2}Zero-inflated response}{45}{subsection.1.16.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.16.3}Overdispersion}{45}{subsection.1.16.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.17}Generalized linear mixed models}{46}{section.1.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.17.1}Longitudinal data analysis and Generalized Estimating Equations}{47}{subsection.1.17.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.18}Causal Inference}{47}{section.1.18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.18.1}Factorial Design (see R lab 7)}{47}{subsection.1.18.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.19}Math 547}{48}{section.1.19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.19.1}Perceptron Algorithm}{48}{subsection.1.19.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.19.2}Mercer's Theorem}{48}{subsection.1.19.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.20}Norms}{49}{section.1.20}}
\newlabel{statlearning.def.nuc.norm}{{1.20.1}{49}{\textbf {Nuclear norm}}{definition.1.20.1}{}}
\newlabel{statlearning.def.spec.norm}{{1.20.2}{49}{\textbf {Spectral norm} or \textbf {operator norm}}{definition.1.20.2}{}}
\newlabel{statlearning.ex.nuc.norm.op.norm.dual}{{1}{49}{}{exercise.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.21}Collaborative Filtering and Trace Regression (Math 541B)}{50}{section.1.21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.21.1}Trace Regression}{50}{subsection.1.21.1}}
\citation{v1_bertsekas2012dynamic}
\@writefile{toc}{\contentsline {section}{\numberline {1.22}Dynamic Programming}{55}{section.1.22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.22.1}Introduction to Dynamic Programming and Principle of Optimality (Sections 1.1 - 1.3 of \citep  {v1_bertsekas2012dynamic} )}{55}{subsection.1.22.1}}
\citation{v1_bertsekas2012dynamic}
\newlabel{stoch.dp.inv.cont.ex}{{1.22.1}{56}{\textbf {Inventory control (Example 1.1.1 in \citep {v1_bertsekas2012dynamic} , p.5)l}}{example.1.22.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Depiction of cost structure for Example \ref  {stoch.dp.inv.cont.ex}. Note that this function \(x \DOTSB \mapstochar \rightarrow \mathcal  {G}(x)\) is convex in \(x\); this is relevant in the proof of Theorem \ref  {stoch.dp.order.up.to.opt}.}}{57}{figure.1.5}}
\newlabel{stoch_dp_ic_fig}{{1.5}{57}{Depiction of cost structure for Example \ref {stoch.dp.inv.cont.ex}. Note that this function \(x \mapsto \mathcal {G}(x)\) is convex in \(x\); this is relevant in the proof of Theorem \ref {stoch.dp.order.up.to.opt}}{figure.1.5}{}}
\citation{v1_bertsekas2012dynamic}
\citation{v1_bertsekas2012dynamic}
\newlabel{stoch.dp.1}{{1.36}{59}{Introduction to Dynamic Programming and Principle of Optimality (Sections 1.1 - 1.3 of \citep {v1_bertsekas2012dynamic} )}{equation.1.22.36}{}}
\citation{v1_bertsekas2012dynamic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.22.2}State Augmentation and Other Reformulations (Section 1.4 of \citep  {v1_bertsekas2012dynamic})}{60}{subsection.1.22.2}}
\citation{v1_bertsekas2012dynamic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.22.3}Inventory Control (Section 3.2 of \citet  {v1_bertsekas2012dynamic} )}{61}{subsection.1.22.3}}
\newlabel{stoch.dp.order.up.to.opt}{{1.22.3.1}{61}{\textbf {Order-up-to Polices Are Optimal}}{theorem.1.22.3.1}{}}
\citation{v1_bertsekas2012dynamic}
\citation{v1_bertsekas2012dynamic}
\newlabel{inv.cont.ctg}{{1.37}{62}{Inventory Control (Section 3.2 of \citet {v1_bertsekas2012dynamic} )}{equation.1.22.37}{}}
\newlabel{dp.inventory.control.cvx.fcn}{{1.38}{62}{Inventory Control (Section 3.2 of \citet {v1_bertsekas2012dynamic} )}{equation.1.22.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Cost-to-go function for inventory control problem. (The notation from \citet  {v1_bertsekas2012dynamic} differs slightly from ours, but the lower figure shows that the cost-to-go is linear to the left of the minimizer and convex and nondecreasing to the right of the minimizer, as shown in Equation (\ref  {dp.inventory.control.cvx.fcn})).}}{63}{figure.1.6}}
\newlabel{dp_inventory_control_fig}{{1.6}{63}{Cost-to-go function for inventory control problem. (The notation from \citet {v1_bertsekas2012dynamic} differs slightly from ours, but the lower figure shows that the cost-to-go is linear to the left of the minimizer and convex and nondecreasing to the right of the minimizer, as shown in Equation (\ref {dp.inventory.control.cvx.fcn}))}{figure.1.6}{}}
\citation{v1_bertsekas2012dynamic}
\citation{v1_bertsekas2012dynamic}
\newlabel{stoch.dp.k.cvx}{{1.22.4}{64}{\textbf {\(K\)-convexity; Definition 3.2.1 in Section 3.2 of \citet {v1_bertsekas2012dynamic}, p. 130}}{definition.1.22.4}{}}
\newlabel{dp.k.cvx.defn}{{1.39}{64}{\textbf {\(K\)-convexity; Definition 3.2.1 in Section 3.2 of \citet {v1_bertsekas2012dynamic}, p. 130}}{equation.1.22.39}{}}
\newlabel{stoch.dp.k.cvx.2}{{1.40}{64}{\textbf {\(K\)-convexity; Definition 3.2.1 in Section 3.2 of \citet {v1_bertsekas2012dynamic}, p. 130}}{equation.1.22.40}{}}
\newlabel{stoch.dp.prop.k.cvx}{{1.22.3.3}{64}{\textbf {Lemma 3.2.1 in \citet {v1_bertsekas2012dynamic}}}{theorem.1.22.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Function is \(K\)-convex (see Definition \ref  {stoch.dp.k.cvx}), but not convex.}}{65}{figure.1.7}}
\newlabel{stoch_dp_k_cvx_fig}{{1.7}{65}{Function is \(K\)-convex (see Definition \ref {stoch.dp.k.cvx}), but not convex}{figure.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces default}}{66}{figure.1.8}}
\newlabel{stoch_dp_k_cvx_p3_fig}{{1.8}{66}{default}{figure.1.8}{}}
\newlabel{stoch.dp.them.opt.fix.cost}{{1.22.3.4}{67}{}{theorem.1.22.3.4}{}}
\newlabel{dp.fixed.costs.inv.opt.policy}{{1.41}{67}{}{equation.1.22.41}{}}
\citation{v1_bertsekas2012dynamic}
\newlabel{dp.def.g_k.fixed.costs}{{1.42}{68}{Inventory Control (Section 3.2 of \citet {v1_bertsekas2012dynamic} )}{equation.1.22.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces If \(G_k(\cdot )\) is convex, the policy (\ref  {dp.fixed.costs.inv.opt.policy}) is optimal, where \(S_k\) is a minimizer of \(G_k(\cdot )\) and \(s_k\) is the smallest value of \(y\) for which \(G_k(y) = K + G_k(S_k)\). However, \(G_k\) is not necessarily convex.}}{68}{figure.1.9}}
\newlabel{dp_k_convexity_fig}{{1.9}{68}{If \(G_k(\cdot )\) is convex, the policy (\ref {dp.fixed.costs.inv.opt.policy}) is optimal, where \(S_k\) is a minimizer of \(G_k(\cdot )\) and \(s_k\) is the smallest value of \(y\) for which \(G_k(y) = K + G_k(S_k)\). However, \(G_k\) is not necessarily convex}{figure.1.9}{}}
\newlabel{stoch_dp_fixed_fig}{{1.9}{68}{If \(G_k(\cdot )\) is convex, the policy (\ref {dp.fixed.costs.inv.opt.policy}) is optimal, where \(S_k\) is a minimizer of \(G_k(\cdot )\) and \(s_k\) is the smallest value of \(y\) for which \(G_k(y) = K + G_k(S_k)\). However, \(G_k\) is not necessarily convex}{figure.1.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Figure 3.2.2 in \citet  {v1_bertsekas2012dynamic}.}}{69}{figure.1.10}}
\newlabel{dp_k_convexity_fig}{{1.10}{69}{Figure 3.2.2 in \citet {v1_bertsekas2012dynamic}}{figure.1.10}{}}
\newlabel{dp.lemma.k.cvx.inventory}{{1.22.3.5}{69}{}{theorem.1.22.3.5}{}}
\newlabel{stoch.dp.fixed.thm.proof.1}{{1.43}{70}{Inventory Control (Section 3.2 of \citet {v1_bertsekas2012dynamic} )}{equation.1.22.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces The red line is the right part of (\ref  {stoch.dp.fixed.thm.proof.1}) (\(J_\ell (x_\ell ) + c x_\ell \)).}}{70}{figure.1.11}}
\newlabel{stoch_dp_fixed_thm_proof_1}{{1.11}{70}{The red line is the right part of (\ref {stoch.dp.fixed.thm.proof.1}) (\(J_\ell (x_\ell ) + c x_\ell \))}{figure.1.11}{}}
\newlabel{stoch.dp.fixed.thm.proof.2}{{1.44}{70}{Inventory Control (Section 3.2 of \citet {v1_bertsekas2012dynamic} )}{equation.1.22.44}{}}
\newlabel{stoch.dp.fixed.thm.proof.3}{{1.45}{71}{Inventory Control (Section 3.2 of \citet {v1_bertsekas2012dynamic} )}{equation.1.22.45}{}}
\newlabel{stoch.dp.fixed.thm.proof.4}{{1.46}{72}{Inventory Control (Section 3.2 of \citet {v1_bertsekas2012dynamic} )}{equation.1.22.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.22.4}Capacity Allocation and Revenue Management}{72}{subsection.1.22.4}}
\newlabel{stoch.dp.two.class.littewood}{{1.22.3}{73}{\textbf {Two-class capacity allocation (Littlewood's formula)}}{example.1.22.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Figure for example \ref  {stoch.dp.two.class.littewood}.}}{74}{figure.1.12}}
\newlabel{stoch_littlewood_fig}{{1.12}{74}{Figure for example \ref {stoch.dp.two.class.littewood}}{figure.1.12}{}}
\newlabel{stoch.dp.n.class.capacity}{{1.22.4}{74}{\textbf {\(n\)-class capacity allocation}}{example.1.22.4}{}}
\newlabel{stoch.dp.n.class.capacity.thm}{{1.22.4.1}{75}{}{theorem.1.22.4.1}{}}
\newlabel{stoch.dp.n.class.capacity.thm.lemma}{{1.22.4.2}{75}{}{theorem.1.22.4.2}{}}
\citation{v1_bertsekas2012dynamic}
\newlabel{stoch.dp.vjprime}{{1.50}{76}{Capacity Allocation and Revenue Management}{equation.1.22.50}{}}
\newlabel{stoch.dp.vjprime.exp}{{1.51}{76}{Capacity Allocation and Revenue Management}{equation.1.22.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces The argument of the expectation operator in (\ref  {stoch.dp.vjprime.exp}) is nonincreasing in \(x\) regardless of the value of the random variable \(D_j\), so the expectation itself is nonincreasing in \(x\).}}{77}{figure.1.13}}
\newlabel{stoch_dp_n_class_fig}{{1.13}{77}{The argument of the expectation operator in (\ref {stoch.dp.vjprime.exp}) is nonincreasing in \(x\) regardless of the value of the random variable \(D_j\), so the expectation itself is nonincreasing in \(x\)}{figure.1.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces Illustration of nested protection levels from Theorem \ref  {stoch.dp.n.class.capacity.thm}.}}{78}{figure.1.14}}
\newlabel{stoch_dp_n_class_pf_fig}{{1.14}{78}{Illustration of nested protection levels from Theorem \ref {stoch.dp.n.class.capacity.thm}}{figure.1.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.22.5}Optimal Stopping (Section 3.4 of \citet  {v1_bertsekas2012dynamic})}{78}{subsection.1.22.5}}
\citation{v2_bertsekas2012dynamic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.22.6}Infinite Horizon (Sections 1.2, 1.5 and 2.1 of \citet  {v2_bertsekas2012dynamic}; starts on p. 210 of pdf for Volume 3)}{79}{subsection.1.22.6}}
\newlabel{statlearning.inf.horizon}{{1.22.6}{79}{Infinite Horizon (Sections 1.2, 1.5 and 2.1 of \citet {v2_bertsekas2012dynamic}; starts on p. 210 of pdf for Volume 3)}{subsection.1.22.6}{}}
\citation{v2_bertsekas2012dynamic}
\newlabel{stoch.dp.operator.ineq}{{1.52}{82}{Infinite Horizon (Sections 1.2, 1.5 and 2.1 of \citet {v2_bertsekas2012dynamic}; starts on p. 210 of pdf for Volume 3)}{equation.1.22.52}{}}
\newlabel{statlearning.def.contraction}{{1.22.7}{82}{\textbf {Contraction Mapping (Section 1.5 of \citet {v2_bertsekas2012dynamic})}}{definition.1.22.7}{}}
\newlabel{dp.blackwell.sc}{{1.22.6.1}{82}{\textbf {Blackwell's Sufficient Conditions}}{theorem.1.22.6.1}{}}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\newlabel{dp.thm.dp.op.mono.cs}{{1.22.6.2}{83}{\textbf {Properties of \(T\) and \(T_\mu \)}}{theorem.1.22.6.2}{}}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\newlabel{dp.op.c.mapping}{{1.22.6.3}{84}{\textbf {Proposition 1.2.6 in \citet {v2_bertsekas2012dynamic}}}{theorem.1.22.6.3}{}}
\newlabel{stoch.dp.bellman.eqn.infinite.horizon}{{1.53}{84}{Infinite Horizon (Sections 1.2, 1.5 and 2.1 of \citet {v2_bertsekas2012dynamic}; starts on p. 210 of pdf for Volume 3)}{equation.1.22.53}{}}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.22.7}Value Iterations and Policy Iterations (Sections 2.2 and 2.3 of \citet  {v2_bertsekas2012dynamic}; starts on p. 210 of pdf for Volume 3)}{87}{subsection.1.22.7}}
\newlabel{stoch.dp.2.2.1.ineq}{{1.54}{88}{Value Iterations and Policy Iterations (Sections 2.2 and 2.3 of \citet {v2_bertsekas2012dynamic}; starts on p. 210 of pdf for Volume 3)}{equation.1.22.54}{}}
\newlabel{stoch.dp.2.2.1.ineq2}{{1.55}{89}{Value Iterations and Policy Iterations (Sections 2.2 and 2.3 of \citet {v2_bertsekas2012dynamic}; starts on p. 210 of pdf for Volume 3)}{equation.1.22.55}{}}
\citation{v2_bertsekas2012dynamic}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces Iteration of VI method. Start with \(J_0\), get \(T J_0\), plug in to \(T\), etc. Note that the optimal solution is the intersection of the \(TJ\) function and the 45 degree line, since the optimal policy satisfies \(TJ^* = J^*\).}}{92}{figure.1.15}}
\newlabel{stoch_dp_vi_vis_fig}{{1.15}{92}{Iteration of VI method. Start with \(J_0\), get \(T J_0\), plug in to \(T\), etc. Note that the optimal solution is the intersection of the \(TJ\) function and the 45 degree line, since the optimal policy satisfies \(TJ^* = J^*\)}{figure.1.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces Iteration of PI method.}}{93}{figure.1.16}}
\newlabel{stoch_dp_pi_vis_fig}{{1.16}{93}{Iteration of PI method}{figure.1.16}{}}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.22.8}Scheduling and Multiarmed Bandit Problems (Section 1.3 of \citet  {v2_bertsekas2012dynamic})}{96}{subsection.1.22.8}}
\newlabel{statlearning.dp.prop.1.3.4.v2}{{1.22.8.1}{96}{\textbf {Proposition 1.3.4 in \citet {v2_bertsekas2012dynamic}, p. 29}}{theorem.1.22.8.1}{}}
\newlabel{statlearning.dp.prop.1.3.1.v2}{{1.22.8.2}{96}{\textbf {Proposition 1.3.1 in \citet {v2_bertsekas2012dynamic}, p. 29}}{theorem.1.22.8.2}{}}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces Figure 1.3.1 in \citet  {v2_bertsekas2012dynamic}, p. 25. Form of the \(\ell \)th project reward function \(J^\ell (x^\ell , M)\) for fixed \(x^\ell \) and definition of the index \(m^\ell (x^\ell )\).}}{97}{figure.1.17}}
\newlabel{dp_fig_1_3_1_fig}{{1.17}{97}{Figure 1.3.1 in \citet {v2_bertsekas2012dynamic}, p. 25. Form of the \(\ell \)th project reward function \(J^\ell (x^\ell , M)\) for fixed \(x^\ell \) and definition of the index \(m^\ell (x^\ell )\)}{figure.1.17}{}}
\newlabel{statlearning.dp.def.index.function}{{1.56}{97}{Scheduling and Multiarmed Bandit Problems (Section 1.3 of \citet {v2_bertsekas2012dynamic})}{equation.1.22.56}{}}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\newlabel{statlearning.dp.retirement.set.def}{{1.57}{98}{\textbf {Project-by-Project Retirement Policy (PPR; pp. 25-26 of \citet {v2_bertsekas2012dynamic})}}{equation.1.22.57}{}}
\newlabel{statlearning.dp.prop.1.3.2.v2}{{1.22.8.3}{98}{\textbf {Proposition 1.3.2 in \citet {v2_bertsekas2012dynamic}}}{theorem.1.22.8.3}{}}
\newlabel{statlearning.dp.prop.1.3.3.v2}{{1.22.8.4}{98}{\textbf {Proposition 1.3.3 in \citet {v2_bertsekas2012dynamic}}}{theorem.1.22.8.4}{}}
\citation{v1_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.22.9}Approximate DP: Q-Learning (Section 6.3.3 of \citet  {v1_bertsekas2012dynamic}, Sections 2.2.3, 2.5.3, and 6.1 - 6.6.1 of \citep  {v2_bertsekas2012dynamic})}{99}{subsection.1.22.9}}
\citation{v1_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\newlabel{statlearning.qlearning.opt}{{1.59}{101}{Approximate DP: Q-Learning (Section 6.3.3 of \citet {v1_bertsekas2012dynamic}, Sections 2.2.3, 2.5.3, and 6.1 - 6.6.1 of \citep {v2_bertsekas2012dynamic})}{equation.1.22.59}{}}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.22.10}Optimal Stopping (Section 6.6.4 of \citet  {v2_bertsekas2012dynamic}, p. 504)}{104}{subsection.1.22.10}}
\citation{v2_bertsekas2012dynamic}
\citation{v2_bertsekas2012dynamic}
\citation{Mathieu2019}
\@writefile{toc}{\contentsline {section}{\numberline {1.23}Notes on \citet  {Mathieu2019}}{110}{section.1.23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.23.1}Notation}{110}{subsection.1.23.1}}
\newlabel{statlearning.emp.risk.min}{{1.60}{110}{\textbf {Empirical Risk Minimizer and Excess Risk}}{equation.1.23.60}{}}
\citation{Catoni2012}
\citation{Devroye2016}
\citation{MATHIEU2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.23.2}Section 1}{111}{subsection.1.23.2}}
\citation{MATHIEU2019}
\citation{Chi2020}
\@writefile{toc}{\contentsline {section}{\numberline {1.24}Random Forests and Notes on \citet  {Chi2020}}{114}{section.1.24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.24.1}Section 2 (Terminology and Review of Random Forest)}{114}{subsection.1.24.1}}
\citation{Chi2020}
\citation{CHI2020}
\newlabel{stat.learn.def.sample.tree.grow.rule}{{1.24.6}{115}{Sample tree growing rule; Definition 3 in \citet {Chi2020}}{definition.1.24.6}{}}
\newlabel{stat.learn.rf.full.inds.spec}{{1.61}{116}{}{equation.1.24.61}{}}
\newlabel{stat.learn.rf.pred.full}{{1.62}{116}{}{equation.1.24.62}{}}
\newlabel{stat.learn.rmk.rf.params}{{13}{116}{}{remark.13}{}}
\citation{Chi2020}
\citation{Chi2020}
\citation{CHI2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.24.2}Section 3: Approximation Accuracy}{117}{subsection.1.24.2}}
\newlabel{statlearn.pop.cart.split.crit.def}{{1.24.9}{117}{}{definition.1.24.9}{}}
\newlabel{statlearn.rf.imp.dec.def}{{1.63}{117}{}{equation.1.24.63}{}}
\newlabel{stat.learn.pop.tree.grow.rule.def}{{1.24.11}{117}{Population tree growing rule; Definition 4 in \citet {Chi2020}}{definition.1.24.11}{}}
\newlabel{stat.learn.pop.tree.grow.rule.eq}{{1.64}{117}{Population tree growing rule; Definition 4 in \citet {Chi2020}}{equation.1.24.64}{}}
\newlabel{lem.var.decomp.rf}{{1.65}{117}{Variance decomposition formula; Lemma 1 in \citet {Chi2020}}{equation.1.24.65}{}}
\@writefile{toc}{\contentsline {subsubsection}{Technical Conditions}{118}{section*.2}}
\citation{Chi2020}
\citation{Chi2020}
\citation{Chi2020}
\citation{CHI2020}
\@writefile{toc}{\contentsline {subsubsection}{Main Results}{119}{section*.3}}
\newlabel{stat.learn.thm.rf.approx.acc}{{1.24.2.2}{119}{Approximation accuracy; Theorem 1 in \citet {Chi2020}}{theorem.1.24.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.24.3}Consistency Rates (Section 4 of \citet  {Chi2020}])}{119}{subsection.1.24.3}}
\citation{Chi2020}
\citation{Chi2020}
\citation{Chi2020}
\citation{Chi2020}
\newlabel{stat.learn.lem.3.approx.bound.2}{{1.66}{120}{Approximation error; Lemma 3 in \citet {Chi2020}}{equation.1.24.66}{}}
\citation{Chi2020}
\citation{Chi2020}
\citation{Chi2020}
\citation{Chi2020}
\citation{CHI2020}
\newlabel{stat.learn.cons.rf.rates}{{1.24.3.3}{121}{Consistency rates; Theorem 2 in \citet {Chi2020}}{theorem.1.24.3.3}{}}
\citation{Chi2020}
\bibstyle{abbrvnat}
\bibdata{mybib2fin}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.24.4}A General Estimation Foundation (Section 5 of \citet  {Chi2020})}{122}{subsection.1.24.4}}
\bibcite{Akaike1973}{{1}{1973}{{Akaike}}{{}}}
\bibcite{Antoniadis2001}{{2}{2001}{{Antoniadis and Fan}}{{}}}
\bibcite{v1_bertsekas2012dynamic}{{3}{2012{a}}{{Bertsekas}}{{}}}
\bibcite{v2_bertsekas2012dynamic}{{4}{2012{b}}{{Bertsekas}}{{}}}
\bibcite{Breiman1995}{{5}{1995}{{Breiman}}{{}}}
\bibcite{Catoni2012}{{6}{2012}{{Catoni}}{{}}}
\bibcite{Chi2020}{{7}{2020}{{Chi et~al.}}{{Chi, Vossler, Fan, and Lv}}}
\bibcite{Devroye2016}{{8}{2016}{{Devroye et~al.}}{{Devroye, Lerasle, Lugosi, and Oliveira}}}
\bibcite{Donoho1994}{{9}{1994}{{Donoho and Johnstone}}{{}}}
\bibcite{James2009}{{10}{2009}{{James et~al.}}{{James, Radchenko, and Lv}}}
\bibcite{Mathieu2019}{{11}{2019}{{Mathieu and Minsker}}{{}}}
\bibcite{Osborne2000}{{12}{2000}{{Osborne et~al.}}{{Osborne, Presnell, and Turlach}}}
\bibcite{Schwarz1978}{{13}{1978}{{Schwarz}}{{}}}
\bibcite{Tibshirani2013}{{14}{2013}{{Tibshirani}}{{}}}
\bibcite{vershynin2018high}{{15}{2018}{{Vershynin}}{{}}}
