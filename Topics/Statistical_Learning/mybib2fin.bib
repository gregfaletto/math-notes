%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Gregory Faletto at 2022-05-28 21:25:42 -0700 


%% Saved with string encoding Unicode (UTF-8) 



@book{vershynin2018high,
	author = {Vershynin, Roman},
	date-added = {2022-05-28 21:25:41 -0700},
	date-modified = {2022-05-28 21:25:41 -0700},
	publisher = {Cambridge university press},
	title = {High-dimensional probability: An introduction with applications in data science},
	volume = {47},
	year = {2018}}

@techreport{Chi2020,
	abstract = {As a flexible nonparametric learning tool, random forest has been widely applied to various real applications with appealing empirical performance, even in the presence of high-dimensional feature space. Unveiling the underlying mechanisms has led to some important recent theoretical results on consistency under the classical setting of fixed dimensionality or for some modified version of the random forest algorithm. Yet the consistency rates of the original version of the random forest algorithm in a general high-dimensional nonparametric regression setting remain largely unexplored. In this paper, we fill such a gap and build a high-dimensional consistency theory for random forest. Our new theoretical results show that random forest can indeed adapt to high dimensions and also provide some insights into the role of sparsity from the perspective of feature relevance. Running title: HRF},
	archiveprefix = {arXiv},
	arxivid = {2004.13953v1},
	author = {Chi, Chien-Ming and Vossler, Patrick and Fan, Yingying and Lv, Jinchi},
	date-added = {2020-07-21 18:21:03 -0700},
	date-modified = {2020-07-21 18:21:03 -0700},
	eprint = {2004.13953v1},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Chi et al. - 2020 - Asymptotic Properties of High-Dimensional Random Forests.pdf:pdf},
	keywords = {Consistency,High dimensionality,Nonparametric learning,Random forest,Rate of convergence,Sparsity},
	title = {{Asymptotic Properties of High-Dimensional Random Forests *}},
	year = {2020}}

@book{shao2012jackknife,
	author = {Shao, J. and Tu, D.},
	date-added = {2020-07-12 11:35:50 -0700},
	date-modified = {2020-07-12 11:35:50 -0700},
	isbn = {9781461207955},
	lccn = {95015074},
	publisher = {Springer New York},
	series = {Springer Series in Statistics},
	title = {The Jackknife and Bootstrap},
	url = {https://books.google.com/books?id=VO3SBwAAQBAJ},
	year = {2012},
	bdsk-url-1 = {https://books.google.com/books?id=VO3SBwAAQBAJ}}

@article{Hampel1974,
	abstract = {This paper treats essentially the first derivative of an estimator viewed as functional and the ways in which it can be used to study local robustness properties. A theory of robust estimation ``near'' strict parametric models is briefly sketched and applied to some classical situations. Relations between von Mises functionals, the jackknife and U-statistics are indicated. A number of classical and new estimators are discussed, including trimmed and Winsorized means, Huber-estimators, and more generally maximum likelihood and M-estimators. Finally, a table with some numerical robustness properties is given. {\textcopyright} 1974, Taylor {\&} Francis Group, LLC.},
	author = {Hampel, Frank R},
	date-added = {2020-07-10 16:37:43 -0700},
	date-modified = {2020-07-10 16:37:43 -0700},
	doi = {10.1080/01621459.1974.10482962},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Hampel - 1974 - The Influence Curve and Its Role in Robust Estimation.pdf:pdf},
	issn = {1537274X},
	journal = {Journal of the American Statistical Association},
	number = {346},
	pages = {383--393},
	title = {{The influence curve and its role in robust estimation}},
	volume = {69},
	year = {1974},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.1974.10482962}}

@book{Efron2016,
	abstract = {The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. 'Big data', 'data science', and 'machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.},
	author = {Efron, Bradley and Hastie, Trevor},
	booktitle = {Computer Age Statistical Inference: Algorithms, Evidence, and Data Science},
	date-added = {2020-07-10 16:26:21 -0700},
	date-modified = {2020-07-10 16:26:21 -0700},
	doi = {10.1017/CBO9781316576533},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Efron, Hastie - 2016 - Computer age statistical inference Algorithms, evidence, and data science.pdf:pdf},
	isbn = {9781316576533},
	pages = {1--475},
	title = {{Computer age statistical inference: Algorithms, evidence, and data science}},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1017/CBO9781316576533}}

@book{koroljuk1994theory,
	author = {Koroljuk, V.S. and Korolyuk, V.S. and Borovskikh, I.V. and Borovskich, Y.V. and Borovskikh, I.V. and Borovskikh, I.U.V. and Borovskich, J.V.},
	date-added = {2020-07-10 13:17:55 -0700},
	date-modified = {2020-07-10 13:17:55 -0700},
	isbn = {9780792326083},
	lccn = {94119398},
	publisher = {Springer Netherlands},
	series = {Mathematics and Its Applications},
	title = {Theory of U-Statistics},
	url = {https://books.google.com/books?id=bp2DgkDxMdMC},
	year = {1994},
	bdsk-url-1 = {https://books.google.com/books?id=bp2DgkDxMdMC}}

@book{demidenko2013mixed,
	author = {Demidenko, E.},
	date-added = {2020-07-10 13:10:19 -0700},
	date-modified = {2020-07-10 13:10:19 -0700},
	isbn = {9781118592991},
	lccn = {2013013415},
	publisher = {Wiley},
	series = {Wiley Series in Probability and Statistics},
	title = {Mixed Models: Theory and Applications with R},
	url = {https://books.google.com/books?id=uSmRAAAAQBAJ},
	year = {2013},
	bdsk-url-1 = {https://books.google.com/books?id=uSmRAAAAQBAJ}}

@article{Li2012,
	abstract = {This article is concerned with screening features in ultrahigh-dimensional data analysis, which has become increasingly important in diverse scientific fields. We develop a sure independence screening procedure based on the distance correlation (DC-SIS). The DC-SIS can be implemented as easily as the sure independence screening (SIS) procedure based on the Pearson correlation proposed by Fan and Lv. However, the DC-SIS can significantly improve the SIS. Fan and Lv established the sure screening property for the SIS based on linear models, but the sure screening property is valid for the DC-SIS under more general settings, including linear models. Furthermore, the implementation of the DC-SIS does not require model specification (e.g., linear model or generalized linear model) for responses or predictors. This is a very appealing property in ultrahigh-dimensional data analysis. Moreover, the DC-SIS can be used directly to screen grouped predictor variables and multivariate response variables. We establish the sure screening property for the DC-SIS, and conduct simulations to examine its finite sample performance. A numerical comparison indicates that the DC-SIS performs much better than the SIS in various models. We also illustrate the DC-SIS through a real-data example. {\textcopyright} 2012 American Statistical Association.},
	archiveprefix = {arXiv},
	arxivid = {1205.4701},
	author = {Li, Runze and Zhong, Wei and Zhu, Liping},
	date-added = {2020-07-10 13:04:25 -0700},
	date-modified = {2020-07-10 13:04:25 -0700},
	doi = {10.1080/01621459.2012.695654},
	eprint = {1205.4701},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Li, Zhong, Zhu - 2012 - Feature Screening via Distance Correlation Learning.pdf:pdf},
	issn = {01621459},
	journal = {Journal of the American Statistical Association},
	keywords = {DSO 607,DSO 607 Week 6,Sure independence screening,Sure screening property,Ultrahigh dimensionality,Variable selection,feature selection,prediction competitions},
	mendeley-tags = {DSO 607,DSO 607 Week 6,feature selection,prediction competitions},
	number = {499},
	pages = {1129--1139},
	title = {{Feature screening via distance correlation learning}},
	url = {https://www.tandfonline.com/action/journalInformation?journalCode=uasa20},
	volume = {107},
	year = {2012},
	bdsk-url-1 = {https://www.tandfonline.com/action/journalInformation?journalCode=uasa20},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.2012.695654}}

@article{Szekely2007,
	abstract = {Distance correlation is a new measure of dependence between random vectors. Distance covariance and distance correlation are analogous to product-moment covariance and correlation, but unlike the classical definition of correlation, distance correlation is zero only if the random vectors are independent. The empirical distance dependence measures are based on certain Euclidean distances between sample elements rather than sample moments, yet have a compact representation analogous to the classical covariance and correlation. Asymptotic properties and applications in testing independence are discussed. Implementation of the test and Monte Carlo results are also presented. {\textcopyright} Institute of Mathematical Statistics, 2007.},
	author = {Sz{\'{e}}kely, G{\'{a}}bor J. and Rizzo, Maria L. and Bakirov, Nail K.},
	date-added = {2020-07-10 11:48:24 -0700},
	date-modified = {2020-07-10 11:48:24 -0700},
	doi = {10.1214/009053607000000505},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Sz{\'{e}}kely, Rizzo, Bakirov - 2007 - Measuring and testing dependence by correlation of distances.pdf:pdf},
	issn = {00905364},
	journal = {Annals of Statistics},
	keywords = {Distance correlation,Distance covariance,Multivariate independence},
	month = {dec},
	number = {6},
	pages = {2769--2794},
	title = {{Measuring and testing dependence by correlation of distances}},
	volume = {35},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1214/009053607000000505}}

@book{Durrett2019,
	address = {USA},
	author = {Durrett, Rick},
	date-added = {2020-07-10 08:22:41 -0700},
	date-modified = {2020-07-10 08:24:46 -0700},
	edition = {5th},
	publisher = {Cambridge University Press},
	title = {Probability: Theory and Examples},
	url = {https://services.math.duke.edu/~rtd/PTE/pte.html},
	year = {2019},
	bdsk-url-1 = {https://services.math.duke.edu/~rtd/PTE/pte.html}}

@article{Devroye2016,
	abstract = {We discuss the possibilities and limitations of estimating the mean of a real-valued random variable from independent and identically distributed observations from a nonasymptotic point of view. In particular, we define estimators with a sub-Gaussian behavior even for certain heavy-tailed distributions. We also prove various impossibility results for mean estimators.},
	archiveprefix = {arXiv},
	arxivid = {1509.05845},
	author = {Devroye, Luc and Lerasle, Matthieu and Lugosi, Gabor and Oliveira, Roberto I},
	date-added = {2020-07-01 15:55:20 -0700},
	date-modified = {2020-07-01 15:55:20 -0700},
	doi = {10.1214/16-AOS1440},
	eprint = {1509.05845},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Devroye et al. - 2016 - SUB-GAUSSIAN MEAN ESTIMATORS.pdf:pdf},
	issn = {00905364},
	journal = {Annals of Statistics},
	keywords = {Minimax bounds,Sub-Gaussian estimators},
	number = {6},
	pages = {2695--2725},
	title = {{Sub-Gaussian Mean Estimators}},
	volume = {44},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1214/16-AOS1440}}

@article{Catoni2012,
	abstract = {We present new M-estimators of the mean and variance of real valued random variables, based on PAC-Bayes bounds. We analyze the non-asymptotic minimax properties of the deviations of those estimators for sample distributions having either a bounded variance or a bounded variance and a bounded kurtosis. Under those weak hypotheses, allowing for heavy-tailed distributions, we show that the worst case deviations of the empirical mean are suboptimal. We prove indeed that for any confidence level, there is some M-estimator whose deviations are of the same order as the deviations of the empirical mean of a Gaussian statistical sample, even when the statistical sample is instead heavy-tailed. Experiments reveal that these new estimators perform even better than predicted by our bounds, showing deviation quantile functions uniformly lower at all probability levels than the empirical mean for non-Gaussian sample distributions as simple as the mixture of two Gaussian measures. {\textcopyright} 2012 Association des Publications de l'Institut Henri Poincar{\'{e}}.},
	archiveprefix = {arXiv},
	arxivid = {1009.2048},
	author = {Catoni, Olivier},
	date-added = {2020-06-25 14:40:11 -0700},
	date-modified = {2020-06-25 14:40:11 -0700},
	doi = {10.1214/11-AIHP454},
	eprint = {1009.2048},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Catoni - 2012 - Challenging the empirical mean and empirical variance A deviation study.pdf:pdf},
	issn = {02460203},
	journal = {Annales de l'institut Henri Poincare (B) Probability and Statistics},
	keywords = {M-estimators,Non-parametric estimation,PAC-Bayes bounds},
	number = {4},
	pages = {1148--1185},
	title = {{Challenging the empirical mean and empirical variance: A deviation study}},
	url = {www.imstat.org/aihpAnnalesdel'},
	volume = {48},
	year = {2012},
	bdsk-url-1 = {www.imstat.org/aihpAnnalesdel'},
	bdsk-url-2 = {https://doi.org/10.1214/11-AIHP454}}

@unpublished{Mathieu2019,
	abstract = {This paper investigates robust versions of the general empirical risk minimization algorithm , one of the core techniques underlying modern statistical methods. Success of the empirical risk minimization is based on the fact that for a "well-behaved" stochastic process tf pXq, f P F u indexed by a class of functions f P F , averages 1 N {\v r} N j"1 f pX j q evaluated over a sample X 1 ,. .. , X N of i.i.d. copies of X provide good approximation to the expectations Ef pXq uniformly over large classes f P F. However, this might no longer be true if the marginal distributions of the process are heavy-tailed or if the sample contains outliers. We propose a version of empirical risk minimization based on the idea of replacing sample averages by robust proxies of the expectation, and obtain high-confidence bounds for the excess risk of resulting estimators. In particular, we show that the excess risk of robust estimators can converge to 0 at fast rates with respect to the sample size. We discuss implications of the main results to the linear and logistic regression problems, and evaluate the numerical performance of proposed methods on simulated and real data.},
	archiveprefix = {arXiv},
	arxivid = {1910.07485v1},
	author = {Mathieu, Timoth{\'{e}}e and Minsker, Stanislav},
	date-added = {2020-06-25 14:03:45 -0700},
	date-modified = {2020-06-25 14:03:45 -0700},
	eprint = {1910.07485v1},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Mathieu, Minsker - Unknown - Excess risk bounds in robust empirical risk minimization.pdf:pdf},
	keywords = {and phrases: robust estimation,classifica-tion,excess risk,median-of-means,regression},
	title = {{Excess risk bounds in robust empirical risk minimization}},
	url = {https://arxiv.org/abs/1910.07485},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1910.07485}}

@article{Wager2018,
	abstract = {Many scientific and engineering challenges---ranging from personalized medicine to customized marketing recommendations---require an understanding of treatment effect heterogeneity. In this paper, we develop a non-parametric causal forest for estimat- ing heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect, and have an asymptotically Gaussian and centered sampling distribution. We also discuss a prac- tical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowl- edge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.},
	annote = {From presentation on 04/15 in DSO 607},
	archiveprefix = {arXiv},
	arxivid = {arXiv:1510.04342v2},
	author = {Wager, Stefan and Athey, Susan},
	date-added = {2020-06-22 13:14:07 -0700},
	date-modified = {2020-06-22 13:14:07 -0700},
	doi = {10.1080/01621459.2017.1319839},
	eprint = {arXiv:1510.04342v2},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Wager, Athey - 2015 - Estimation and Inference of Heterogeneous Treatment Effects using Random Forests ∗ arXiv 1510 . 04342v1 stat ..pdf:pdf},
	issn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	keywords = {DSO 607,DSO 607 Week 12,adaptive nearest neighbors matching,asymptotic normality,business applications,causal inference,outcomes,potential,unconfoundedness},
	mendeley-tags = {DSO 607,DSO 607 Week 12,business applications,causal inference},
	month = {jul},
	number = {523},
	pages = {1228--1242},
	title = {{Estimation and Inference of Heterogeneous Treatment Effects using Random Forests}},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839},
	volume = {113},
	year = {2018},
	bdsk-url-1 = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.2017.1319839}}

@book{dasgupta2008asymptotic,
	author = {DasGupta, A.},
	date-added = {2020-06-21 18:03:35 -0700},
	date-modified = {2020-06-21 18:03:35 -0700},
	isbn = {9780387759715},
	lccn = {2008921241},
	publisher = {Springer New York},
	series = {Springer Texts in Statistics},
	title = {Asymptotic Theory of Statistics and Probability},
	url = {https://books.google.com/books?id=sX4\_AAAAQBAJ},
	year = {2008},
	bdsk-url-1 = {https://books.google.com/books?id=sX4%5C_AAAAQBAJ}}

@book{lee2012introduction,
	author = {Lee, J.},
	date-added = {2020-04-30 15:37:56 -0700},
	date-modified = {2020-04-30 15:37:56 -0700},
	isbn = {9781441999825},
	lccn = {2012945172},
	publisher = {Springer New York},
	series = {Graduate Texts in Mathematics},
	title = {Introduction to Smooth Manifolds},
	url = {https://books.google.com/books?id=xygVcKGPsNwC},
	year = {2012},
	bdsk-url-1 = {https://books.google.com/books?id=xygVcKGPsNwC}}

@book{spivak1971calculus,
	author = {Spivak, M.},
	date-added = {2020-04-29 08:48:52 -0700},
	date-modified = {2020-04-29 08:48:52 -0700},
	isbn = {9780813346120},
	publisher = {Avalon Publishing},
	series = {Mathematics monograph series},
	title = {Calculus On Manifolds: A Modern Approach To Classical Theorems Of Advanced Calculus},
	url = {https://books.google.com/books?id=POIJJJcCyUkC},
	year = {1971},
	bdsk-url-1 = {https://books.google.com/books?id=POIJJJcCyUkC}}

@book{lang2005algebra,
	author = {Lang, S.},
	date-added = {2020-04-27 04:36:11 -0700},
	date-modified = {2020-04-27 04:36:11 -0700},
	isbn = {9780387953854},
	lccn = {01054916},
	publisher = {Springer New York},
	series = {Graduate Texts in Mathematics},
	title = {Algebra},
	url = {https://books.google.com/books?id=Fge-BwqhqIYC},
	year = {2005},
	bdsk-url-1 = {https://books.google.com/books?id=Fge-BwqhqIYC}}

@book{stewart2015calculus,
	author = {Stewart, J.},
	date-added = {2020-04-22 10:21:58 -0700},
	date-modified = {2020-04-22 10:21:58 -0700},
	isbn = {9781305480513},
	publisher = {Cengage Learning},
	title = {Calculus},
	url = {https://books.google.com/books?id=spiaBAAAQBAJ},
	year = {2015},
	bdsk-url-1 = {https://books.google.com/books?id=spiaBAAAQBAJ}}

@book{v2_bertsekas2012dynamic,
	author = {Bertsekas, D.P.},
	date-added = {2020-02-18 14:09:57 -0800},
	date-modified = {2020-02-18 14:09:57 -0800},
	isbn = {9781886529441},
	number = {v. 2},
	publisher = {Athena Scientific},
	series = {Athena Scientific optimization and computation series},
	title = {Dynamic Programming and Optimal Control},
	url = {https://books.google.com/books?id=H-PSMwEACAAJ},
	year = {2012},
	bdsk-url-1 = {https://books.google.com/books?id=H-PSMwEACAAJ}}

@book{v1_bertsekas2012dynamic,
	author = {Bertsekas, D.P.},
	date-added = {2020-02-18 14:09:26 -0800},
	date-modified = {2020-02-18 14:09:40 -0800},
	isbn = {9781886529434},
	number = {v. 1},
	publisher = {Athena Scientific},
	series = {Athena Scientific optimization and computation series},
	title = {Dynamic Programming and Optimal Control},
	url = {https://books.google.com/books?id=REp7swEACAAJ},
	year = {2012},
	bdsk-url-1 = {https://books.google.com/books?id=REp7swEACAAJ}}

@book{rudin1976principles,
	author = {Rudin, W.},
	date-added = {2020-02-07 14:22:21 -0800},
	date-modified = {2020-02-07 14:22:21 -0800},
	isbn = {9780070856134},
	lccn = {75179033},
	publisher = {McGraw-Hill},
	series = {International series in pure and applied mathematics},
	title = {Principles of Mathematical Analysis},
	url = {https://books.google.com/books?id=kwqzPAAACAAJ},
	year = {1976},
	bdsk-url-1 = {https://books.google.com/books?id=kwqzPAAACAAJ}}

@book{pugh2015real,
	author = {Pugh, C.C.},
	date-added = {2020-01-13 10:37:31 -0800},
	date-modified = {2020-01-13 10:37:31 -0800},
	isbn = {9783319177717},
	publisher = {Springer International Publishing},
	series = {Undergraduate Texts in Mathematics},
	title = {Real Mathematical Analysis},
	url = {https://books.google.com/books?id=2NVJCgAAQBAJ},
	year = {2015},
	bdsk-url-1 = {https://books.google.com/books?id=2NVJCgAAQBAJ}}

@book{lehmann2005testing,
	abstract = {``We won't comment here on the long history of the book, which is recounted in [E. L. Lehmann, Statist. Sci. 12 (1997), no. 1, 48--52; MR1466430], but shall use this Preface to indicate the principal changes from the second edition [E. L. Lehmann, Testing statistical hypotheses, Second edition, Wiley, New York, 1986; MR0852406 (87j:62001)]. ``The present volume is divided into two parts. Part I (Chapters 1--10) treats small-sample theory, while Part II (Chapters 11--15) treats large-sample theory. The preface to the second edition stated that `the most important omission is an adequate treatment of optimality paralleling that given for estimation in the companion volume [E. L. Lehmann and G. Casella, Theory of point estimation, Second edition, Springer, New York, 1998; MR1639875 (99g:62025)]'. We here remedy this failure by treating the difficult topic of asymptotic optimality (in Chapter 13) together with the large-sample tools needed for this purpose (in Chapters 11 and 12). Having developed these tools, we use them in Chapter 14 to give a much fuller treatment of tests of goodness of fit than was possible in the second edition, and in Chapter 15 to provide an introduction to the bootstrap and related techniques. Various large-sample considerations that in the second edition were discussed in earlier chapters now have been moved to Chapter 11.

``Another major addition is a more comprehensive treatment of multiple testing including some recent optimality results. This topic is now presented in Chapter 9. In order to make room for these extensive additions, we had to eliminate some material found in the second edition, primarily the coverage of the multivariate linear hypothesis.

``Except for some of the basic results from Part I, a detailed knowledge of small-sample theory is not required for Part II. In particular, the necessary background should include: Chapter 3, Sections 3.1--3.5, 3.8--3.9; Chapter 4: Sections 4.1--4.4; Chapter 5, Sections 5.1--5.3; Chapter 6, Sections 6.1--6.2; Chapter 7, Sections 7.1--7.2; Chapter 8, Sections 8.1--8.2, 8.4--8.5.'' },
	added-at = {2010-03-21T00:26:58.000+0100},
	address = {New York},
	author = {Lehmann, E. L. and Romano, Joseph P.},
	biburl = {https://www.bibsonomy.org/bibtex/28201f448d2af0d4fd287181787c0d128/peter.ralph},
	date-added = {2019-10-07 11:36:37 -0700},
	date-modified = {2019-10-07 11:37:22 -0700},
	edition = {Third},
	interhash = {9517319578e44006dcf98ae19f983394},
	intrahash = {8201f448d2af0d4fd287181787c0d128},
	isbn = {0-387-98864-5},
	keywords = {book reference statistics},
	mrclass = {62-02 (62C05 62F03 62H15)},
	mrnumber = {MR2135927 (2006m:62005)},
	pages = {xiv+784},
	publisher = {Springer},
	series = {Springer Texts in Statistics},
	timestamp = {2010-03-21T00:26:59.000+0100},
	title = {Testing Statistical Hypotheses},
	year = 2005}

@book{faraway2002,
	author = {Faraway, Julian J.},
	date-added = {2019-06-18 15:16:08 -0700},
	date-modified = {2019-06-18 15:17:11 -0700},
	title = {Practical Regression and Anova using R},
	year = {2002}}

@article{Hurvich1990,
	author = {Hurvich, Clifford M and Tsai, Chih-Ling},
	date-added = {2019-06-18 15:11:51 -0700},
	date-modified = {2019-06-18 15:11:51 -0700},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Hurvich, Tsai - 1990 - The Impact of Model Selection on Inference in Linear Regression.pdf:pdf},
	journal = {The American Statistician},
	keywords = {post-selection inference},
	mendeley-tags = {post-selection inference},
	number = {3},
	pages = {214--217},
	title = {{The Impact of Model Selection on Inference in Linear Regression}},
	url = {https://www-jstor-org.libproxy2.usc.edu/stable/pdf/2685338.pdf?refreqid=excelsior{\%}3Ac89047c5490380fabd3d24aa23ab4155},
	volume = {44},
	year = {1990}}

@book{james2013introduction,
	author = {James, G. and Witten, D. and Hastie, T. and Tibshirani, R.},
	date-added = {2019-06-18 14:45:42 -0700},
	date-modified = {2019-06-18 14:45:42 -0700},
	isbn = {9781461471387},
	keywords = {book.ml,ebook},
	publisher = {Springer},
	series = {Springer Texts in Statistics},
	title = {An Introduction to Statistical Learning: with Applications in R},
	url = {/bib/james/james2013introduction/ISLR+First+Printing.pdf,http://www-bcf.usc.edu/~gareth/ISL/,http://books.google.com.tr/books?id=qcI\_AAAAQBAJ},
	year = {2013},
	bdsk-url-1 = {/bib/james/james2013introduction/ISLR+First+Printing.pdf,http://www-bcf.usc.edu/~gareth/ISL/,http://books.google.com.tr/books?id=qcI%5C_AAAAQBAJ}}

@article{Banerjee2008,
	abstract = {We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added 1-norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our first algorithm uses block coordinate descent, and can be interpreted as recursive 1-norm penalized regression. Our second algorithm, based on Nesterov's first order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data.},
	author = {Banerjee, Onureena and Ghaoui, Laurent El and Edu, Aspremon@princeton},
	date-added = {2019-06-09 15:30:21 -0700},
	date-modified = {2019-06-09 15:30:21 -0700},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Banerjee, Ghaoui, Edu - 2008 - Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data Ale.pdf:pdf},
	journal = {Journal of Machine Learning Research},
	keywords = {DSO 607,precision matrix estimation},
	mendeley-tags = {DSO 607,precision matrix estimation},
	pages = {485--516},
	title = {{Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data Alexandre d'Aspremont}},
	url = {http://delivery.acm.org.libproxy2.usc.edu/10.1145/1400000/1390696/p485-banerjee.pdf?ip=154.59.124.74{\&}id=1390696{\&}acc=OPEN{\&}key=B63ACEF81C6334F5.C52804B674E616B8.4D4702B0C3E38B35.6D218144511F3437{\&}{\_}{\_}acm{\_}{\_}=1559857568{\_}ade261c54a2f13c3a609b160683e627e},
	volume = {9},
	year = {2008},
	bdsk-url-1 = {http://delivery.acm.org.libproxy2.usc.edu/10.1145/1400000/1390696/p485-banerjee.pdf?ip=154.59.124.74%7B%5C&%7Did=1390696%7B%5C&%7Dacc=OPEN%7B%5C&%7Dkey=B63ACEF81C6334F5.C52804B674E616B8.4D4702B0C3E38B35.6D218144511F3437%7B%5C&%7D%7B%5C_%7D%7B%5C_%7Dacm%7B%5C_%7D%7B%5C_%7D=1559857568%7B%5C_%7Dade261c54a2f13c3a609b160683e627e}}

@article{Johnstone2001,
	author = {Johnstone, Iain M},
	date-added = {2019-06-07 11:07:48 -0700},
	date-modified = {2019-06-07 11:07:48 -0700},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Johnstone - 2001 - On The Distribution of the Largest Eigenvalue in Principal Components Analysis.pdf:pdf},
	isbn = {5103001600950},
	journal = {The Annals of Statistics},
	keywords = {DSO 607},
	mendeley-tags = {DSO 607},
	number = {2},
	pages = {295--327},
	title = {{On The Distribution of the Largest Eigenvalue in Principal Components Analysis}},
	url = {https://projecteuclid.org/download/pdf{\_}1/euclid.aos/1009210544},
	volume = {29},
	year = {2001},
	bdsk-url-1 = {https://projecteuclid.org/download/pdf%7B%5C_%7D1/euclid.aos/1009210544}}

@article{Geman1980,
	author = {Geman, Stuart},
	date-added = {2019-06-07 11:07:14 -0700},
	date-modified = {2019-06-07 11:07:14 -0700},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Geman - 1980 - A Limit Theorem for the Norm of Random Matrices.pdf:pdf},
	journal = {The Annals of Probability},
	keywords = {DSO 607},
	mendeley-tags = {DSO 607},
	number = {2},
	pages = {252--261},
	title = {{A Limit Theorem for the Norm of Random Matrices}},
	url = {https://www-jstor-org.libproxy2.usc.edu/stable/pdf/2243269.pdf?refreqid=excelsior{\%}3Aab0d3919c2193ca3adda220e5928076c},
	volume = {8},
	year = {1980}}

@article{Bai2011,
	abstract = {Estimating covariance matrices is an important part of portfolio selection, risk management, and asset pricing. This paper reviews the recent development in estimating high dimensional covariance matrices, where the number of variables can be greater than the number of observations. The limitations of the sample covariance matrix are discussed. Several new approaches are presented, including the shrinkage method, the observable and latent factor method, the Bayesian approach, and the random matrix theory approach. For each method, the construction of covariance matrices is given. The relationships among these methods are discussed.},
	author = {Bai, Jushan and Shi, Shuzhong},
	date-added = {2019-06-07 11:06:42 -0700},
	date-modified = {2019-06-07 11:06:42 -0700},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Bai, Shi - 2011 - Estimating High Dimensional Covariance Matrices and its Applications.pdf:pdf},
	journal = {Annals of Economics and Finance},
	keywords = {DSO 607,precision matrix estimation},
	mendeley-tags = {DSO 607,precision matrix estimation},
	number = {2},
	pages = {199--215},
	title = {{Estimating High Dimensional Covariance Matrices and its Applications}},
	url = {http://aeconf.com/articles/nov2011/aef120201.pdf},
	volume = {12},
	year = {2011},
	bdsk-url-1 = {http://aeconf.com/articles/nov2011/aef120201.pdf}}

@article{Fan2008,
	abstract = {High dimensionality comparable to sample size is common in many statistical problems. We examine covariance matrix estimation in the asymptotic framework that the dimensionality p tends to ∞ as the sample size n increases. Motivated by the Arbitrage Pricing Theory in finance, a multi-factor model is employed to reduce dimensionality and to estimate the covariance matrix. The factors are observable and the number of factors K is allowed to grow with p. We investigate the impact of p and K on the performance of the model-based covariance matrix estimator. Under mild assumptions, we have established convergence rates and asymptotic normality of the model-based estimator. Its performance is compared with that of the sample covariance matrix. We identify situations under which the factor approach increases performance substantially or marginally. The impacts of covariance matrix estimation on optimal portfolio allocation and portfolio risk assessment are studied. The asymptotic results are supported by a thorough simulation study.},
	author = {Fan, Jianqing and Fan, Yingying and Lv, Jinchi},
	date-added = {2019-06-07 11:05:38 -0700},
	date-modified = {2019-06-07 11:05:38 -0700},
	doi = {10.1016/j.jeconom.2008.09.017},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Fan, Fan, Lv - 2008 - High dimensional covariance matrix estimation using a factor model.pdf:pdf},
	journal = {Journal of Econometrics},
	keywords = {DSO 607},
	mendeley-tags = {DSO 607},
	pages = {186--197},
	title = {{High dimensional covariance matrix estimation using a factor model}},
	url = {www.elsevier.com/locate/jeconom},
	volume = {147},
	year = {2008},
	bdsk-url-1 = {www.elsevier.com/locate/jeconom},
	bdsk-url-2 = {https://doi.org/10.1016/j.jeconom.2008.09.017}}

@article{Friedman2008,
	abstract = {We consider the problem of estimating sparse graphs by a lasso penalty applied to the inverse covariance matrix. Using a coordinate descent procedure for the lasso, we develop a simple algorithm-the graphical lasso-that is remarkably fast: It solves a 1000-node problem (∼500 000 parameters) in at most a minute and is 30-4000 times faster than competing methods. It also provides a conceptual link between the exact problem and the approximation suggested by Meinshausen and B{\"{u}}hlmann (2006). We illustrate the method on some cell-signaling data from proteomics.},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	date-added = {2019-06-06 15:26:57 -0700},
	date-modified = {2019-06-06 15:26:57 -0700},
	doi = {10.1093/biostatistics/kxm045},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Friedman, Hastie, Tibshirani - 2008 - Sparse inverse covariance estimation with the graphical lasso.pdf:pdf},
	journal = {Biostatistics},
	keywords = {DSO 607,precision matrix estimation},
	mendeley-tags = {DSO 607,precision matrix estimation},
	number = {3},
	pages = {432--441},
	title = {{Sparse inverse covariance estimation with the graphical lasso}},
	url = {https://academic.oup.com/biostatistics/article-abstract/9/3/432/224260},
	volume = {9},
	year = {2008},
	bdsk-url-1 = {https://academic.oup.com/biostatistics/article-abstract/9/3/432/224260},
	bdsk-url-2 = {https://doi.org/10.1093/biostatistics/kxm045}}

@article{Muller1959,
	author = {Muller, Mervin E},
	date-added = {2019-06-02 16:46:59 -0400},
	date-modified = {2019-06-02 16:46:59 -0400},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Mvller - 1957 - A Note on a Uniformly Method for Generating Points on N-Dimensional Spheres.pdf:pdf},
	journal = {Communications of the ACM},
	pages = {19--20},
	title = {{A Note on a Uniformly Method for Generating Points on N-Dimensional Spheres}},
	url = {http://delivery.acm.org.libproxy1.usc.edu/10.1145/380000/377946/p19-muller.pdf?ip=132.174.255.3{\&}id=377946{\&}acc=ACTIVE SERVICE{\&}key=B63ACEF81C6334F5.C52804B674E616B8.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1559508442{\_}3ad6c8548bc08a0694a945e1d7f5e5e5},
	volume = {2},
	year = {1959},
	bdsk-url-1 = {http://delivery.acm.org.libproxy1.usc.edu/10.1145/380000/377946/p19-muller.pdf?ip=132.174.255.3%7B%5C&%7Did=377946%7B%5C&%7Dacc=ACTIVE%20SERVICE%7B%5C&%7Dkey=B63ACEF81C6334F5.C52804B674E616B8.4D4702B0C3E38B35.4D4702B0C3E38B35%7B%5C&%7D%7B%5C_%7D%7B%5C_%7Dacm%7B%5C_%7D%7B%5C_%7D=1559508442%7B%5C_%7D3ad6c8548bc08a0694a945e1d7f5e5e5}}

@article{Schennach2016,
	abstract = {This article reviews recent significant progress made in developing estimation and inference methods for nonlinear models in the presence of mis-measured data that may or may not conform to the classical assumption of independent zero-mean errors. The aim is to cover a broad range of methods having differing levels of complexity and strength of the required assumptions. Simple approaches that form the elementary building blocks of more advanced approaches are discussed first. Then, special attention is devoted to methods that rely on readily available auxiliary variables (e.g., repeated measurements, indicators, or instrumental variables). Results relaxing most of the commonly invoked simplifying assumptions are presented (linear measurement structure, independent errors, zero-mean errors, availability of auxiliary information). This article also provides an overview of important connections with related fields, such as latent variable models, nonlinear panel data, factor models, and set identification, and applications of the methods to other fields traditionally unrelated to measurement error models.},
	author = {Schennach, Susanne M},
	date-added = {2019-06-02 16:45:40 -0400},
	date-modified = {2019-06-02 16:45:40 -0400},
	doi = {10.1146/annurev-economics-080315-015058},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Schennach - 2016 - Recent Advances in the Measurement Error Literature.pdf:pdf},
	journal = {Annual Review of Economics},
	keywords = {Read for Jacob,measurement error},
	mendeley-tags = {Read for Jacob,measurement error},
	pages = {341--377},
	title = {{Recent Advances in the Measurement Error Literature}},
	url = {www.annualreviews.org},
	volume = {8},
	year = {2016},
	bdsk-url-1 = {www.annualreviews.org},
	bdsk-url-2 = {https://doi.org/10.1146/annurev-economics-080315-015058}}

@article{Schwarz1978,
	author = {Schwarz, Gideon},
	date-added = {2019-05-24 09:36:49 -0700},
	date-modified = {2019-05-24 09:36:49 -0700},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Schwarz - 1978 - Estimating the Dimension of a Model.pdf:pdf},
	journal = {The Annals of Statistics},
	number = {2},
	pages = {461--464},
	title = {{Estimating the Dimension of a Model}},
	url = {https://www.andrew.cmu.edu/user/kk3n/simplicity/schwarzbic.pdf},
	volume = {6},
	year = {1978},
	bdsk-url-1 = {https://www.andrew.cmu.edu/user/kk3n/simplicity/schwarzbic.pdf}}

@article{Phillips1986,
	abstract = {This paper develops a general asymptotic theory of regression for processes which are integrated of order one. The theory includes vector autoregressions and multivariate regressions amongst integrated processes that are driven by innovation sequences which allow for a wide class of weak dependence and heterogeneity. The models studied cover cointegrated systems such as those advanced recently by Granger and Engle and quite general linear simultaneous equations systems with contemporaneous regressor error correlation and serially correlated errors. Problems of statistical testing in vector autoregressions and multivariate regressions with integrated processes are also studied. It is shown that the asympotic theory for conventional tests involves major departures from classical theory and raises new and important issues of the presence of nuisance parameters in the limiting distribution theory. ? 1986 Oxford University Press.},
	author = {Phillips, P. C. B. and Durlauf, S. N.},
	date-added = {2019-05-23 20:23:35 -0700},
	date-modified = {2019-05-23 20:23:35 -0700},
	doi = {10.2307/2297602},
	file = {:Users/gregfaletto/Google Drive/Data Science/LaTeX/Archive/Econ 613/Problem Set 4/Phillips and Durlaf (1986) Multiple time series regression with integrated processes.pdf:pdf},
	issn = {00346527},
	journal = {The Review of Economic Studies},
	number = {4},
	pages = {473--495},
	title = {{Multiple Time Series Regression with Integrated Processes}},
	volume = {53},
	year = {1986},
	bdsk-url-1 = {https://doi.org/10.2307/2297602}}

@article{Breiman1995,
	author = {Breiman, Leo},
	date-added = {2019-05-23 15:58:01 -0700},
	date-modified = {2019-05-23 15:58:01 -0700},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Breiman - 1995 - Better Subset Regression Using the Nonnegative Garrote.pdf:pdf},
	journal = {Technometrics},
	number = {4},
	pages = {373--384},
	title = {{Better Subset Regression Using the Nonnegative Garrote}},
	url = {https://www-jstor-org.libproxy2.usc.edu/stable/pdf/1269730.pdf?refreqid=excelsior{\%}3A76eea9bd08301e990d7d6edd86067262},
	volume = {37},
	year = {1995}}

@book{ross2008stochastic,
	author = {Ross, S.M.},
	date-added = {2019-05-23 15:44:56 -0700},
	date-modified = {2019-05-23 15:45:15 -0700},
	edition = {2nd Ed.},
	isbn = {9788126517572},
	publisher = {Wiley India Pvt. Limited},
	series = {Wiley series in probability and statistics},
	title = {Stochastic Processes},
	url = {https://books.google.com/books?id=HVHqPgAACAAJ},
	year = {2008},
	bdsk-url-1 = {https://books.google.com/books?id=HVHqPgAACAAJ}}

@incollection{2014xi,
	address = {Boston},
	booktitle = {Introduction to Probability Models (Eleventh Edition)},
	date-added = {2019-05-23 15:41:37 -0700},
	date-modified = {2019-05-23 15:41:37 -0700},
	doi = {https://doi.org/10.1016/B978-0-12-407948-9.00016-5},
	edition = {Eleventh Edition},
	editor = {Sheldon Ross},
	isbn = {978-0-12-407948-9},
	pages = {xi - xv},
	publisher = {Academic Press},
	title = {Preface},
	url = {http://www.sciencedirect.com/science/article/pii/B9780124079489000165},
	year = {2014},
	bdsk-url-1 = {http://www.sciencedirect.com/science/article/pii/B9780124079489000165},
	bdsk-url-2 = {https://doi.org/10.1016/B978-0-12-407948-9.00016-5}}

@book{2014i,
	address = {Boston},
	author = {Ross, Sheldon},
	booktitle = {Introduction to Probability Models (Eleventh Edition)},
	date-added = {2019-05-23 15:41:37 -0700},
	date-modified = {2019-05-23 15:42:49 -0700},
	doi = {https://doi.org/10.1016/B978-0-12-407948-9.00012-8},
	edition = {Eleventh Edition},
	isbn = {978-0-12-407948-9},
	pages = {i},
	publisher = {Academic Press},
	title = {Introduction to Probability Models},
	url = {http://www.sciencedirect.com/science/article/pii/B9780124079489000128},
	year = {2014},
	bdsk-url-1 = {http://www.sciencedirect.com/science/article/pii/B9780124079489000128},
	bdsk-url-2 = {https://doi.org/10.1016/B978-0-12-407948-9.00012-8}}

@book{CaseBerg:01,
	abstract = {{This book builds theoretical statistics from the first
		  principles of probability theory. Starting from the basics
		  of probability, the authors develop the theory of
		  statistical inference using techniques, definitions, and
		  concepts that are statistical and are natural extensions
		  and consequences of previous concepts. Intended for
		  first-year graduate students, this book can be used for
		  students majoring in statistics who have a solid
		  mathematics background. It can also be used in a way that
		  stresses the more practical uses of statistical theory,
		  being more concerned with understanding basic statistical
		  concepts and deriving reasonable statistical procedures for
		  a variety of situations, and less concerned with formal
		  optimality investigations.}},
	added-at = {2009-10-28T04:42:52.000+0100},
	author = {Casella, George and Berger, Roger},
	biburl = {https://www.bibsonomy.org/bibtex/21597678f36e23439610affbf46adec1c/jwbowers},
	citeulike-article-id = {105644},
	date-added = {2019-05-23 15:36:44 -0700},
	date-modified = {2019-05-23 15:36:44 -0700},
	howpublished = {{Textbook Binding}},
	interhash = {2dd8caad6c0b6fb80e6334986a231a05},
	intrahash = {1597678f36e23439610affbf46adec1c},
	isbn = {0534243126},
	keywords = {methodology probability statistics},
	month = {June},
	opturl = {http://www.amazon.fr/exec/obidos/ASIN/0534243126/citeulike04-21},
	publisher = {{Duxbury Resource Center}},
	timestamp = {2009-10-28T04:42:57.000+0100},
	title = {Statistical Inference},
	year = 2001}

@article{James2009,
	abstract = {We propose a new algorithm, DASSO, for fitting the entire coefficient path of the Dantzig selector with a similar computational cost to the least angle regression algorithm that is used to compute the lasso. DASSO efficiently constructs a piecewise linear path through a sequential simplex-like algorithm, which is remarkably similar to the least angle regression algorithm. Comparison of the two algorithms sheds new light on the question of how the lasso and Dantzig selector are related. In addition, we provide theoretical conditions on the design matrix X under which the lasso and Dantzig selector coefficient estimates will be identical for certain tuning parameters. As a consequence, in many instances, we can extend the powerful non-asymptotic bounds that have been developed for the Dantzig selector to the lasso. Finally, through empirical studies of simulated and real world data sets we show that in practice, when the bounds hold for the Dantzig selector, they almost always also hold for the lasso.},
	author = {James, Gareth M and Radchenko, Peter and Lv, Jinchi},
	date-added = {2019-05-23 15:24:37 -0700},
	date-modified = {2019-05-23 15:24:37 -0700},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/James, Radchenko, Lv - 2009 - DASSO connections between the Dantzig selector and lasso(2).pdf:pdf},
	journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
	keywords = {DSO 607,DSO 607 Week 4,feature selection},
	mendeley-tags = {DSO 607,DSO 607 Week 4,feature selection},
	number = {1},
	pages = {127--142},
	title = {{DASSO: connections between the Dantzig selector and lasso}},
	url = {https://rss-onlinelibrary-wiley-com.libproxy1.usc.edu/doi/pdf/10.1111/j.1467-9868.2008.00668.x},
	volume = {71},
	year = {2009},
	bdsk-url-1 = {https://rss-onlinelibrary-wiley-com.libproxy1.usc.edu/doi/pdf/10.1111/j.1467-9868.2008.00668.x}}

@article{Donoho1994,
	author = {Donoho, David L and Johnstone, Iain M},
	date-added = {2019-05-23 15:22:40 -0700},
	date-modified = {2019-05-23 15:22:40 -0700},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Donoho, Johnstone - 1994 - Ideal Spatial Adaptation by Wavelet Shrinkage.pdf:pdf},
	journal = {Biometrika},
	number = {3},
	pages = {425--455},
	title = {{Ideal Spatial Adaptation by Wavelet Shrinkage}},
	url = {https://www-jstor-org.libproxy2.usc.edu/stable/pdf/2337118.pdf?refreqid=excelsior{\%}3Afb36dc2b9ad4d3d57225eebb75e05df2},
	volume = {81},
	year = {1994}}

@article{Antoniadis2001,
	abstract = {In this paper, we introduce nonlinear regularized wavelet estimators for estimating nonparametric regression functions when sampling points are not uniformly spaced. The approach can apply readily to many other statistical contexts. Various new penalty functions are proposed. The hard-thresholding and soft-thresholding estimators of Donoho and Johnstone are specii c members of nonlinear regularized wavelet estimators. They correspond to the lower and upper envelopes of a class of the penalized least squares estimators. Necessary conditions for penalty functions are given for regularized estimators to possess thresholding properties. Oracle inequalities and universal thresholding parameters are obtained for a large class of penalty functions. The sampling properties of nonlinear regularized wavelet estimators are established and are shown to be adaptively minimax. To eff ciently solve penalized least squares problems, nonlinear regularized Sobolev interpolators (NRS I) are proposed as initial estimators, which are shown to have good sampling properties. The NRS I is further ameliorated by regularized one-step estimators, which are the one-step estimators of the penalized least squares problems using the NRS I as initial estimators. The graduated nonconvexity algorithm is also introduced to handle penalized least squares problems. The newly introduced approaches are illustrated by a few numerical examples.},
	author = {Antoniadis, Anestis and Fan, Jianqing},
	date-added = {2019-05-23 15:17:10 -0700},
	date-modified = {2019-05-23 15:17:10 -0700},
	doi = {10.1198/016214501753208942},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Antoniadis, Fan - 2001 - Regularization of Wavelet Approximations(2).pdf:pdf},
	issn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	keywords = {Asymptotic minimax,Irregular designs,Nonquadratic penality functions,Oracle inequalities,Penalized least-squares,ROSE,Wavelets},
	pages = {939--967},
	title = {{Regularization of Wavelet Approximations}},
	url = {https://www.tandfonline.com/action/journalInformation?journalCode=uasa20},
	volume = {96},
	year = {2001},
	bdsk-url-1 = {https://www.tandfonline.com/action/journalInformation?journalCode=uasa20},
	bdsk-url-2 = {https://doi.org/10.1198/016214501753208942}}

@inproceedings{Akaike1973,
	address = {Tsahkadsor, Armenia, USSR},
	author = {Akaike, Hirotugu},
	booktitle = {2nd International Symposium on Information Theory},
	date-added = {2019-05-23 15:12:35 -0700},
	date-modified = {2019-05-23 15:12:35 -0700},
	editor = {Petrov, B. N. and Cs{\'{a}}ki, F},
	pages = {267--281},
	title = {{Information theory and an extension of the maximum likelihood principle}},
	year = {1973}}

@book{boyd2004convex,
	author = {Boyd, S. and Boyd, S.P. and Vandenberghe, L. and Cambridge University Press},
	date-added = {2019-05-23 14:49:04 -0700},
	date-modified = {2019-05-23 14:49:04 -0700},
	isbn = {9780521833783},
	lccn = {03063284},
	publisher = {Cambridge University Press},
	series = {Berichte {\"u}ber verteilte messysteme},
	title = {Convex Optimization},
	url = {https://books.google.com/books?id=mYm0bLd3fcoC},
	year = {2004},
	bdsk-url-1 = {https://books.google.com/books?id=mYm0bLd3fcoC}}

@book{loeve1977probability,
	author = {Loeve, M.},
	date-added = {2019-05-22 12:14:55 -0700},
	date-modified = {2019-05-22 12:14:55 -0700},
	isbn = {9780387902104},
	lccn = {76028332},
	publisher = {Springer},
	series = {Comprehensive Manuals of Surgical Specialties},
	title = {Probability Theory I},
	url = {https://books.google.com/books?id=\_9xWBlvUEuIC},
	year = {1977},
	bdsk-url-1 = {https://books.google.com/books?id=%5C_9xWBlvUEuIC}}

@book{rao1973linear,
	author = {Rao, C.R.},
	date-added = {2019-05-22 12:12:35 -0700},
	date-modified = {2019-05-22 12:12:35 -0700},
	isbn = {9780471708230},
	lccn = {72013093},
	publisher = {Wiley},
	series = {Wiley series in probability and mathematical statistics: Probability and mathematical statistics},
	title = {Linear statistical inference and its applications},
	url = {https://books.google.com/books?id=lPhQAAAAMAAJ},
	year = {1973},
	bdsk-url-1 = {https://books.google.com/books?id=lPhQAAAAMAAJ}}

@book{serfling1980,
	added-at = {2009-08-21T09:49:34.000+0200},
	address = {New York, NY [u.a.]},
	author = {Serfling, {Robert J.}},
	biburl = {https://www.bibsonomy.org/bibtex/2fc5789fd0d355fc0adf3391a0753c972/fbw_hannover},
	date-added = {2019-05-22 12:10:31 -0700},
	date-modified = {2019-05-22 12:10:42 -0700},
	edition = {[Nachdr.]},
	interhash = {4a6682027949f152924a6c3fe52fd2bc},
	intrahash = {fc5789fd0d355fc0adf3391a0753c972},
	isbn = {0471024031},
	keywords = {Asymptotische_Statistik Mathematische_Statistik Sch{\"a}tztheorie},
	pagetotal = {XIV, 371},
	ppn_gvk = {024353353},
	publisher = {Wiley},
	series = {Wiley series in probability and mathematical statistics : Probability and mathematical statistics},
	timestamp = {2009-08-21T09:50:25.000+0200},
	title = {Approximation theorems of mathematical statistics},
	url = {http://gso.gbv.de/DB=2.1/CMD?ACT=SRCHA&SRT=YOP&IKT=1016&TRM=ppn+024353353&sourceid=fbw_bibsonomy},
	year = 1980,
	bdsk-url-1 = {http://gso.gbv.de/DB=2.1/CMD?ACT=SRCHA&SRT=YOP&IKT=1016&TRM=ppn+024353353&sourceid=fbw_bibsonomy}}

@book{kingman1966introduction,
	author = {Kingman, J.F.C. and Taylor, S.J.},
	date-added = {2019-05-22 12:07:31 -0700},
	date-modified = {2019-05-22 12:07:31 -0700},
	publisher = {Cambridge University Press},
	title = {Introduction to Measure and Probability},
	url = {https://books.google.com/books?id=JbtEAAAAIAAJ},
	year = {1966},
	bdsk-url-1 = {https://books.google.com/books?id=JbtEAAAAIAAJ}}

@book{grimmett2001probability,
	added-at = {2012-05-18T19:41:41.000+0200},
	author = {Grimmett, G.R. and Stirzaker, D.R.},
	biburl = {https://www.bibsonomy.org/bibtex/22bbae2be9d547ef9bbc4103b367805ca/peter.ralph},
	date-added = {2019-05-22 12:05:01 -0700},
	date-modified = {2019-05-22 12:05:01 -0700},
	interhash = {9e023bc0f37e304148d2a3cd4abf32f6},
	intrahash = {2bbae2be9d547ef9bbc4103b367805ca},
	keywords = {probability reference textbook},
	number = 391,
	publisher = {Oxford university press},
	timestamp = {2012-05-18T19:41:41.000+0200},
	title = {Probability and random processes},
	url = {http://scholar.google.com/scholar.bib?q=info:xzStZXK20NkJ:scholar.google.com/&output=citation&hl=en&as_sdt=0,5&ct=citation&cd=0},
	volume = 80,
	year = 2001,
	bdsk-url-1 = {http://scholar.google.com/scholar.bib?q=info:xzStZXK20NkJ:scholar.google.com/&output=citation&hl=en&as_sdt=0,5&ct=citation&cd=0}}

@article{Osborne2000,
	abstract = {Proposed by Tibshirani, the least absolute shrinkage and selection operator (LASSO) estimates a vector of regression coefficients by minimizing the residual sum of squares subject to a constraint on the l'-norm of the coefficient vector. The LASSO estimator typically has one or more zero elements and thus shares characteristics of both shrinkage estimation and variable selection. In this article we treat the LASSO as a convex programming problem and derive its dual. Consideration of the primal and dual problems together leads to important new insights into the characteristics of the LASSO estimator and to an improved method for estimating its covariance matrix. Using these results we also develop an efficient algorithm for computing LASSO estimates which is usable even in cases where the number of regressors exceeds the number of observations. An S-Plus library based on this algorithm is available from StatLib.},
	author = {Osborne, Michael R and Presnell, Brett and Turlach, Berwin A},
	date-added = {2019-05-17 18:48:40 -0700},
	date-modified = {2019-05-17 18:48:40 -0700},
	doi = {10.1080/10618600.2000.10474883},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Osborne, Presnell, Turlach - 2000 - On the LASSO and its Dual.pdf:pdf},
	issn = {1537-2715},
	journal = {Journal of Computational and Graphical Statistics},
	keywords = {Convex programming,Dual problem,Partial least squares,Penalized re-gression,Quadratic programming,Regression,Shrinkage,Subset selection,Variable se-lection},
	number = {2},
	pages = {319--337},
	title = {{On the LASSO and its Dual}},
	url = {https://www.tandfonline.com/action/journalInformation?journalCode=ucgs20},
	volume = {9},
	year = {2000},
	bdsk-url-1 = {https://www.tandfonline.com/action/journalInformation?journalCode=ucgs20},
	bdsk-url-2 = {https://doi.org/10.1080/10618600.2000.10474883}}

@article{Tibshirani2013,
	abstract = {The lasso is a popular tool for sparse linear regression, especially for problems in which the number of variables p exceeds the number of observations n. But when p{\textgreater}n, the lasso criterion is not strictly convex, and hence it may not have a unique minimum. An important question is: when is the lasso solution well-defined (unique)? We review results from the literature, which show that if the predictor variables are drawn from a continuous probability distribution, then there is a unique lasso solution with probability one, regardless of the sizes of n and p. We also show that this result extends easily to {\$}\backslashell{\_}1{\$} penalized minimization problems over a wide range of loss functions. A second important question is: how can we deal with the case of non-uniqueness in lasso solutions? In light of the aforementioned result, this case really only arises when some of the predictor variables are discrete, or when some post-processing has been performed on continuous predictor measurements. Though we certainly cannot claim to provide a complete answer to such a broad question, we do present progress towards understanding some aspects of non-uniqueness. First, we extend the LARS algorithm for computing the lasso solution path to cover the non-unique case, so that this path algorithm works for any predictor matrix. Next, we derive a simple method for computing the component-wise uncertainty in lasso solutions of any given problem instance, based on linear programming. Finally, we review results from the literature on some of the unifying properties of lasso solutions, and also point out particular forms of solutions that have distinctive properties.},
	author = {Tibshirani, Ryan J.},
	date-added = {2019-05-17 23:22:22 +0000},
	date-modified = {2019-05-17 23:22:22 +0000},
	doi = {10.1214/13-EJS815},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Tibshirani - 2013 - The lasso problem and uniqueness(3).pdf:pdf},
	issn = {19357524},
	journal = {Electronic Journal of Statistics},
	keywords = {High-dimensional,LARS,Lasso,Uniqueness},
	number = {1},
	pages = {1456--1490},
	title = {{The lasso problem and uniqueness}},
	volume = {7},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1214/13-EJS815}}

@unpublished{Pandit2019,
	abstract = {We consider the problem of estimating the parameters of a multivariate Bernoulli process with auto-regressive feedback in the high-dimensional setting where the number of samples available is much less than the number of parameters. This problem arises in learning interconnections of networks of dynamical systems with spiking or binary-valued data. We allow the process to depend on its past up to a lag p, for a general p ≥ 1, allowing for more realistic modeling in many applications. We propose and analyze an 1-regularized maximum likelihood estimator (MLE) under the assumption that the parameter tensor is approximately sparse. Rigorous analysis of such estimators is made challenging by the dependent and non-Gaussian nature of the process as well as the presence of the nonlinearities and multi-level feedback. We derive precise upper bounds on the mean-squared estimation error in terms of the number of samples, dimensions of the process, the lag p and other key statistical properties of the model. The ideas presented can be used in the high-dimensional analysis of regularized M-estimators for other sparse nonlinear and non-Gaussian processes with long-range dependence.},
	archiveprefix = {arXiv},
	arxivid = {1903.09631v1},
	author = {Pandit, Parthe and Sahraee-Ardakan, Mojtaba and Amini, Arash A and Rangan, Sundeep and Fletcher, Alyson K},
	date-added = {2019-05-16 14:42:10 +0000},
	date-modified = {2019-05-16 14:42:10 +0000},
	eprint = {1903.09631v1},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Pandit et al. - Unknown - High-Dimensional Bernoulli Autoregressive Process with Long-Range Dependence.pdf:pdf},
	title = {{High-Dimensional Bernoulli Autoregressive Process with Long-Range Dependence}},
	url = {https://arxiv.org/pdf/1903.09631.pdf},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/pdf/1903.09631.pdf}}

@article{Kleinberg2015,
	abstract = {Empirical policy research often focuses on causal inference. Since policy choices seem to depend on understanding the counterfactual--- what happens with and without a policy---this tight link of causality and policy seems natural. While this link holds in many cases, we argue that there are also many policy applications where causal inference is not central, or even necessary. Consider two toy examples. One policymaker facing a drought must decide whether to invest in a rain dance to increase the chance of rain. Another seeing clouds must decide whether to take an umbrella to work to avoid getting wet on the way home. Both decisions could benefit from an empirical study of rain. But each has differ-ent requirements of the estimator. One requires causality: Do rain dances cause rain? The other does not, needing only prediction: Is the chance of rain high enough to merit an umbrella? We often focus on rain dance--like policy problems. But there are also many umbrella-like policy problems. Not only are these prediction prob-lems neglected, machine learning can help us solve them more effectively. In this paper, we (i) provide a simple frame-work that clarifies the distinction between causation and prediction; (ii) explain how machine learning adds value over traditional regression approaches in solving prediction problems; (iii) provide an empirical example from health policy to illustrate how improved predictions can generate large social impact; (iv) illustrate how " umbrella " problems are common and important in many important pol-icy domains; and (v) argue that solving these problems produces not just policy impact but also theoretical and economic insights. 1 I. Prediction and Causation Let Y be an outcome variable (such as rain) which depends in an unknown way on a set of variables X 0 and X . A policymaker must decide on X 0 (e.g., an umbrella or rain dance) in order to maximize a (known) payoff function $\pi$(X 0 , Y) . Our decision of X 0 depends on the derivative d$\pi$(X 0 , Y)},
	author = {Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil and Obermeyer, Ziad},
	date-added = {2019-05-13 02:50:12 +0000},
	date-modified = {2019-05-13 02:50:12 +0000},
	doi = {10.1257/aer.p20151023},
	file = {:Users/gregfaletto/Desktop/ContentServer.pdf:pdf},
	issn = {0002-8282},
	journal = {American Economic Review},
	keywords = {causal inference,public policy},
	mendeley-tags = {causal inference,public policy},
	number = {5},
	pages = {491--495},
	title = {{Prediction Policy Problems}},
	url = {http://pubs.aeaweb.org/doi/10.1257/aer.p20151023},
	volume = {105},
	year = {2015},
	bdsk-url-1 = {http://pubs.aeaweb.org/doi/10.1257/aer.p20151023},
	bdsk-url-2 = {https://doi.org/10.1257/aer.p20151023}}

@article{Williams2019,
	abstract = {Standard penalized methods of variable selection and parameter estimation rely on the magnitude of coefficient estimates to decide which variables to include in the final model. However, coefficient estimates are unreliable when the design matrix is collinear. To overcome this challenge, an entirely new perspective on variable selection is presented within a generalized fidu-cial inference framework. This new procedure is able to effectively account for linear dependencies among subsets of covariates in a high-dimensional setting where p can grow almost exponentially in n, as well as in the classical setting where p ≤ n. It is shown that the procedure very naturally assigns small probabilities to subsets of covariates which include redundancies by way of explicit L 0 minimization. Furthermore, with a typical sparsity assumption , it is shown that the proposed method is consistent in the sense that the probability of the true sparse subset of covariates converges in probability to 1 as n → ∞, or as n → ∞ and p → ∞. Very reasonable conditions are needed, and little restriction is placed on the class of possible subsets of covariates to achieve this consistency result. 1. Introduction. A strategy for developing variable selection procedures with desirable consistency properties entails exploiting some distinguishing property of the theoretical true data generating model. For example, standard penalized methods of variable selection within a linear model framework such as LASSO of Tib-shirani [21], SCAD of Fan and Li [9] and the Dantzig Selector of Candes and Tao [8] rely on the magnitude of the coefficients in the true data generating model being relatively larger than those of the other coefficients. Johnson and Rossell [13] use this property to construct nonlocal prior densities over all subsets of covari-ates (see also Rossell and Telesca [19] and Shin, Bhattacharya and Johnson [20]). The defining property of their nonlocal density is that it takes the value of zero for subsets containing a covariate with a zero-valued coefficient. We propose a more desirable way for eliminating redundancies from the sample space of candidate subsets which does not explicitly rely on coefficient magnitudes. That is, any candidate true model should be nonredundant in the sense that it},
	author = {Williams, Jonathan P and Hannig, Jan},
	date-added = {2019-05-12 20:37:21 +0000},
	date-modified = {2019-05-12 20:37:21 +0000},
	doi = {10.1214/18-AOS1733},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Williams, Hannig - 2019 - NONPENALIZED VARIABLE SELECTION IN HIGH-DIMENSIONAL LINEAR MODEL SETTINGS VIA GENERALIZED FIDUCIAL INFERENCE.pdf:pdf},
	journal = {The Annals of Statistics},
	keywords = {feature selection},
	mendeley-tags = {feature selection},
	number = {3},
	pages = {1723--1753},
	title = {{Nonpenalized Variable Selection In High-Dimensional Linear Model Settings Via Generalized Fiducial Inference}},
	url = {https://doi.org/10.1214/18-AOS1733},
	volume = {47},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1214/18-AOS1733}}

@article{Tibshirani1996,
	author = {Tibshirani, Robert},
	date-added = {2019-05-10 22:15:00 +0000},
	date-modified = {2019-05-10 22:15:00 +0000},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Tibshirani - 1996 - Regression Shrinkage and Selection via the Lasso.pdf:pdf},
	journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
	keywords = {DSO 607,DSO 607 Week 4},
	mendeley-tags = {DSO 607,DSO 607 Week 4},
	number = {1},
	pages = {267--288},
	title = {{Regression Shrinkage and Selection via the Lasso}},
	volume = {58},
	year = {1996}}

@article{Zhao2006,
	author = {Zhao, Peng and Yu, Bin},
	date-added = {2019-05-10 19:31:47 +0000},
	date-modified = {2019-05-10 19:31:47 +0000},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Zhao, Yu - 2006 - On Model Selection Consistency of Lasso.pdf:pdf},
	journal = {Journal of Machine Learning Research},
	keywords = {DSO 607,DSO 607 Week 4,consistency,lasso,model selection,regularization,sparsity},
	mendeley-tags = {DSO 607,DSO 607 Week 4},
	pages = {2541--2563},
	title = {{On Model Selection Consistency of Lasso}},
	volume = {7},
	year = {2006}}

@book{pesaran-2015-text,
	abstract = {This book is concerned with recent developments in time series and panel data techniques for the analysis of macroeconomic and financial data. It provides a rigorous, nevertheless user-friendly, account of the time series techniques dealing with univariate and multivariate time series models, as well as panel data models. It is distinct from other time series texts in the sense that it also covers panel data models and attempts at a more coherent integration of time series, multivariate analysis, and panel data models. It builds on the author's extensive research in the areas of time series and panel data analysis and covers a wide variety of topics in one volume. Different parts of the book can be used as teaching material for a variety of courses in econometrics. It can also be used as reference manual. It begins with an overview of basic econometric and statistical techniques, and provides an account of stochastic processes, univariate and multivariate time series, tests for unit roots, cointegration, impulse response analysis, autoregressive conditional heteroskedasticity models, simultaneous equation models, vector autoregressions, causality, forecasting, multivariate volatility models, panel data models, aggregation and global vector autoregressive models (GVAR). The techniques are illustrated using Microfit 5 (Pesaran and Pesaran, 2009, OUP) with applications to real output, inflation, interest rates, exchange rates, and stock prices.},
	author = {Pesaran, M. Hashem},
	date-added = {2019-04-17 13:51:50 +0000},
	date-modified = {2019-04-17 13:52:07 +0000},
	isbn = {ARRAY(0x3bdaaf68)},
	number = {9780198759980},
	publisher = {Oxford University Press},
	series = {OUP Catalogue},
	title = {{Time Series and Panel Data Econometrics}},
	url = {https://ideas.repec.org/b/oxp/obooks/9780198759980.html},
	year = 2015,
	bdsk-url-1 = {https://ideas.repec.org/b/oxp/obooks/9780198759980.html}}

@article{Bien,
	author = {Bien, Jacob and Wegkamp, Marten},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Bien, Wegkamp - Unknown - Discussion of ``correlated variables in regression clustering and sparse estimation''.pdf:pdf},
	journal = {Journal of Statistical Planning and Inference},
	number = {11},
	pages = {1--7},
	title = {{Discussion of ``correlated variables in regression: clustering and sparse estimation''}},
	volume = {143},
	year = {2013}}

@article{Bien2011,
	abstract = {Agglomerative hierarchical clustering is a popular class of methods for understanding the structure of a dataset. The nature of the clustering depends on the choice of linkage-that is, on how one measures the distance between clusters. In this article we investigate minimax linkage, a recently introduced but little-studied linkage. Minimax linkage is unique in naturally associating a prototype chosen from the original dataset with every interior node of the dendrogram. These prototypes can be used to greatly enhance the interpretability of a hierarchical clustering. Furthermore, we prove that minimax linkage has a number of desirable theoretical properties; for example, minimax-linkage dendrograms cannot have inversions (unlike centroid linkage) and is robust against certain perturbations of a dataset. We provide an efficient implementation and illustrate minimax linkage's strengths as a data analysis and visualization tool on a study of words from encyclopedia articles and on a dataset of images of human faces.},
	author = {Bien, Jacob and Tibshirani, Robert},
	doi = {10.1198/jasa.2011.tm10183},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Bien, Tibshirani - 2011 - Hierarchical Clustering With Prototypes via Minimax Linkage.pdf:pdf},
	journal = {Journal of the American Statistical Association},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	pages = {1075--1084},
	title = {{Hierarchical Clustering With Prototypes via Minimax Linkage}},
	url = {https://www.tandfonline.com/action/journalInformation?journalCode=uasa20},
	volume = {106},
	year = {2011},
	bdsk-url-1 = {https://www.tandfonline.com/action/journalInformation?journalCode=uasa20},
	bdsk-url-2 = {https://doi.org/10.1198/jasa.2011.tm10183}}

@article{Lim2016,
	abstract = {Cross-validation (CV) is often used to select the regularization parameter in high-dimensional problems. However, when applied to the sparse modeling method Lasso, CV leads to models that are unstable in high-dimensions, and consequently not suited for reliable interpretation. In this article, we propose a model-free criterion ESCV based on a new estimation stability (ES) metric and CV. Our proposed ESCV finds a smaller and locally ES-optimal model smaller than the CV choice so that it fits the data and also enjoys estimation stability property. We demonstrate that ESCV is an effective alternative to CV at a similar easily parallelizable computational cost. In particular, we compare the two approaches with respect to several performance measures when applied to the Lasso on both simulated and real datasets. For dependent predictors common in practice, our main finding is that ESCV cuts down false positive rates often by a large margin, while sacrificing little of true positive rates. ESCV usually outperforms CV in terms of parameter estimation while giving similar performance as CV in terms of prediction. For the two real datasets from neuroscience and cell biology, the models found by ESCV are less than half of the model sizes by CV, but preserves CV's predictive performance and corroborates with subject knowledge and independent work. We also discuss some regularization parameter alignment issues that come up in both approaches. Supplementary materials are available online.},
	author = {Lim, Chinghway and Yu, Bin},
	doi = {10.1080/10618600.2015.1020159},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Lim, Yu - 2016 - Estimation Stability With Cross-Validation (ESCV).pdf:pdf},
	journal = {Journal of Computational and Graphical Statistics},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	number = {2},
	pages = {464--492},
	title = {{Estimation Stability With Cross-Validation (ESCV)}},
	url = {https://www.tandfonline.com/action/journalInformation?journalCode=ucgs20},
	volume = {25},
	year = {2016},
	bdsk-url-1 = {https://www.tandfonline.com/action/journalInformation?journalCode=ucgs20},
	bdsk-url-2 = {https://doi.org/10.1080/10618600.2015.1020159}}

@article{Xue2017,
	abstract = {Penalty-based variable selection methods are powerful in selecting relevant covariates and estimating coefficients simultaneously. However, variable selection could fail to be consistent when covariates are highly correlated. The partial correlation approach has been adopted to solve the problem with correlated covariates. Nevertheless, the restrictive range of partial correlation is not effective for capturing signal strength for relevant covariates. In this paper, we propose a new Semi-standard PArtial Covariance (SPAC) which is able to reduce correlation effects from other predictors while incorporating the magnitude of coefficients. The proposed SPAC variable selection facilitates choosing covariates which have direct association with the response variable, via utilizing dependency among covariates. We show that the proposed method with the Lasso penalty (SPAC-Lasso) enjoys strong sign consistency in both finite-dimensional and high-dimensional settings under regularity conditions. Simulation studies and the `HapMap' gene data application show that the proposed method outperforms the traditional Lasso, adaptive Lasso, SCAD, and Peter-Clark-simple (PC-simple) methods for highly correlated predictors.},
	archiveprefix = {arXiv},
	arxivid = {1709.04840},
	author = {Xue, Fei and Qu, Annie},
	eprint = {1709.04840},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Xue, Qu - 2017 - Variable Selection for Highly Correlated Predictors.pdf:pdf},
	keywords = {Read for Jacob,feature selection,irrepresentable condition,lasso,model selection consistency,partial correlation,scad},
	mendeley-tags = {feature selection,Read for Jacob},
	title = {{Variable Selection for Highly Correlated Predictors}},
	url = {http://arxiv.org/abs/1709.04840},
	volume = {61820},
	year = {2017},
	bdsk-url-1 = {http://arxiv.org/abs/1709.04840}}

@article{GSell2016,
	abstract = {We consider a multiple hypothesis testing setting where the hypotheses are ordered and one is only permitted to reject an initial contiguous block, H{\_}1,$\backslash$dots,H{\_}k, of hypotheses. A rejection rule in this setting amounts to a procedure for choosing the stopping point k. This setting is inspired by the sequential nature of many model selection problems, where choosing a stopping point or a model is equivalent to rejecting all hypotheses up to that point and none thereafter. We propose two new testing procedures, and prove that they control the false discovery rate in the ordered testing setting. We also show how the methods can be applied to model selection using recent results on p-values in sequential model selection settings.},
	archiveprefix = {arXiv},
	arxivid = {1309.5352v2},
	author = {G'Sell, Max Grazier and Wager, Stefan and Chouldechova, Alexandra and Tibshirani, Robert},
	doi = {10.1111/rssb.12122},
	eprint = {1309.5352v2},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/G'Sell et al. - 2016 - Sequential selection procedures and false discovery rate control.pdf:pdf},
	issn = {14679868},
	journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
	keywords = {False discovery rate,Multiple-hypothesis testing,Read for Jacob,Sequential testing,Stopping rule,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	title = {{Sequential selection procedures and false discovery rate control}},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1111/rssb.12122}}

@inproceedings{Sanchez2007,
	abstract = {Adequate selection of features may improve accuracy and efficiency of classifier methods. There are two main approaches for feature selection: wrapper methods, in which the features are selected using the classifier, and filter methods, in which the selection of features is independent of the classifier used. Although the wrapper approach may obtain better performances, it requires greater computational resources. For this reason, lately a new paradigm, hybrid approach, that combines both filter and wrapper methods has emerged. One of its problems is to select the filter method that gives the best relevance index for each case, and this is not an easy to solve question. Different approaches to relevance evaluation lead to a large number of indices for ranking and selection. In this paper, several filter methods are applied over artificial data sets with different number of relevant features, level of noise in the output, interaction between features and increasing number of samples. The results obtained for the four filters studied (ReliefF, Correlation-based Feature Selection, Fast Correlated Based Filter and INTERACT) are compared and discussed. The final aim of this study is to select a filter to construct a hybrid method for feature selection.},
	address = {Berlin, Heidelberg},
	author = {S{\'a}nchez-Maro{\~{n}}o, Noelia and Alonso-Betanzos, Amparo and Tombilla-Sanrom{\'a}n, Mar{\'\i}a},
	booktitle = {Intelligent Data Engineering and Automated Learning - IDEAL 2007},
	editor = {Yin, Hujun and Tino, Peter and Corchado, Emilio and Byrne, Will and Yao, Xin},
	isbn = {978-3-540-77226-2},
	pages = {178--187},
	publisher = {Springer Berlin Heidelberg},
	title = {Filter Methods for Feature Selection -- A Comparative Study},
	year = {2007}}

@techreport{Reid2015,
	abstract = {We propose a new approach for sparse regression and marginal testing, for data with correlated features. Our procedure first clusters the features, and then chooses as the cluster prototype the most informative feature in that cluster. Then we apply either sparse regression (lasso) or marginal significance testing to these prototypes. While this kind of strategy is not entirely new, a key feature of our proposal is its use of the post-selection inference theory of Taylor et al. (2014) and Lee et al. (2014) to compute exact p-values and confidence intervals that properly account for the selection of prototypes. We also apply the recent "knockoff" idea of Barber {\&} Cand{\`{e}}s (2014) to provide exact finite sample control of the FDR of our regression procedure. We illustrate our proposals on both real and simulated data.},
	archiveprefix = {arXiv},
	arxivid = {1503.00334v2},
	author = {Reid, Stephen and Tibshirani, Robert},
	eprint = {1503.00334v2},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Reid, Tibshirani - Unknown - Sparse regression and marginal testing using cluster prototypes.pdf:pdf},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	title = {{Sparse regression and marginal testing using cluster prototypes}},
	url = {https://arxiv.org/pdf/1503.00334.pdf},
	year = {2015},
	bdsk-url-1 = {https://arxiv.org/pdf/1503.00334.pdf}}

@misc{Lumley2017,
	author = {Lumley, Thomas},
	keywords = {feature selection},
	mendeley-tags = {feature selection},
	title = {{Fixing an infelicity in leaps}},
	url = {https://notstatschat.rbind.io/2017/01/09/fixing-an-infelicity-inleaps/},
	urldate = {2019-03-31},
	year = {2017},
	bdsk-url-1 = {https://notstatschat.rbind.io/2017/01/09/fixing-an-infelicity-inleaps/}}

@techreport{Jacob2009,
	abstract = {We propose a new penalty function which, when used as regularization for empirical risk minimization procedures, leads to sparse estimators. The support of the sparse vector is typically a union of potentially overlapping groups of co-variates defined a priori, or a set of covariates which tend to be connected to each other when a graph of covariates is given. We study theoretical properties of the estimator, and illustrate its behavior on simulated and breast cancer gene expression data.},
	author = {Jacob, Laurent and Obozinski, Guillaume and {Vert JEAN-PHILIPPEVERT}, Jean-Philippe},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Jacob, Obozinski, Vert JEAN-PHILIPPEVERT - 2009 - Group Lasso with Overlap and Graph Lasso.pdf:pdf},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	title = {{Group Lasso with Overlap and Graph Lasso}},
	url = {http://delivery.acm.org.libproxy2.usc.edu/10.1145/1560000/1553431/p433-jacob.pdf?ip=154.59.124.74{\&}id=1553431{\&}acc=ACTIVE SERVICE{\&}key=B63ACEF81C6334F5.C52804B674E616B8.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1554064234{\_}9d18459636a12f46fc34ccb79ecf},
	year = {2009},
	bdsk-url-1 = {http://delivery.acm.org.libproxy2.usc.edu/10.1145/1560000/1553431/p433-jacob.pdf?ip=154.59.124.74%7B%5C&%7Did=1553431%7B%5C&%7Dacc=ACTIVE%20SERVICE%7B%5C&%7Dkey=B63ACEF81C6334F5.C52804B674E616B8.4D4702B0C3E38B35.4D4702B0C3E38B35%7B%5C&%7D%7B%5C_%7D%7B%5C_%7Dacm%7B%5C_%7D%7B%5C_%7D=1554064234%7B%5C_%7D9d18459636a12f46fc34ccb79ecf}}

@unpublished{Gong2019,
	abstract = {In variable selection, most existing screening methods focus on marginal effects and ignore dependence between covariates. To improve the performance of selection, we incorporate pairwise effects in covariates for screening and penalization. We achieve this by studying the asymptotic distribution of the maximal absolute pairwise sample correlation among independent covariates. The novelty of the theory is in that the convergence is with respect to the dimensionality p, and is uniform with respect to the sample size n. Moreover, we obtain an upper bound for the maximal pairwise R squared when regressing the response onto two different covariates. Based on these extreme value results, we propose a screening procedure to detect covariates pairs that are potentially correlated and associated with the response. We further combine the pairwise screening with Sure Independence Screening [Fan and Lv, 2008] and develop a new regularized variable selection procedure. Numerical studies show that our method is very competitive in terms of both prediction accuracy and variable selection accuracy.},
	archiveprefix = {arXiv},
	arxivid = {1902.03308v1},
	author = {Gong, Siliang and Zhang, Kai and Liu, Yufeng},
	date-added = {2019-02-13 20:40:16 +0000},
	date-modified = {2019-02-13 20:40:16 +0000},
	eprint = {1902.03308v1},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Gong, Zhang, Liu - Unknown - Penalized linear regression with high-dimensional pairwise screening.pdf:pdf},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	title = {{Penalized linear regression with high-dimensional pairwise screening}},
	url = {https://arxiv.org/pdf/1902.03308.pdf},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/pdf/1902.03308.pdf}}

@unpublished{Dong2019,
	abstract = {Variable importance is central to scientific studies, including the social sciences and causal inference, healthcare, and in other domains. However, current notions of variable importance are often tied to a specific predictive model. This is problematic: what if there were multiple well-performing predictive models, and a specific variable is important to some of them and not to others? In that case, we may not be able to tell from a single well-performing model whether a variable is always important in predicting the outcome. Rather than depending on variable importance for a single predictive model, we would like to explore variable importance for all approximately-equally-accurate predictive models. This work introduces the concept of a variable importance cloud, which maps every variable to its importance for every good predictive model. We show properties of the variable importance cloud and draw connections other areas of statistics. We introduce variable importance diagrams as a projection of the variable importance cloud into two dimensions for visualization purposes. Experiments with criminal justice and marketing data illustrate how variables can change dramatically in importance for approximately-equally-accurate predictive models.},
	archiveprefix = {arXiv},
	arxivid = {1901.03209},
	author = {Dong, Jiayun and Rudin, Cynthia},
	date-added = {2019-02-13 20:37:32 +0000},
	date-modified = {2019-02-13 20:37:32 +0000},
	eprint = {1901.03209},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Dong, Rudin - 2019 - Variable Importance Clouds A Way to Explore Variable Importance for the Set of Good Models.pdf:pdf},
	keywords = {Read for Jacob,algorithmic fairness,feature selection},
	mendeley-tags = {Read for Jacob,algorithmic fairness,feature selection},
	title = {{Variable Importance Clouds: A Way to Explore Variable Importance for the Set of Good Models}},
	year = {2019}}

@article{Li2018,
	abstract = {Sparse models for high-dimensional linear regression and machine learning have received substantial attention over the past two decades. Model selection, or determining which features or covariates are the best explanatory variables, is critical to the interpretability of a learned model. Much of the current literature assumes that covariates are only mildly correlated. However, in modern applications ranging from functional MRI to genome-wide association studies, covariates are highly correlated and do not exhibit key properties (such as the restricted eigenvalue condition, RIP, or other related assumptions). This paper considers a high-dimensional regression setting in which a graph governs both correlations among the covariates and the similarity among regression coefficients. Using side information about the strength of correlations among features, we form a graph with edge weights corresponding to pairwise covariances. This graph is used to define a graph total variation regularizer that promotes similar weights for highly correlated features. The graph structure encapsulated by this regularizer helps precondition correlated features to yield provably accurate estimates. Using graph-based regularizers to develop theoretical guarantees for highly-correlated covariates has not been previously examined. This paper shows how our proposed graph-based regularization yields mean-squared error guarantees for a broad range of covariance graph structures and correlation strengths which in many cases are optimal by imposing additional structure on {\$}\backslashbeta{\^{}}{\{}\backslashstar{\}}{\$} which encourages $\backslash$emph{\{}alignment{\}} with the covariance graph. Our proposed approach outperforms other state-of-the-art methods for highly-correlated design in a variety of experiments on simulated and real fMRI data.},
	archiveprefix = {arXiv},
	arxivid = {1803.07658},
	author = {Li, Yuan and Mark, Benjamin and Raskutti, Garvesh and Willett, Rebecca},
	date-added = {2019-02-13 04:48:34 +0000},
	date-modified = {2019-02-13 04:48:34 +0000},
	eprint = {1803.07658},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2018 - Graph-based regularization for regression problems with highly-correlated designs.pdf:pdf},
	isbn = {9781728112954},
	title = {{Graph-based regularization for regression problems with highly-correlated designs}},
	url = {http://arxiv.org/abs/1803.07658},
	year = {2018},
	bdsk-url-1 = {http://arxiv.org/abs/1803.07658}}

@article{She2010,
	abstract = {This paper studies a generic sparse regression problem with a customizable sparsity pattern matrix, motivated by, but not limited to, a supervised gene clustering problem in microarray data analysis. The clustered lasso method is proposed with the l 1-type penalties imposed on both the coefficients and their pairwise differences. Somewhat surprisingly, it behaves differently than the lasso or the fused lasso-the exact clustering effect expected from the l 1 penalization is rarely seen in applications. An asymp-totic study is performed to investigate the power and limitations of the l 1-penalty in sparse regression. We propose to combine data-augmentation and weights to improve the l 1 technique. To address the computational issues in high dimensions, we successfully generalize a popular iterative algorithm both in practice and in theory and propose an 'annealing' algorithm applicable to generic sparse regressions (including the fused/clustered lasso). Some effective accelerating techniques are further investigated to boost the convergence. The accelerated annealing (AA) algorithm, involving only matrix multiplications and thresholdings, can handle a large design matrix as well as a large sparsity pattern matrix.},
	author = {She, Yiyuan},
	date-added = {2019-02-13 02:57:54 +0000},
	date-modified = {2019-02-13 02:57:54 +0000},
	doi = {10.1214/10-EJS578},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/She - 2010 - Sparse regression with exact clustering.pdf:pdf},
	journal = {Electronic Journal of Statistics},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	pages = {1055--1096},
	title = {{Sparse regression with exact clustering}},
	url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.ejs/1286889184},
	volume = {4},
	year = {2010},
	bdsk-url-1 = {https://projecteuclid.org/download/pdfview%7B%5C_%7D1/euclid.ejs/1286889184},
	bdsk-url-2 = {https://doi.org/10.1214/10-EJS578}}

@article{Sharma,
	abstract = {Statistical procedures for variable selection have become integral elements in any analysis. Successful procedures are characterized by high predictive accuracy, yielding interpretable models while retaining computational efficiency. Penalized methods that perform coefficient shrinkage have been shown to be successful in many cases. Models with correlated predictors are particularly challenging to tackle. We propose a penal-ization procedure that performs variable selection while clustering groups of predictors automatically. The oracle properties of this procedure, including consistency in group identification, are also studied. The proposed method compares favorably with existing selection approaches in both prediction accuracy and model discovery, while retaining its computational efficiency. Supplementary materials are available online.},
	author = {Sharma, Dhruv B and Bondell, Howard D and {Helen Zhang}, Hao},
	date-added = {2019-02-13 02:54:39 +0000},
	date-modified = {2019-02-13 02:54:39 +0000},
	doi = {10.1080/15533174.2012.707849},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Sharma et al. - Unknown - Journal of Computational and Graphical Statistics Consistent Group Identification and Variable Selection in Re.pdf:pdf},
	journal = {Journal of Computational and Graphical Statistics},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	number = {2},
	pages = {319--340},
	title = {{Journal of Computational and Graphical Statistics Consistent Group Identification and Variable Selection in Regression With Correlated Predictors Consistent Group Identification and Variable Selection in Regression With Correlated Predictors}},
	url = {https://www.tandfonline.com/action/journalInformation?journalCode=ucgs20},
	volume = {22},
	year = {2013},
	bdsk-url-1 = {https://www.tandfonline.com/action/journalInformation?journalCode=ucgs20},
	bdsk-url-2 = {https://doi.org/10.1080/15533174.2012.707849}}

@article{Su2017,
	abstract = {In regression settings where explanatory variables have very low correlations and there are relatively few effects, each of large magnitude, we expect the Lasso to find the important variables with few errors, if any. This paper shows that in a regime of linear sparsity-meaning that the fraction of variables with a nonvanishing effect tends to a constant, however small-this cannot really be the case, even when the design variables are stochastically independent. We demonstrate that true features and null features are always interspersed on the Lasso path, and that this phenomenon occurs no matter how strong the effect sizes are. We derive a sharp asymptotic trade-off between false and true positive rates or, equivalently, between measures of type I and type II errors along the Lasso path. This trade-off states that if we ever want to achieve a type II error (false negative rate) under a critical value, then anywhere on the Lasso path the type I error (false positive rate) will need to exceed a given threshold so that we can never have both errors at a low level at the same time. Our analysis uses tools from approximate message passing (AMP) theory as well as novel elements to deal with a possibly adaptive selection of the Lasso regularizing parameter.},
	author = {Su, Weijie and Bogdan, Ma{\l}gorzata and Cand{\`{e}}s, Emmanuel},
	date-added = {2019-02-13 01:47:12 +0000},
	date-modified = {2019-02-13 01:47:12 +0000},
	doi = {10.1214/16-AOS1521},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Su, Bogdan, Cand{\`{e}}s - 2017 - FALSE DISCOVERIES OCCUR EARLY ON THE LASSO PATH.pdf:pdf},
	journal = {The Annals of Statistics},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	number = {5},
	pages = {2133--2150},
	title = {{FALSE DISCOVERIES OCCUR EARLY ON THE LASSO PATH}},
	url = {https://projecteuclid-org.libproxy1.usc.edu/download/pdfview{\_}1/euclid.aos/1509436830},
	volume = {45},
	year = {2017},
	bdsk-url-1 = {https://projecteuclid-org.libproxy1.usc.edu/download/pdfview%7B%5C_%7D1/euclid.aos/1509436830},
	bdsk-url-2 = {https://doi.org/10.1214/16-AOS1521}}

@article{Su2018,
	abstract = {Applied statisticians use sequential regression procedures to produce a ranking of explanatory variables and, in settings of low correlations between variables and strong true effect sizes, expect that variables at the very top of this ranking are truly relevant to the response. In a regime of certain sparsity levels, however, three examples of sequential procedures-forward stepwise, the lasso, and least angle regression-are shown to include the first spurious variable unexpectedly early. We derive a rigorous, sharp prediction of the rank of the first spurious variable for these three procedures, demonstrating that the first spurious variable occurs earlier and earlier as the regression coefficients become denser. This counterintuitive phenomenon persists for statistically independent Gaussian random designs and an arbitrarily large magnitude of the true effects. We gain a better understanding of the phenomenon by identifying the underlying cause and then leverage the insights to introduce a simple visualization tool termed the "double-ranking diagram" to improve on sequential methods. As a byproduct of these findings, we obtain the first provable result certifying the exact equivalence between the lasso and least angle regression in the early stages of solution paths beyond orthogonal designs. This equivalence can seamlessly carry over many important model selection results concerning the lasso to least angle regression.},
	author = {Su, Weijie J},
	date-added = {2019-02-13 01:44:45 +0000},
	date-modified = {2019-02-13 01:44:45 +0000},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Su - 2018 - When Is the First Spurious Variable Selected by Sequential Regression Procedures.pdf:pdf},
	journal = {Biometrika},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	title = {{When Is the First Spurious Variable Selected by Sequential Regression Procedures?}},
	url = {http://stat.wharton.upenn.edu/{~}suw/paper/FPlasso.pdf},
	year = {2018},
	bdsk-url-1 = {http://stat.wharton.upenn.edu/%7B~%7Dsuw/paper/FPlasso.pdf}}

@article{Yu2013,
	abstract = {Reproducibility is imperative for any scientific discovery. More often than not, modern scientific findings rely on statistical analysis of high-dimensional data. At a minimum, reproducibility manifests itself in stability of statistical results relative to "reasonable" perturbations to data and to the model used. Jacknife, bootstrap, and cross-validation are based on perturbations to data, while robust statistics methods deal with perturbations to models. In this article, a case is made for the importance of stability in statistics. Firstly, we motivate the necessity of stability for interpretable and reliable encoding models from brain fMRI signals. Secondly, we find strong evidence in the literature to demonstrate the central role of stability in statistical inference, such as sensitivity analysis and effect detection. Thirdly, a smoothing parameter selector based on estimation stability (ES), ES-CV, is proposed for Lasso, in order to bring stability to bear on cross-validation (CV). ES-CV is then utilized in the encoding models to reduce the number of predictors by 60{\%} with almost no loss (1.3{\%}) of prediction performance across over 2,000 voxels. Last, a novel "stability" argument is seen to drive new results that shed light on the intriguing interactions between sample to sample variability and heavier tail error distribution (e.g., double-exponential) in high-dimensional regression models with p predictors and n independent samples. In particular, when p/n → $\kappa$ ∈ (0.3, 1) and the error distribution is double-exponential, the Ordinary Least Squares (OLS) is a better estimator than the Least Absolute Deviation (LAD) estimator.},
	author = {Yu, Bin},
	date-added = {2019-02-13 01:37:18 +0000},
	date-modified = {2019-02-13 01:37:18 +0000},
	doi = {10.3150/13-BEJSP14},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Yu - 2013 - Stability.pdf:pdf},
	journal = {Bernoulli},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	number = {4},
	pages = {1484--1500},
	title = {{Stability}},
	url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.bj/1377612862},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://projecteuclid.org/download/pdfview%7B%5C_%7D1/euclid.bj/1377612862},
	bdsk-url-2 = {https://doi.org/10.3150/13-BEJSP14}}

@article{Park2007,
	author = {Park, M.Y. and Hastie, Trevor and Tibshirani, Rob},
	date-added = {2019-02-11 01:06:32 +0000},
	date-modified = {2019-02-11 01:06:32 +0000},
	journal = {Biostatistics},
	keywords = {Read for Jacob,clustering features,feature selection},
	mendeley-tags = {feature selection,Read for Jacob,clustering features},
	pages = {212--227},
	title = {{Averaged gene expressions for regression}},
	volume = {8},
	year = {2007}}

@article{Hastie2001,
	author = {Hastie, Trevor; and Tibshirani, Rob; and Botstein, R.; and Brown, P.},
	date-added = {2019-02-11 01:04:05 +0000},
	date-modified = {2019-02-11 01:04:05 +0000},
	journal = {Genome Biology},
	keywords = {Read for Jacob,clustering features,feature selection},
	mendeley-tags = {Read for Jacob,feature selection,clustering features},
	number = {1},
	pages = {3.1--3.12},
	title = {{Supervised harvesting of expression trees}},
	volume = {2},
	year = {2001}}

@article{Bondell2008,
	abstract = {Variable selection can be challenging, particularly in situations with a large number of predic-tors with possibly high correlations, such as gene expression data. In this article, a new method called the OSCAR (octagonal shrinkage and clustering algorithm for regression) is proposed to simultaneously select variables while grouping them into predictive clusters. In addition to improving prediction accuracy and interpretation, these resulting groups can then be investigated further to discover what contributes to the group having a similar behavior. The technique is based on penalized least squares with a geometrically intuitive penalty function that shrinks some coefficients to exactly zero. Additionally, this penalty yields exact equality of some coefficients, encouraging correlated predictors that have a similar effect on the response to form predictive clusters represented by a single coefficient. The proposed procedure is shown to compare favorably to the existing shrinkage and variable selection techniques in terms of both prediction error and model complexity, while yielding the additional grouping information.},
	author = {Bondell, Howard D and Reich, Brian J},
	date-added = {2019-02-11 00:59:19 +0000},
	date-modified = {2019-02-11 00:59:19 +0000},
	doi = {10.1111/j.1541-0420.2007.00843.x},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Bondell, Reich - 2008 - Simultaneous Regression Shrinkage, Variable Selection, and Supervised Clustering of Predictors with OSCAR.pdf:pdf},
	journal = {Biometrics},
	keywords = {Read for Jacob,feature selection},
	mendeley-tags = {Read for Jacob,feature selection},
	pages = {115--123},
	title = {{Simultaneous Regression Shrinkage, Variable Selection, and Supervised Clustering of Predictors with OSCAR}},
	url = {https://onlinelibrary-wiley-com.libproxy1.usc.edu/doi/pdf/10.1111/j.1541-0420.2007.00843.x},
	volume = {64},
	year = {2008},
	bdsk-url-1 = {https://onlinelibrary-wiley-com.libproxy1.usc.edu/doi/pdf/10.1111/j.1541-0420.2007.00843.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.1541-0420.2007.00843.x}}

@article{Buhlmann2013,
	abstract = {We consider estimation in a high-dimensional linear model with strongly correlated variables. We propose to cluster the variables first and do subsequent sparse estimation such as the Lasso for cluster-representatives or the group Lasso based on the structure from the clusters. Regarding the first step, we present a novel and bottom-up agglomerative clustering algorithm based on canonical correlations, and we show that it finds an optimal solution and is statistically consistent. We also present some theoretical arguments that canonical correlation based clustering leads to a better-posed compatibility constant for the design matrix which ensures identifiability and an oracle inequality for the group Lasso. Furthermore, we discuss circumstances where cluster-representatives and using the Lasso as subsequent estimator leads to improved results for prediction and detection of variables. We complement the theoretical analysis with various empirical results. {\textcopyright} 2013 Elsevier B.V.},
	archiveprefix = {arXiv},
	arxivid = {1209.5908},
	author = {B{\"{u}}hlmann, Peter and R{\"{u}}timann, Philipp and van de Geer, Sara and Zhang, Cun Hui},
	date-added = {2019-02-11 00:55:49 +0000},
	date-modified = {2019-02-11 00:55:49 +0000},
	doi = {10.1016/j.jspi.2013.05.019},
	eprint = {1209.5908},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/B{\"{u}}hlmann et al. - 2013 - Correlated variables in regression Clustering and sparse estimation.pdf:pdf},
	isbn = {03783758},
	issn = {03783758},
	journal = {Journal of Statistical Planning and Inference},
	keywords = {Canonical correlation,Group Lasso,Hierarchical clustering,High-dimensional inference,Lasso,Oracle inequality,Variable screening,Variable selection},
	number = {11},
	pages = {1835--1858},
	pmid = {11974822},
	publisher = {Elsevier},
	title = {{Correlated variables in regression: Clustering and sparse estimation}},
	url = {http://dx.doi.org/10.1016/j.jspi.2013.05.019},
	volume = {143},
	year = {2013},
	bdsk-url-1 = {http://dx.doi.org/10.1016/j.jspi.2013.05.019}}

@article{Zou2005,
	abstract = {We propose the elastic net, a new regularization and variable selection method.Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p?n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths},
	annote = {Brief discussion in 2.3 about how lasso tends to choose one of highly correlated features at random; defers theory to Efron et al (2004) LARS paper.},
	author = {Zou, Hui and Hastie, Trevor},
	date-added = {2019-02-11 00:42:59 +0000},
	date-modified = {2019-02-11 00:42:59 +0000},
	doi = {10.1016/S0042-6989(99)00110-8},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Zou, Hastie - 2005 - Regularization and variable selection via the elastic net.pdf:pdf},
	isbn = {1369-7412},
	issn = {00426989},
	journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
	keywords = {Contrast constancy,Contrast perception,DSO 607,DSO 607 Week 4,Read for Jacob,Suprathreshold,Texture,Visual modelling},
	mendeley-tags = {DSO 607,DSO 607 Week 4,Read for Jacob},
	number = {2},
	pages = {301--320},
	pmid = {20713001},
	title = {{Regularization and variable selection via the elastic net}},
	volume = {67},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1016/S0042-6989(99)00110-8}}

@article{Efron2004,
	abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
	annote = {Accordng to section 2.3 of elastic net paper, this paper contains a theoretical explanation of why lasso tends to pick one variable among highly correlated ones.},
	author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
	date-added = {2019-02-11 00:42:16 +0000},
	date-modified = {2019-02-11 00:42:16 +0000},
	doi = {10.1214/009053604000000067},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Efron et al. - 2004 - Least Angle Regression.pdf:pdf},
	issn = {0090-5364},
	journal = {The Annals of Statistics},
	keywords = {DSO 607,DSO 607 Week 4,Read for Jacob,and phrases,boosting,coefficient paths,lasso,linear regression,variable selection},
	mendeley-tags = {DSO 607,DSO 607 Week 4,Read for Jacob},
	number = {2},
	pages = {407--499},
	pmid = {1000198917},
	title = {{Least Angle Regression}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=25879},
	volume = {32},
	year = {2004},
	bdsk-url-1 = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=25879},
	bdsk-url-2 = {https://doi.org/10.1214/009053604000000067}}

@techreport{Org2015,
	abstract = {In this paper we introduce fuzzy forests, a novel machine learning algorithm for ranking the importance of features in high-dimensional classification and regression problems. Fuzzy forests is specifically designed to provide relatively unbiased rankings of variable importance in the presence of highly correlated features, especially when p {\textgreater}{\textgreater} n. We introduce our implementation of fuzzy forests in the R package, fuzzyforest. Fuzzy forests works by taking advantage of the network structure between features. First, the features are partitioned into separate modules such that the correlation within modules is high and the correlation between modules is low. The package fuzzyforest allows for easy use of Weighted Gene Coexpression Network Analysis (WGCNA) to form modules of features such that the modules are roughly uncorrelated. Then recursive feature elimination random forests (RFE-RFs) are used on each module, separately. From the surviving features, a final group is selected and ranked using one last round of RFE-RFs. This procedure results in a ranked variable importance list whose size is pre-specified by the user. The selected features can then be used to construct a predictive model.},
	author = {Conn, Daniel and Ngun, Tuck and Li, Gang and Ramirez, Christina M},
	date-added = {2019-02-11 00:41:24 +0000},
	date-modified = {2019-02-11 00:44:57 +0000},
	file = {:Users/gregfaletto/Library/Application Support/Mendeley Desktop/Downloaded/Org et al. - 2015 - UCLA Research Reports Title Fuzzy Forests Extending Random Forests for Correlated, High-Dimensional Data Publication.pdf:pdf},
	keywords = {feature selection,prediction competitions},
	mendeley-tags = {feature selection,prediction competitions},
	title = {{Fuzzy Forests: Extending Random Forests for Correlated, High-Dimensional Data Publication Date Fuzzy Forests: Extending Random Forests for Correlated, High-Dimensional Data}},
	url = {https://escholarship.org/uc/item/55h4h0w7},
	year = {2015},
	bdsk-url-1 = {https://escholarship.org/uc/item/55h4h0w7}}

@article{kang_kuznetsova_choi_luca_2013,
	author = {Kang, Jun Seok and Kuznetsova, Polina and Choi, Yejin and Luca, Michael},
	date-added = {2018-10-19 10:08:32 -0700},
	date-modified = {2018-10-19 10:08:32 -0700},
	doi = {10.2139/ssrn.2293165},
	journal = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	month = {Oct},
	pages = {1443--1448},
	title = {Using Text Analysis to Target Government Inspections: Evidence from Restaurant Hygiene Inspections and Online Reviews},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.2139/ssrn.2293165}}

@article{rockoff_jacob_kane_staiger_2011,
	author = {Rockoff, Jonah and Jacob, Brian and Kane, Thomas and Staiger, Douglas},
	date-added = {2018-10-18 17:16:11 -0700},
	date-modified = {2018-10-18 17:16:11 -0700},
	doi = {10.3386/w14485},
	journal = {Education Finance and Policy},
	number = {1},
	pages = {43--74},
	title = {Can You Recognize an Effective Teacher When You Recruit One?},
	volume = {6},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.3386/w14485}}

@misc{miconi-fairness,
	author = {Thomas Miconi},
	date-added = {2018-10-18 17:08:42 -0700},
	date-modified = {2018-10-18 17:08:53 -0700},
	eprint = {arXiv:1707.01195},
	title = {The impossibility of "fairness": a generalized impossibility result for decisions},
	year = {2017}}

@article{arnold_dobbie_yang_2017,
	author = {Arnold, David and Dobbie, Will and Yang, Crystal},
	date-added = {2018-10-18 17:02:24 -0700},
	date-modified = {2018-10-18 17:02:24 -0700},
	doi = {10.3386/w23421},
	title = {Racial Bias in Bail Decisions},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.3386/w23421}}

@article{kamishima_akaho_asoh_sakuma_2012,
	author = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
	date-added = {2018-10-18 16:48:54 -0700},
	date-modified = {2018-10-18 16:48:54 -0700},
	doi = {10.1007/978-3-642-33486-3_3},
	journal = {Machine Learning and Knowledge Discovery in Databases Lecture Notes in Computer Science},
	pages = {35--50},
	title = {Fairness-Aware Classifier with Prejudice Remover Regularizer},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-642-33486-3_3}}

@article{saunders_hunt_hollywood_2016,
	author = {Saunders, Jessica and Hunt, Priscillia and Hollywood, John S.},
	date-added = {2018-10-04 20:17:01 -0700},
	date-modified = {2018-10-04 20:17:01 -0700},
	doi = {10.1007/s11292-016-9272-0},
	journal = {Journal of Experimental Criminology},
	month = {Dec},
	number = {3},
	pages = {347--371},
	title = {Predictions put into practice: a quasi-experimental evaluation of Chicago's predictive policing pilot},
	volume = {12},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1007/s11292-016-9272-0}}

@misc{tan-2017,
	author = {Sarah Tan and Rich Caruana and Giles Hooker and Yin Lou},
	date-added = {2018-10-04 20:14:37 -0700},
	date-modified = {2018-10-04 20:14:44 -0700},
	eprint = {arXiv:1710.06169},
	title = {Auditing Black-Box Models Using Transparent Model Distillation With Side Information},
	year = {2017}}

@article{ludwig-2015,
	author = {Nicole Ludwig and Stefan Feuerriegel and Dirk Neumann},
	date-added = {2018-10-04 17:11:57 -0700},
	date-modified = {2018-10-04 17:12:06 -0700},
	doi = {10.1080/12460125.2015.994290},
	eprint = {https://doi.org/10.1080/12460125.2015.994290},
	journal = {Journal of Decision Systems},
	number = {1},
	pages = {19-36},
	publisher = {Taylor & Francis},
	title = {Putting Big Data analytics to work: Feature selection for forecasting electricity prices using the LASSO and random forests},
	url = {https://doi.org/10.1080/12460125.2015.994290},
	volume = {24},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1080/12460125.2015.994290}}

@article{schnabel-2015,
	author = {Renate B Schnabel and Xiaoyan Yin and Philimon Gona and Martin G Larson and Alexa S Beiser and David D McManus and Christopher Newton-Cheh and Steven A Lubitz and Jared W Magnani and Patrick T Ellinor and Sudha Seshadri and Philip A Wolf and Ramachandran S Vasan and Emelia J Benjamin and Daniel Levy},
	date-added = {2018-10-04 17:09:18 -0700},
	date-modified = {2018-10-04 17:09:40 -0700},
	doi = {https://doi.org/10.1016/S0140-6736(14)61774-8},
	issn = {0140-6736},
	journal = {The Lancet},
	number = {9989},
	pages = {154 - 162},
	title = {50 year trends in atrial fibrillation prevalence, incidence, risk factors, and mortality in the Framingham Heart Study: a cohort study},
	url = {http://www.sciencedirect.com/science/article/pii/S0140673614617748},
	volume = {386},
	year = {2015},
	bdsk-url-1 = {http://www.sciencedirect.com/science/article/pii/S0140673614617748},
	bdsk-url-2 = {https://doi.org/10.1016/S0140-6736(14)61774-8}}

@article{reichman-2001,
	abstract = {No abstract is available for this item.},
	author = {Reichman, Nancy E. and Teitler, Julien O. and Garfinkel, Irwin and McLanahan, Sara S.},
	date-added = {2018-10-04 16:58:27 -0700},
	date-modified = {2018-10-04 16:58:38 -0700},
	journal = {Children and Youth Services Review},
	number = {4-5},
	pages = {303-326},
	title = {{Fragile Families: sample and design}},
	url = {https://ideas.repec.org/a/eee/cysrev/v23y2001i4-5p303-326.html},
	volume = {23},
	year = 2001,
	bdsk-url-1 = {https://ideas.repec.org/a/eee/cysrev/v23y2001i4-5p303-326.html}}

@misc{lundberg-2018,
	author = {Ian Lundberg and Arvind Narayanan and Karen Levy and Matthew J. Salganik},
	date-added = {2018-10-04 16:55:55 -0700},
	date-modified = {2018-10-04 16:56:06 -0700},
	eprint = {arXiv:1809.00103},
	title = {Privacy, ethics, and data access: A case study of the Fragile Families Challenge},
	year = {2018}}

@article{sorlie_tibshirani_2003,
	author = {S{\o}rlie, Therese and Tibshirani, Robert and Parker, Joel and Hastie, Trevor and Marron, J. S. and Nobel, Andrew and Deng, Shibing and Johnsen, Hilde and Pesich, Robert and Geisler, Stephanie and et al.},
	date-added = {2018-10-04 15:57:01 -0700},
	date-modified = {2018-10-04 15:57:01 -0700},
	doi = {10.1073/pnas.0932692100},
	journal = {Proceedings of the National Academy of Sciences},
	number = {14},
	pages = {8418--8423},
	title = {Repeated observation of breast tumor subtypes in independent gene expression data sets},
	volume = {100},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1073/pnas.0932692100}}

@article{bogdan_berg_sabatti_su_candes_2015,
	author = {Bogdan, Ma{\l}gorzata and Berg, Ewout Van Den and Sabatti, Chiara and Su, Weijie and Cand{\`e}s, Emmanuel J.},
	date-added = {2018-10-04 15:39:09 -0700},
	date-modified = {2018-10-04 15:39:09 -0700},
	doi = {10.1214/15-aoas842},
	journal = {The Annals of Applied Statistics},
	number = {3},
	pages = {1103--1140},
	title = {SLOPE---Adaptive variable selection via convex optimization},
	volume = {9},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1214/15-aoas842}}

@article{barber_candes_2015,
	author = {Barber, Rina Foygel and Cand{\`e}s, Emmanuel J.},
	date-added = {2018-10-04 15:35:16 -0700},
	date-modified = {2018-10-04 15:35:16 -0700},
	doi = {10.1214/15-aos1337},
	journal = {The Annals of Statistics},
	number = {5},
	pages = {2055--2085},
	title = {Controlling the false discovery rate via knockoffs},
	volume = {43},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1214/15-aos1337}}

@misc{gsell-2013,
	author = {Max Grazier G'Sell and Trevor Hastie and Robert Tibshirani},
	date-added = {2018-10-04 14:08:18 -0700},
	date-modified = {2018-10-04 14:08:40 -0700},
	eprint = {arXiv:1302.2303},
	title = {False Variable Selection Rates in Regression},
	year = {2013}}

@article{shah_samworth_2013,
	author = {Shah, Rajen D. and Samworth, Richard J.},
	date-added = {2018-10-04 14:04:46 -0700},
	date-modified = {2018-10-04 14:04:46 -0700},
	doi = {10.1016/j.jspi.2013.05.022},
	journal = {Journal of Statistical Planning and Inference},
	number = {11},
	pages = {1866--1868},
	title = {Discussion of `Correlated variables in regression: Clustering and sparse estimation' by Peter B{\"u}hlmann, Philipp R{\"u}timann, Sara van de Geer and Cun-Hui Zhang},
	volume = {143},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1016/j.jspi.2013.05.022}}

@article{shah_samworth_2012,
	author = {Shah, Rajen D. and Samworth, Richard J.},
	date-added = {2018-10-04 14:03:28 -0700},
	date-modified = {2018-10-04 14:03:28 -0700},
	doi = {10.1111/j.1467-9868.2011.01034.x},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	number = {1},
	pages = {55--80},
	title = {Variable selection with error control: another look at stability selection},
	volume = {75},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1111/j.1467-9868.2011.01034.x}}

@article{meinshausen-2010,
	author = {Meinshausen, Nicolai and B{\"u}hlmann, Peter},
	date-added = {2018-10-04 13:59:56 -0700},
	date-modified = {2018-10-04 14:01:59 -0700},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	month = {May},
	number = {4},
	pages = {417-473},
	title = {Stability Selection},
	volume = {72},
	year = {2010}}

@article{simmons_nelson_simonsohn_2011,
	author = {Simmons, Joseph and Nelson, Leif and Simonsohn, Uri},
	date-added = {2018-08-07 17:34:12 -0700},
	date-modified = {2018-08-07 17:34:12 -0700},
	doi = {10.1037/e519702015-014},
	journal = {PsycEXTRA Dataset},
	title = {False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1037/e519702015-014}}

@misc{gelman_2017,
	author = {Gelman, Andrew},
	date-added = {2018-08-07 17:26:43 -0700},
	date-modified = {2018-10-04 13:53:31 -0700},
	journal = {Statistical Modeling, Causal Inference, and Social Science},
	month = {Dec},
	title = {The 80 Percent Power Lie},
	url = {http://andrewgelman.com/2017/12/04/80-power-lie/},
	year = {2017},
	bdsk-url-1 = {http://andrewgelman.com/2017/12/04/80-power-lie/}}

@article{baker_2016,
	author = {Baker, Monya},
	date-added = {2018-08-07 17:20:27 -0700},
	date-modified = {2018-08-07 17:20:27 -0700},
	doi = {10.1038/533452a},
	journal = {Nature},
	number = {7604},
	pages = {452--454},
	title = {1,500 scientists lift the lid on reproducibility},
	volume = {533},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1038/533452a}}

@article{gould_2009,
	author = {Gould, Elise},
	date-added = {2018-08-07 17:06:31 -0700},
	date-modified = {2018-08-07 17:06:31 -0700},
	doi = {10.1289/ehp.0800408},
	journal = {Environmental Health Perspectives},
	number = {7},
	pages = {1162--1167},
	title = {Childhood Lead Poisoning: Conservative Estimates of the Social and Economic Benefits of Lead Hazard Control},
	volume = {117},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1289/ehp.0800408}}

@article{nevin_2000,
	author = {Nevin, Rick},
	date-added = {2018-08-07 16:58:27 -0700},
	date-modified = {2018-08-07 16:58:27 -0700},
	doi = {10.1006/enrs.1999.4045},
	journal = {Environmental Research},
	number = {1},
	pages = {1--22},
	title = {How Lead Exposure Relates to Temporal Changes in IQ, Violent Crime, and Unwed Pregnancy},
	volume = {83},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1006/enrs.1999.4045}}

@article{mannar-dunn-salt,
	author = {Mannar, M. G. Venkatesh and Dunn, John Thornton},
	date-added = {2018-08-07 16:48:23 -0700},
	date-modified = {2018-08-07 16:50:02 -0700},
	journal = {International Council for Control of Iodine Deficiency Disorders},
	title = {Salt iodization for the elimination of iodine deficiency},
	year = {1995}}

@unpublished{adhvaryu_bednar_nyshadham_molina_nguyen_2018,
	author = {Adhvaryu, Achyuta and Bednar, Steven and Nyshadham, Anant and Molina, Teresa and Nguyen, Quynh},
	date-added = {2018-08-07 16:43:16 -0700},
	date-modified = {2018-08-07 16:43:50 -0700},
	doi = {10.3386/w24847},
	journal = {NBER Working Papers 24847},
	title = {When It Rains It Pours: The Long-run Economic Impacts of Salt Iodization in the United States},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3386/w24847}}

@article{feyrer_politi_weil_2017,
	author = {Feyrer, James and Politi, Dimitra and Weil, David},
	date-added = {2018-08-07 16:34:59 -0700},
	date-modified = {2018-08-07 16:34:59 -0700},
	doi = {10.1093/jeea/jvw002},
	journal = {Journal of the European Economic Association},
	month = {Apr},
	number = {2},
	pages = {355--387},
	title = {The Cognitive Effects of Micronutrient Deficiency: Evidence from Salt Iodization in the United States},
	volume = {15},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1093/jeea/jvw002}}

@article{mccarthy_fader_hardie_2017,
	author = {Mccarthy, Daniel and Fader, Peter and Hardie, Bruce},
	date-added = {2018-08-05 16:35:36 -0700},
	date-modified = {2018-08-05 16:35:36 -0700},
	doi = {10.1509/jm.15.0519},
	journal = {Journal of Marketing},
	month = {Jan},
	pages = {17--35},
	title = {Valuing Subscription-Based Businesses Using Publicly Disclosed Customer Data},
	volume = {81},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1509/jm.15.0519}}

@article{lum_isaac_2016,
	author = {Lum, Kristian and Isaac, William},
	date-added = {2018-08-05 15:47:18 -0700},
	date-modified = {2018-08-05 15:47:18 -0700},
	doi = {10.1111/j.1740-9713.2016.00960.x},
	journal = {Significance},
	number = {5},
	pages = {14--19},
	title = {To predict and serve?},
	volume = {13},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1111/j.1740-9713.2016.00960.x}}

@article{howard_zhang_horvitz_2017,
	author = {Howard, Ayanna and Zhang, Cha and Horvitz, Eric},
	date-added = {2018-08-05 15:46:29 -0700},
	date-modified = {2018-08-05 15:46:29 -0700},
	doi = {10.1109/arso.2017.8025197},
	journal = {2017 IEEE Workshop on Advanced Robotics and its Social Impacts (ARSO)},
	title = {Addressing bias in machine learning algorithms: A pilot study on emotion recognition for intelligent systems},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/arso.2017.8025197}}

@article{horvitz_mulligan_2015,
	author = {Horvitz, E. and Mulligan, D.},
	date-added = {2018-08-05 15:41:56 -0700},
	date-modified = {2018-08-05 15:41:56 -0700},
	doi = {10.1126/science.aac4520},
	journal = {Science},
	number = {6245},
	pages = {253--255},
	title = {Data, privacy, and the greater good},
	volume = {349},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1126/science.aac4520}}

@article{policing-ensign,
	author = {Danielle Ensign and Sorelle A Friedler and Scott Neville and Carlos Scheidegger and Suresh Venkatasubramanian},
	date-added = {2018-08-05 15:33:11 -0700},
	date-modified = {2018-08-05 15:33:50 -0700},
	journal = {arXiv preprint arXiv:1706.09847},
	title = {Runaway feedback loops in predictive policing},
	year = {2017}}

@article{friedler-fairness,
	author = {Sorelle A Friedler and Carlos Scheidegger and Suresh Venkatasubramanian},
	date-added = {2018-08-05 15:30:56 -0700},
	date-modified = {2018-08-05 15:32:27 -0700},
	journal = {arXiv preprint arXiv:1609.07236},
	title = {On the (im) possibility of fairness},
	year = {2016}}

@article{adler_falk_friedler_rybeck_scheidegger_smith_venkatasubramanian_nix_2018,
	author = {Adler, Philip and Falk, Casey and Friedler, Sorelle A. and Rybeck, Gabriel and Scheidegger, Carlos and Smith, Brandon and Venkatasubramanian, Suresh and Nix, Tionney},
	date-added = {2018-08-05 15:29:43 -0700},
	date-modified = {2018-08-05 15:29:43 -0700},
	doi = {10.1109/icdm.2016.0011},
	journal = {Knowledge and Information Systems},
	number = {1},
	pages = {95--122},
	title = {Auditing Black-Box Models for Indirect Influence},
	volume = {54},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/icdm.2016.0011}}

@article{sapiezynski_kassarnig_wilson_2017,
	author = {Sapiezynski, Piotr and Kassarnig, Valentin and Wilson, Christo},
	date-added = {2018-08-05 15:25:24 -0700},
	date-modified = {2018-08-05 15:25:24 -0700},
	doi = {10.18122/B20Q5R},
	journal = {Proceedings of FATREC Workshop on Responsible Recommendation at ACM RecSys, Como, Italy (FATREC'17)},
	month = {Aug},
	title = {Academic performance prediction in a gender-imbalanced environment},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.18122/B20Q5R}}

@article{gebru-buolamwini,
	author = {Buolamwini, Joy and Gebru, Timnit},
	date-added = {2018-08-05 15:06:47 -0700},
	date-modified = {2018-08-05 15:10:49 -0700},
	journal = {Conference on Fairness, Accountability, and Transparency},
	pages = {1-15},
	title = {Gender Shades: Intersectional Accuracy Disparities in Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
	volume = {81},
	year = {2018}}

@article{wong_mulligan_wyk_pierce_chuang_2017,
	author = {Wong, Richmond Y. and Mulligan, Deirdre K. and Wyk, Ellen Van and Pierce, James and Chuang, John},
	date-added = {2018-08-05 14:54:34 -0700},
	date-modified = {2018-08-05 14:54:34 -0700},
	doi = {10.1145/3134746},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	month = {Jun},
	number = {CSCW},
	pages = {1--26},
	title = {Eliciting Values Reflections by Engaging Privacy Futures Using Design Workbooks},
	volume = {1},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1145/3134746}}

@article{mulligan_koopman_doty_2016,
	author = {Mulligan, Deirdre K. and Koopman, Colin and Doty, Nick},
	date-added = {2018-08-05 14:45:38 -0700},
	date-modified = {2018-08-05 14:45:38 -0700},
	doi = {10.1098/rsta.2016.0118},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	number = {2083},
	pages = {20160118},
	title = {Privacy is an essentially contested concept: a multi-dimensional analytic for mapping privacy},
	volume = {374},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1098/rsta.2016.0118}}

@article{nair_misra_hornbuckle_mishra_acharya_2013,
	author = {Nair, Harikesh and Misra, Sanjog and Hornbuckle, William J. and Mishra, Ranjan and Acharya, Anand},
	date-added = {2018-05-11 22:38:48 +0000},
	date-modified = {2018-05-18 05:49:29 +0000},
	doi = {10.1287/mksc.2017.1039},
	journal = {Marketing Science},
	number = {5},
	pages = {699-725},
	title = {Big Data and Marketing Analytics in Gaming: Combining Empirical Models and Field Experimentation},
	volume = {36},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.2139/ssrn.2399676}}

@article{athey_imbens_2016,
	author = {Athey, Susan and Imbens, Guido},
	date-added = {2018-05-11 22:15:02 +0000},
	date-modified = {2018-05-11 22:15:02 +0000},
	doi = {10.1073/pnas.1510489113},
	journal = {Proceedings of the National Academy of Sciences},
	month = {May},
	number = {27},
	pages = {7353--7360},
	title = {Recursive partitioning for heterogeneous causal effects: Table 1.},
	volume = {113},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1073/pnas.1510489113}}

@article{hill-2012,
	author = {Jennifer L. Hill},
	date-added = {2018-05-03 23:13:59 +0000},
	date-modified = {2018-05-11 22:35:34 +0000},
	doi = {10.1198/jcgs.2010.08162},
	eprint = {https://doi.org/10.1198/jcgs.2010.08162},
	journal = {Journal of Computational and Graphical Statistics},
	number = {1},
	pages = {217-240},
	publisher = {Taylor & Francis},
	title = {Bayesian Nonparametric Modeling for Causal Inference},
	url = {https://doi.org/10.1198/jcgs.2010.08162},
	volume = {20},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1198/jcgs.2010.08162}}

@article{hill-carnegie-harada-2016,
	author = {Carnegie, Nicole Bohme and Harada, Masataka and Hill, Jennifer L.},
	date-added = {2018-05-03 23:11:07 +0000},
	date-modified = {2018-05-03 23:11:57 +0000},
	doi = {10.1080/19345747.2015.1078862},
	eprint = {https://doi.org/10.1080/19345747.2015.1078862},
	journal = {Journal of Research on Educational Effectiveness},
	number = {3},
	pages = {395-420},
	publisher = {Routledge},
	title = {Assessing Sensitivity to Unmeasured Confounding Using a Simulated Potential Confounder},
	url = {https://doi.org/10.1080/19345747.2015.1078862},
	volume = {9},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1080/19345747.2015.1078862}}

@article{VanderWeele-Arah-2011,
	abstract = {Uncontrolled confounding in observational studies gives rise to biased effect estimates. Sensitivity analysis techniques can be useful in assessing the magnitude of these biases. In this paper, we use the potential outcomes framework to derive a general class of sensitivity-analysis formulas for outcomes, treatments, and measured and unmeasured confounding variables that may be categorical or continuous. We give results for additive, risk-ratio and odds-ratio scales. We show that these results encompass a number of more specific sensitivity-analysis methods in the statistics and epidemiology literature. The applicability, usefulness, and limits of the bias-adjustment formulas are discussed. We illustrate the sensitivity-analysis techniques that follow from our results by applying them to 3 different studies. The bias formulas are particularly simple and easy to use in settings in which the unmeasured confounding variable is binary with constant effect on the outcome across treatment levels.},
	author = {Tyler J. VanderWeele and Onyebuchi A. Arah},
	date-added = {2018-05-03 23:04:41 +0000},
	date-modified = {2018-05-03 23:04:54 +0000},
	issn = {10443983},
	journal = {Epidemiology},
	number = {1},
	pages = {42--52},
	publisher = {Lippincott Williams & Wilkins},
	title = {Bias Formulas for Sensitivity Analysis of Unmeasured Confounding for General Outcomes, Treatments, and Confounders},
	url = {http://www.jstor.org/stable/29764679},
	volume = {22},
	year = {2011},
	bdsk-url-1 = {http://www.jstor.org/stable/29764679}}

@article{mccandless_somers_2017,
	author = {McCandless, Lawrence C and Somers, Julian M},
	date-added = {2018-05-03 23:00:13 +0000},
	date-modified = {2018-05-03 23:30:35 +0000},
	doi = {10.1177/0962280217729844},
	journal = {Statistical Methods in Medical Research},
	month = {Jul},
	pages = {096228021772984},
	title = {Bayesian sensitivity analysis for unmeasured confounding in causal mediation analysis},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1177/0962280217729844}}

@article{McCandless-2017,
	abstract = {Bias from unmeasured confounding is a persistent concern in observational studies, and sensitivity analysis has been proposed as a solution. In the recent years, probabilistic sensitivity analysis using either Monte Carlo sensitivity analysis (MCSA) or Bayesian sensitivity analysis (BSA) has emerged as a practical analytic strategy when there are multiple bias parameters inputs. BSA uses Bayes theorem to formally combine evidence from the prior distribution and the data. In contrast, MCSA samples bias parameters directly from the prior distribution. Intuitively, one would think that BSA and MCSA ought to give similar results. Both methods use similar models and the same (prior) probability distributions for the bias parameters. In this paper, we illustrate the surprising finding that BSA and MCSA can give very different results. Specifically, we demonstrate that MCSA can give inaccurate uncertainty assessments (e.g. 95\% intervals) that do not reflect the data's influence on uncertainty about unmeasured confounding. Using a data example from epidemiology and simulation studies, we show that certain combinations of data and prior distributions can result in dramatic prior‐to‐posterior changes in uncertainty about the bias parameters. This occurs because the application of Bayes theorem in a non‐identifiable model can sometimes rule out certain patterns of unmeasured confounding that are not compatible with the data. Consequently, the MCSA approach may give 95\% intervals that are either too wide or too narrow and that do not have 95\% frequentist coverage probability. Based on our findings, we recommend that analysts use BSA for probabilistic sensitivity analysis. Copyright {\copyright} 2017 John Wiley \& Sons, Ltd.},
	author = {McCandless, Lawrence C. and Gustafson, Paul},
	date-added = {2018-05-03 22:41:39 +0000},
	date-modified = {2018-05-03 23:31:03 +0000},
	doi = {10.1002/sim.7298},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7298},
	journal = {Statistics in Medicine},
	keywords = {Bayesian analysis, causal inference, non‐identifiable model, sensitivity analysis},
	number = {18},
	pages = {2887-2901},
	title = {A comparison of Bayesian and Monte Carlo sensitivity analysis for unmeasured confounding},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7298},
	volume = {36},
	year = {2017},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7298},
	bdsk-url-2 = {https://doi.org/10.1002/sim.7298}}

@article{dorie_harada_carnegie_hill_2016,
	author = {Dorie, Vincent and Harada, Masataka and Carnegie, Nicole Bohme and Hill, Jennifer},
	date-added = {2018-05-03 22:37:27 +0000},
	date-modified = {2018-05-03 22:37:27 +0000},
	doi = {10.1002/sim.6973},
	journal = {Statistics in Medicine},
	month = {Mar},
	number = {20},
	pages = {3453--3470},
	title = {A flexible, interpretable framework for assessing sensitivity to unmeasured confounding},
	volume = {35},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1002/sim.6973}}

@article{athey-yelp-2018,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180107826A},
	archiveprefix = {arXiv},
	author = {{Athey}, S. and {Blei}, D. and {Donnelly}, R. and {Ruiz}, F. and {Schmidt}, T.},
	date-added = {2018-05-03 21:16:14 +0000},
	date-modified = {2018-05-03 21:16:49 +0000},
	eprint = {1801.07826},
	journal = {ArXiv e-prints},
	keywords = {Economics - Econometrics, Computer Science - Artificial Intelligence, Statistics - Applications, Statistics - Machine Learning},
	month = jan,
	title = {{Estimating Heterogeneous Consumer Preferences for Restaurants and Travel Time Using Mobile Location Data}},
	year = 2018}

@article{peters-2016,
	author = {Peters, Jonas and B{\"u}hlmann, Peter and Meinshausen, Nicolai},
	date-added = {2018-05-03 19:41:49 +0000},
	date-modified = {2018-05-03 22:14:19 +0000},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	pages = {947-1012},
	title = {Causal inference by using invariant prediction: identification and confidence intervals},
	volume = {78},
	year = {2016}}

@book{pearl_2009,
	author = {Pearl, Judea},
	date-added = {2018-05-03 19:05:54 +0000},
	date-modified = {2018-05-03 22:18:32 +0000},
	place = {Cambridge},
	publisher = {Cambridge University Press},
	title = {Causality: models, reasoning, and inference},
	year = {2009}}

@book{imbens_rubin_2015,
	author = {Imbens, Guido and Rubin, Donald B.},
	date-added = {2018-05-03 18:41:44 +0000},
	date-modified = {2018-05-03 18:41:44 +0000},
	place = {Cambridge},
	publisher = {Cambridge University Press},
	title = {Causal inference for statistics, social, and biomedical sciences: an introduction},
	year = {2015}}

@manual{gdata,
	author = {Gregory R. Warnes and Ben Bolker and Gregor Gorjanc and Gabor Grothendieck and Ales Korosec and Thomas Lumley and Don MacQueen and Arni Magnusson and Jim Rogers and {others}},
	date-added = {2018-01-07 20:48:14 +0000},
	date-modified = {2018-01-07 20:48:18 +0000},
	note = {R package version 2.18.0},
	title = {gdata: Various R Programming Tools for Data Manipulation},
	url = {https://CRAN.R-project.org/package=gdata},
	year = {2017},
	bdsk-url-1 = {https://CRAN.R-project.org/package=gdata}}

@book{ggplot2,
	author = {Hadley Wickham},
	date-added = {2018-01-07 20:47:15 +0000},
	date-modified = {2018-01-07 20:47:21 +0000},
	isbn = {978-0-387-98140-6},
	publisher = {Springer-Verlag New York},
	title = {ggplot2: Elegant Graphics for Data Analysis},
	url = {http://ggplot2.org},
	year = {2009},
	bdsk-url-1 = {http://ggplot2.org}}

@book{DMwR,
	author = {L. Torgo},
	date-added = {2018-01-07 20:46:18 +0000},
	date-modified = {2018-01-07 20:46:25 +0000},
	publisher = {Chapman and Hall/CRC},
	title = {Data Mining with R, learning with case studies},
	url = {http://www.dcc.fc.up.pt/~ltorgo/DataMiningWithR},
	year = {2010},
	bdsk-url-1 = {http://www.dcc.fc.up.pt/~ltorgo/DataMiningWithR}}

@book{r-lattice,
	address = {New York},
	author = {Deepayan Sarkar},
	date-added = {2018-01-07 20:45:31 +0000},
	date-modified = {2018-01-07 20:45:39 +0000},
	note = {ISBN 978-0-387-75968-5},
	publisher = {Springer},
	title = {Lattice: Multivariate Data Visualization with R},
	url = {http://lmdvr.r-forge.r-project.org},
	year = {2008},
	bdsk-url-1 = {http://lmdvr.r-forge.r-project.org}}

@manual{readability,
	address = {Buffalo, New York},
	author = {Tyler W. Rinker},
	date-added = {2018-01-07 20:44:31 +0000},
	date-modified = {2018-01-07 20:44:37 +0000},
	note = {version 0.1.1},
	organization = {University at Buffalo/SUNY},
	title = {{readability}: Tools to Calculate Readability Scores},
	url = {http://github.com/trinker/readability},
	year = {2017},
	bdsk-url-1 = {http://github.com/trinker/readability}}

@manual{r-matrix,
	author = {Douglas Bates and Martin Maechler},
	date-added = {2018-01-07 20:43:40 +0000},
	date-modified = {2018-01-07 20:43:47 +0000},
	note = {R package version 1.2-11},
	title = {Matrix: Sparse and Dense Matrix Classes and Methods},
	url = {https://CRAN.R-project.org/package=Matrix},
	year = {2017},
	bdsk-url-1 = {https://CRAN.R-project.org/package=Matrix}}

@manual{stargazer,
	address = {Cambridge, USA},
	author = {Marek Hlavac},
	date-added = {2018-01-07 20:41:01 +0000},
	date-modified = {2018-01-07 20:41:07 +0000},
	note = {R package version 5.2},
	organization = {Harvard University},
	title = {stargazer: Well-Formatted Regression and Summary Statistics Tables},
	url = {http://CRAN.R-project.org/package=stargazer},
	year = {2015},
	bdsk-url-1 = {http://CRAN.R-project.org/package=stargazer}}

@article{glmnet,
	author = {Jerome Friedman and Trevor Hastie and Robert Tibshirani},
	date-added = {2018-01-07 20:39:10 +0000},
	date-modified = {2018-01-07 20:39:19 +0000},
	journal = {Journal of Statistical Software},
	number = {1},
	pages = {1--22},
	title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
	url = {http://www.jstatsoft.org/v33/i01/},
	volume = {33},
	year = {2010},
	bdsk-url-1 = {http://www.jstatsoft.org/v33/i01/}}

@article{plyr,
	author = {Hadley Wickham},
	date-added = {2018-01-07 20:37:07 +0000},
	date-modified = {2018-01-07 20:37:52 +0000},
	journal = {Journal of Statistical Software},
	number = {1},
	pages = {1--29},
	title = {The Split-Apply-Combine Strategy for Data Analysis},
	url = {http://www.jstatsoft.org/v40/i01/},
	volume = {40},
	year = {2011},
	bdsk-url-1 = {http://www.jstatsoft.org/v40/i01/}}

@manual{foreach,
	author = {{Revolution Analytics} and Steve Weston},
	date-added = {2018-01-07 20:35:17 +0000},
	date-modified = {2018-01-11 20:21:53 +0000},
	note = {R package version 1.4.3},
	title = {foreach: Provides Foreach Looping Construct for R},
	url = {https://CRAN.R-project.org/package=foreach},
	year = {2015},
	bdsk-url-1 = {https://CRAN.R-project.org/package=foreach}}

@manual{r-webmining,
	author = {Mario Annau},
	date-added = {2018-01-07 20:33:49 +0000},
	date-modified = {2018-01-07 20:34:03 +0000},
	note = {R package version 1.3},
	title = {tm.plugin.webmining: Retrieve Structured, Textual Data from Various Web Sources},
	url = {https://CRAN.R-project.org/package=tm.plugin.webmining},
	year = {2015},
	bdsk-url-1 = {https://CRAN.R-project.org/package=tm.plugin.webmining}}

@manual{r-cite,
	address = {Vienna, Austria},
	author = {{R Core Team}},
	date-added = {2018-01-07 20:31:46 +0000},
	date-modified = {2018-01-07 20:32:07 +0000},
	organization = {R Foundation for Statistical Computing},
	title = {R: A Language and Environment for Statistical Computing},
	url = {https://www.R-project.org/},
	year = {2017},
	bdsk-url-1 = {https://www.R-project.org/}}

@url{usda,
	author = {{United States Department of Agriculture Economic Research Service}},
	date-added = {2018-01-06 01:52:41 +0000},
	date-modified = {2018-01-11 20:19:16 +0000},
	lastchecked = {2017},
	title = {2013 Rural-Urban Continuum Codes},
	url = {https://www.ers.usda.gov/data-products/rural-urban-continuum-codes.aspx},
	year = {2013},
	bdsk-url-1 = {https://www.ers.usda.gov/data-products/rural-urban-continuum-codes.aspx}}

@article{tibshirani-2014,
	author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert},
	date-added = {2018-01-06 01:49:11 +0000},
	date-modified = {2018-01-06 01:49:26 +0000},
	fjournal = {The Annals of Statistics},
	issn = {0090-5364},
	journal = {Ann. Statist.},
	mrclass = {62J05 (62F03 62J07)},
	mrnumber = {3210970},
	number = {2},
	pages = {413--468},
	title = {A significance test for the lasso},
	url = {https://doi.org/10.1214/13-AOS1175},
	volume = {42},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1214/13-AOS1175}}

@article{tibshirani-2016,
	author = {Tibshirani, Ryan J. and Taylor, Jonathan and Lockhart, Richard and Tibshirani, Robert},
	date-added = {2018-01-06 01:47:49 +0000},
	date-modified = {2018-01-06 01:48:05 +0000},
	fjournal = {Journal of the American Statistical Association},
	issn = {0162-1459},
	journal = {J. Amer. Statist. Assoc.},
	mrclass = {62H15 (62-04 62F25 62J07 62L10)},
	mrnumber = {3538689},
	mrreviewer = {Gabriela Ciuperca},
	number = {514},
	pages = {600--620},
	title = {Exact post-selection inference for sequential regression procedures},
	url = {https://doi.org/10.1080/01621459.2015.1108848},
	volume = {111},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.2015.1108848}}

@article{javanmard,
	author = {Javanmard, Adel and Montanari, Andrea},
	date-added = {2018-01-06 01:43:56 +0000},
	date-modified = {2018-01-06 01:44:11 +0000},
	fjournal = {Journal of Machine Learning Research (JMLR)},
	issn = {1532-4435},
	journal = {J. Mach. Learn. Res.},
	mrclass = {62J07 (62F12 62F25)},
	mrnumber = {3277152},
	mrreviewer = {Zaixing Li},
	pages = {2869--2909},
	title = {Confidence intervals and hypothesis testing for high-dimensional regression},
	volume = {15},
	year = {2014}}

@article{vandegeer,
	author = {van de Geer, Sara and B\"uhlmann, Peter and Ritov, Ya'acov and Dezeure, Ruben},
	date-added = {2018-01-06 01:42:14 +0000},
	date-modified = {2018-01-06 01:42:25 +0000},
	fjournal = {The Annals of Statistics},
	issn = {0090-5364},
	journal = {Ann. Statist.},
	mrclass = {62J07 (62F12 62F25 62J12)},
	mrnumber = {3224285},
	mrreviewer = {Hiroto Hyakutake},
	number = {3},
	pages = {1166--1202},
	title = {On asymptotically optimal confidence regions and tests for high-dimensional models},
	url = {https://doi.org/10.1214/14-AOS1221},
	volume = {42},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1214/14-AOS1221}}

@electronic{grammakov,
	author = {{grammakov (GitHub user)}},
	date-added = {2018-01-06 01:39:47 +0000},
	date-modified = {2018-01-11 20:19:45 +0000},
	lastchecked = {2017},
	title = {US Cities, Counties and States (data set)},
	url = {https://github.com/grammakov/USA-cities- and-states},
	year = {2014},
	bdsk-url-1 = {https://github.com/grammakov/USA-cities-%20and-states}}

@book{hastie,
	added-at = {2008-05-16T16:17:42.000+0200},
	address = {New York, NY, USA},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	biburl = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
	date-added = {2018-01-06 01:36:13 +0000},
	date-modified = {2018-01-06 01:37:36 +0000},
	edition = {2nd ed},
	interhash = {d585aea274f2b9b228fc1629bc273644},
	intrahash = {f58afc5c9793fcc8ad8389824e57984c},
	keywords = {ml statistics},
	publisher = {Springer New York Inc.},
	series = {Springer Series in Statistics},
	timestamp = {2008-05-16T16:17:43.000+0200},
	title = {The Elements of Statistical Learning},
	year = 2017}

@unpublished{dube-misra,
	author = {Dub{\'e}, J. and Misra, S},
	date-added = {2018-01-06 01:32:02 +0000},
	date-modified = {2018-01-06 01:34:48 +0000},
	month = {August},
	note = {https://ssrn.com/abstract=2992257},
	title = {Scalable Price Targeting},
	year = {2017}}

@article{chatterjee,
	author = {Chatterjee, A. and Lahiri, S. N.},
	date-added = {2018-01-06 01:31:07 +0000},
	date-modified = {2018-01-06 01:31:15 +0000},
	fjournal = {The Annals of Statistics},
	issn = {0090-5364},
	journal = {Ann. Statist.},
	mrclass = {62J07 (62E20 62G09)},
	mrnumber = {3113809},
	number = {3},
	pages = {1232--1259},
	title = {Rates of convergence of the adaptive {LASSO} estimators to the oracle distribution and higher order refinements by the bootstrap},
	url = {https://doi.org/10.1214/13-AOS1106},
	volume = {41},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1214/13-AOS1106}}

@url{bea-regions,
	author = {{Bureau of Economic Analysis, US Department of Commerce}},
	date-added = {2018-01-06 01:28:06 +0000},
	date-modified = {2018-01-11 20:20:10 +0000},
	lastchecked = {2017},
	title = {Component state list for BEA regions},
	url = {https://www.bea.gov/regional/methods.cfm},
	year = {2017},
	bdsk-url-1 = {https://www.bea.gov/regional/methods.cfm}}

@book{angrist_mostly_2008,
	added-at = {2014-11-20T12:13:56.000+0100},
	author = {Angrist, Joshua D. and Pischke, J{\"o}rn-Steffen},
	biburl = {https://www.bibsonomy.org/bibtex/2bad3c0e83619dbdc442142ce714e5529/rlipp},
	date-added = {2018-01-06 01:25:16 +0000},
	date-modified = {2018-01-06 02:12:18 +0000},
	interhash = {6d41194af2b0685c2adb94316d607d56},
	intrahash = {bad3c0e83619dbdc442142ce714e5529},
	isbn = {0691120358},
	keywords = {imported},
	publisher = {Princeton University Press},
	shorttitle = {Mostly Harmless Econometrics},
	timestamp = {2014-11-20T12:13:56.000+0100},
	title = {Mostly Harmless Econometrics: An Empiricist's Companion},
	year = 2008}

@article{adalasso2017,
	author = {Audrino, Francesco and Camponovo, Lorenzo},
	date-added = {2018-01-06 00:47:27 +0000},
	date-modified = {2018-01-06 01:18:05 +0000},
	journal = {Journal of Time Series Analysis},
	month = {11},
	title = {Oracle Properties, Bias Correction, and Bootstrap Inference for Adaptive Lasso for Time Series M-Estimators},
	year = {2017}}

@article{adalasso,
	author = {Zou, Hui},
	date-added = {2018-01-06 00:42:43 +0000},
	date-modified = {2018-01-06 00:47:37 +0000},
	fjournal = {Journal of the American Statistical Association},
	issn = {0162-1459},
	journal = {J. Amer. Statist. Assoc.},
	mrclass = {62F07 (62F12 62F35 62J02 62J12)},
	mrnumber = {2279469},
	mrreviewer = {Alexander G. Kukush},
	number = {476},
	pages = {1418--1429},
	title = {The adaptive lasso and its oracle properties},
	url = {https://doi.org/10.1198/016214506000000735},
	volume = {101},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1198/016214506000000735}}
