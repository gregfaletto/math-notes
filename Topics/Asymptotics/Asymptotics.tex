%\documentclass{article}
%
%\usepackage{fancyhdr}
%\usepackage{extramarks}
%\usepackage{amsmath}
%\usepackage{amsthm}
%\usepackage{amsfonts}
%\usepackage{tikz}
%\usepackage{enumerate}
%\usepackage{graphicx}
%\graphicspath{ {images/} }
%\usepackage[plain]{algorithm}
%\usepackage{algpseudocode}
%\usepackage[document]{ragged2e}
%\usepackage{textcomp}
%\usepackage{color}   %May be necessary if you want to color links
%\usepackage{import}
%\usepackage{hyperref}
%\hypersetup{
%    colorlinks=true, %set true if you want colored links
%    linktoc=all,     %set to all if you want both sections and subsections linked
%    linkcolor=black,  %choose some color if you want links to stand out
%}
%
%\usetikzlibrary{automata,positioning}
%
%%%%%%%
%%%%%%% Basic Document Settings
%%%%%%%
%
%\topmargin=-0.45in
%\evensidemargin=0in
%\oddsidemargin=0in
%\textwidth=6.5in
%\textheight=9.0in
%\headsep=0.25in
%\setlength{\parskip}{1em}
%
%\linespread{1.1}
%
%\pagestyle{fancy}
%\lhead{\hmwkAuthorName}
%\lfoot{\lastxmark}
%\cfoot{\thepage}
%
%\renewcommand\headrulewidth{0.4pt}
%\renewcommand\footrulewidth{0.4pt}
%
%\setlength\parindent{0pt}
%
%
%\newcommand{\hmwkTitle}{Math Review Notes---Asymptotics and Convergence}
%\newcommand{\hmwkAuthorName}{\textbf{G. Faletto} }
%
%%%%%%%
%%%%%%% Title Page
%%%%%%%
%
%\title{
%    \vspace{2in}
%    \textmd{\textbf{ \hmwkTitle}}\\
%}
%
%\author{Gregory Faletto}
%\date{}
%
%\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}
%
%%%%%%%
%%%%%%% Various Helper Commands
%%%%%%%
%
%%%%%%% Useful for algorithms
%\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}
%
%%%%%%% For derivatives
%\newcommand{\deriv}[2]{\frac{\mathrm{d} #1}{\mathrm{d} #2}}
%
%%%%%%% For partial derivatives
%\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
%
%%%%%%% Integral dx
%\newcommand{\dx}{\mathrm{d}x}
%
%%%%%%% Alias for the Solution section header
%\newcommand{\solution}{\textbf{\large Solution}}
%
%%%%%%% Probability commands: Expectation, Variance, Covariance, Bias
%\newcommand{\E}{\mathbb{E}}
%\newcommand{\Var}{\mathrm{Var}}
%\newcommand{\Cov}{\mathrm{Cov}}
%\newcommand{\Bias}{\mathrm{Bias}}
%\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
%\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
%\DeclareMathOperator{\Tr}{Tr}
%
%\theoremstyle{definition}
%\newtheorem{theorem}{Theorem}
%\theoremstyle{definition}
%\newtheorem{proposition}[theorem]{Proposition}
%\theoremstyle{definition}
%\newtheorem{lemma}[theorem]{Lemma}
%\theoremstyle{definition}
%\newtheorem{corollary}{Corollary}[theorem]
%\theoremstyle{definition}
%\newtheorem{definition}{Definition}[section]
%\newtheorem*{remark}{Remark}
%
%%%%%%% Tilde
%\newcommand{\textapprox}{\raisebox{0.5ex}{\texttildelow}}
%
%\begin{document}
%
%\maketitle
%
%\pagebreak
%
%\tableofcontents
%
%\
%
%\
%
%\begin{center}
%Last updated \today
%\end{center}
%
%
%
%\newpage
%
%%
%%
%%
%%
%%
%%
%%
%%
%%

%%% Chapter 8
\section{Asymptotics and Convergence}

These notes are based on my notes from chapter 8 of \textit{Time Series and Panel Data Econometrics} (1st edition) by M. Hashem Pesaran and coursework for Economics 613: Economic and Financial Time Series I at USC, as well as Math 505A at USC and chapter 7 from \textit{Probability and Random Processes} (Grimmet and Stirkazer) 3rd edition.

%%%% Chapter 8
%\subsection{Chapter 8: Asymptotic Theory}

\subsection{Preliminaries (5.9 and 7.1, Grimmett and Stirzaker)}
\label{asym.preliminaries}

\begin{definition} \textbf{Definition 7.1.4, Grimmett and Stirzaker.} If for all \(x \in [0, 1]\) the sequence \(\{f_n(x)\}\) of real numbers satisfies \(f_n(x) \to f(x)\) as \(n \to \infty\) then we say \(f_n \to f\) \textbf{pointwise.}
\end{definition} 

\begin{remark} In practice pointwise convergence is often not useful for functions because a sequence of functions may be continuous while its limit is not. For instance, consider \(\{f_n: f_n = x^n \ \forall x \in [0, 1]\}\). Then \(f_n\) is continuous for all \(n\) but

\[
\lim_{n \to \infty} f_n = \begin{cases}
0 & x \leq 1 \\
1 & x = 1
\end{cases}
\]

Instead, the following definition is often more useful. \end{remark}

\begin{definition} \textbf{(from class notes.)} We say that \textbf{ \(f_n\) \textbf{uniformly converges} to \(f\) on \([a, b]\)} if for every \(\epsilon > 0\) there exists \(N\) such that for every \(n > N\),

\[
\forall x \in [a, b] \ |f_n(x) - f(x)| < \epsilon
\]

\end{definition}

\begin{definition} \textbf{(Definition 7.1.5, Grimmett and Stirzaker.)} Let \(V\) be a collection of functions mapping \([0, 1]\) into \(\mathbb{R}\) and assume \(V\) is endowed with a function \(\lVert \cdot \rVert : V \to \mathbb{R}\) satisfying

\begin{enumerate}[(a)]

\item \(\lVert f \rVert \geq 0\) for all \(f \in V\)

\item \(\lVert f \rVert = 0\) if and only if \(f\) is the zero function (or equivalent to it)

\item \(\lVert af \rVert = |a| \cdot \lVert f \rVert \) for all \(a \in \mathbb{R}\), \(f \in V\)

\item \(\lVert f + g \rVert \leq \lVert f \rVert + \lVert g \rVert \) (triangle inequality)

\end{enumerate}

The function \(\lVert \cdot \rVert\) is called a \textbf{norm}. If \(\{f_n\}\) is a sequence of members of \(V\) then we say that \textbf{\(f_n \to f\) with respect to the norm \(\lVert \cdot \rVert\)} if \(\lVert f_n - f \rVert \to 0 \) as \(n \to \infty\).

\end{definition}

\begin{definition} \textbf{(Definition 7.16, Grimmett and Stirzaker.)} Let \(\epsilon > 0\) be prescribed, and define the distance between two functions \(g, h: [0, 1] \to \mathbb{R}\) by

\[
d_\epsilon(g,h) = \int_E dx
\]

where \(E = \{ u \in[0, 1] : |g(u) - h(u)| > \epsilon \}\). We say that \textbf{ \(f_n \to f\) in measure} if 

\[
d_\epsilon(f_n, f) \to 0 \text{ as } n \to \infty \text{ for all } \epsilon > 0
\]

\end{definition}

\begin{theorem} \textbf{Inversion Theorem (Theorem 5.9.2, Grimmett and Stirzaker).} Let \(X\) have distribution function \(F\) and characteristic function \(\phi\). Define \(\overline{F}: \mathbb{R} \to [0, 1]\) by

\[
\overline{F}(x) = \frac{1}{2} \big[ F(x) + \lim_{y \to x^-} F(y) \big]
\]

Then

\[
\overline{F}(b) - \overline{F}(a) = \lim_{N \to \infty} \int_{-N}^N \frac{\exp(-iat) - \exp(-ibt)}{2\pi i t} \cdot \phi(t) dt
\]

\end{theorem}

\begin{proof} See Kingman and Taylor (1966). \end{proof}

\begin{corollary} \textbf{Corollary 5.9.3.} Random variables \(X\) and \(Y\) have the same characteristic function if and only if they have the same distribution function.
\end{corollary}

\begin{proof} \textbf{Available in Grimmett and Stirzaker section 5.9, pp. 189 - 190.} \end{proof}

\begin{definition} \textbf{(Definition 5.9.4, Grimmett and Stirzaker.)} We say that the sequence \(F_1, F_2, \ldots\) of distribution functions \textbf{converges} to the distribution function \(F\) (written \(F_n \to F\)) if \(F(x) = \lim_{n\to \infty} F_n(x)\) at each point \(x\) where \(F\) is continuous.
\end{definition}

\begin{theorem}
\textbf{Continuity theorem (Thereom 5.9.5; in notes from Friday 10/26, Lecture 28).} Supose that \(F_1, F_2, \ldots\) is a sequence of distribution functions with corresponding characteristic functions \(\phi_1, \phi_2, \ldots\). 

\begin{enumerate}[(a)]

\item If \(F_n(x) \to F(x)\) for some distribution function \(F\) with characteristic function \(\phi\) (at \(x\) where \(F\) is continuous), then \(\phi_n(t) \to \phi(t)\) for all \(t\).

\item Conversely, if \(\phi(t) = \lim_{n \to \infty} \phi_n(t)\) exists and \(\phi(t)\) is continuous at \(t = 0\), then \(\phi\) is the characteristic function of some distribution function \(F\), and \(F_n \to F\). 

\end{enumerate}
\end{theorem}

\begin{proof} See Kingman and Taylor (1966). \end{proof}


%%% Section 8.6
\subsection{Inequalities (8.6 of Pesaran)}
\label{sec:tsch8.6}

\textbf{Inequalities}

\begin{itemize}

\item Probabilities

\begin{itemize}

\item \begin{lemma}\label{asym.markov} \textbf{Markov's Inequality (Grimmett and Stirzaker p. 311, 319) ):} For \(a > 0\),

\[
\Pr(|X| \geq a) \leq \frac{\E(|X|)}{a}
\]
\end{lemma}
\begin{proof} Note that \( a \cdot \boldsymbol{1}_{\{|X| \geq a\}} \leq |X|\), where \(\boldsymbol{1}\) is the indicator function. Dividing both sides by \(a\) and taking expectations yields the result. \end{proof}

\item \begin{theorem}\label{asym.cheby}\textbf{Chebyshev's Inequality:} (probability p. 319) Let \(X\) be an (integrable) random variable with finite expected value \(\mu\) and finite nonzero variance \(\sigma^2\). Then for any real number \(k > 0\)

\[
\Pr \big(\left| X - \mu \right| \geq k \sigma \big) \leq \frac{1}{k^2}
\] 
\end{theorem}

(Can be used to demonstrate consistency of estimators: if we can show that as \(T \to \infty\) \(\Var(X) = \sigma^2 \to 0\), then this implies \(\Pr \big(\left| X - \mu \right| \geq k \sigma \big) \to 0\) as \(T \to \infty\), showing consistency.)

\item \begin{theorem} \textbf{Chernoff} For \(x \geq 0\), \(a > 0\), \(\forall \ t > 0\),

\[
\Pr(X \geq a) = \Pr( e^{tx} \geq e^{ta} ) \leq \frac{\E(e^{tx})}{e^{ta}}
\]
\end{theorem} 
\end{itemize}

\item \textbf{Moments}

\begin{itemize}

\item \begin{theorem} \textbf{Cauchy-Schwarz.} (and \textbf{Bunyakovsky})

\[
\E(XY)^2 \leq \E(X^2) \E(Y^2)
\]
\end{theorem}

\item \textbf{Krylov}

\item \begin{theorem} \textbf{Jensen's} (Grimmett and Stirzaker p.181, 349) If \(u\) is convex and \(\E X < \infty\), 

\[
\E(u(X)) \geq u(\E(X))
\]
\end{theorem}

\item \begin{theorem} \textbf{Holder} (Grimmett and Stirzaker p. p. 143, 319) Generalization of Cauchy-Schwarz. For \(p, q > 1\) satisfying \(1/p + 1/q =1\) we have

\[
\E(|XY|) \leq (\E(|X^p|))^{1/p}(\E(|X^q|))^{1/q} 
\]
\end{theorem}

\item \begin{theorem} \textbf{Minkowski} (Grimmett and Stirzaker p. p. 143) For \(p \geq 1\),

\[
[\E(|X + Y|^p)]^{1/p} \leq (\E|X^p|)^{1/p} + (\E|Y^p|)^{1/p}
\]
\end{theorem}

\item Useful for showing lower order moments are finite (e.g. finite variance implies finite mean). \begin{lemma}\textbf{Lyapunov's Inequality (Grimmett and Stirzaker p. 143).}\label{asym.lyapunov} For \(0 < r \leq s < \infty\),

\[
\E(|X|^r)^{1/r} \leq \E(|X|^s)^{1/s} 
\]
\end{lemma}
\end{itemize}

\end{itemize}

\textbf{Monotone convergence theorem.}

\textbf{Dominated convergence theorem.}

%%% Section 8.2
\subsection{Modes of Convergence (7.2 of Grimmet and Strikazer, 8.2 and 8.4 of Pesaran)}

 Let \(\{X_n\} = \{X_1, X_2, \ldots\}\) and \(X\) be random variables defined on a probability space \((\Omega, \mathcal{F}, \mathbb{P})\).

%%%% Convergence in probability
\begin{definition} \textbf{Convergence in probability.} \(\{X_n\}\) is said to \textbf{converge in probability} to \(X\) if
\begin{itemize}

\item Grimmett and Strizaker definition:
\[
\lim_{n \to \infty} \Pr(|X_n -X| > \epsilon) = 0, \text{ for every } \epsilon > 0
\]

\item Pesaran definition:
\[
\lim_{n \to \infty} \Pr(|X_n -X| < \epsilon) = 1, \text{ for every } \epsilon > 0
\]

\end{itemize}

This mode of convergence is also often denoted by \(X_n \xrightarrow{p} X\) and when \(X\) is a fixed constant it is referred to as the \textbf{probability limit of \(X_n\)}, written as \(Plim(X_n) = x\), as \(n \to \infty\).

The above concept is readily extended to multivariate cases where \(\{ \boldsymbol{X}_n, n = 1, 2, \ldots \}\) denote \(m\)-dimensional vectors of random variables. Then the condition is

\[
\lim_{n \to \infty} \Pr(\lVert \boldsymbol{X}_n -\boldsymbol{X}\rVert  < \epsilon) = 1, \text{ for every } \epsilon > 0
\]

where \(\lVert \cdot \rVert \) denotes an appropriate norm (say \(\ell_2\)). Convergence in probability is often referred to as "weak convergence" (in contrast to convergence with probability 1, below).
\end{definition} 

%%%% Convergence with probability 1 or almost surely
\begin{definition}
\textbf{Convergence with probability 1 or almost surely.} The sequence of random variables \(\{X_n\}\) is said to \textbf{converge with probability 1} (or \textbf{almost surely}) to \(X\) if 

\begin{itemize}

\item (505A class notes definition)

\[
\Pr\big( \{\omega \in \Omega: \lim_{n \to \infty} X_n(\omega) = X(\omega) \} \big) = 1
\]

(Note: pointwise convergence can hardly ever be shown here and is not useful.)

\item Grimmett and Strikazer textbook definition:
\[
\Pr \big( \{\omega \in \Omega: X_n(\omega) \to X(\omega) \text{ as } n \to \infty \} \big) = 1
\]

\item Pesaran textbook definition:

\[
\Pr \bigg( \lim_{n \to \infty} X_n = X \bigg) = 1
\]

\end{itemize}

This is often written as \(X_n \xrightarrow{w.p.1} X\) or \(X_n \xrightarrow{a.s.} X\). An equivalent condition for convergence with probability 1 is given by

\[
\lim_{n \to \infty} \Pr( |X_m - X| < \epsilon, \text{ for all } m \geq =n) = 1, \text{ for every } \epsilon > 0
\]

which shows that convergence in probability is a special case of convergence with probability 1 (obtained by setting \(m = n\)). Convergence with probability 1 is stronger than convergence in probability and is often referred to as ``strong convergence." 

\end{definition}

%%%% Convergence in r-th mean
\begin{definition}
\textbf{Convergence in \(r\)-th mean.} \(X_n \to X\) \textbf{in \(r\)th mean} where \(r \geq 1\) if \(\E|X_n^r| < \infty\) for all \(n\) and

\[
\lim_{n \to \infty} \E(|X_n - X|^r) = 0
\]

Convergence in \(r\)th mean is often written \(X_n \xrightarrow{r} X\).
\end{definition}

%%%%%%% Convergence in Distribution
\begin{definition}
\textbf{Convergence in Distribution.} Let \(X_1, X_2, \ldots\) have distribution functions \(F_1(\cdot), F_2(\cdot), \ldots \) respectively. Then \(X_n\) is said to \textbf{converge in distribution to \(X\)} if

\[
\lim_{n \to \infty} \Pr(X_n \leq u) = \Pr(X \leq u)
\]

for all \(u\) at which \(F_X(x) = \Pr(X \leq x)\) is continuous. This can also be written

\[
\lim_{n \to \infty} F_n(u) = F(u)
\]

for all \(u\) at which \(F\) is continuous. Convergence in distribution is usually denoted by \(X_n \xrightarrow{d} X\), \(X_n \xrightarrow{L} X\), or \(F_n \implies F\). By the Continuity Theorem (section \ref{asym.preliminaries}), this is equivalent to

\[
\lim_{n \to \infty} \phi_{X_n}(t) = \phi_X(t), \ \ t \in \mathbb{R}
\]
\end{definition}

\begin{theorem}
\textbf{(Theorem 7.2.3, Grimmett and Stirzaker.)} The following implications hold:

\begin{itemize}

\item \( (X_n \xrightarrow{a.s.} X) \implies (X_n \xrightarrow{p} X) \)

\item \( (X_n \xrightarrow{r} X) \implies (X_n \xrightarrow{p} X) \text{ for any } r \geq 1 \)

\item \( (X_n \xrightarrow{p} X) \implies (X_n \xrightarrow{d} X) \)

\end{itemize}

Also, if \(r > s \geq 1\), then \((X_n \xrightarrow{r} X) \implies (X_n \xrightarrow{s} X)\). No other implications hold in general.
\end{theorem}

\begin{theorem}\label{asym.7.2.4}\textbf{Some exceptions (Theorem 7.2.4).}

\begin{itemize}

\item If \(X_n \xrightarrow{d} c\) where \(c\) is constant, then \(X_n \xrightarrow{p} c\).

\item If \(X_n \xrightarrow{p} X\) and \(\Pr(|X_n| \leq k) = 0\) for all \(n\) and some \(k\), then \(X_n \xrightarrow{r} X\) for all \(r \geq 1\).

\item If \(P_n(\epsilon) = \Pr(|X_n - X| > \epsilon)\) satisfies \(\sum_n P_n(\epsilon) < \infty\) for all \(\epsilon > 0\), then \(X_n \xrightarrow{a.s.} X\). 

\end{itemize}
\end{theorem}

\begin{proof}(Part (c).) Let \(A_n(\epsilon) = \{|X_n - X| > \epsilon\}\) (so that \(P_n(\epsilon) = \Pr[A_n(\epsilon))]\), and let \(B_m(\epsilon) = \bigcup_{n \geq m} A_n(\epsilon)\). Then

\[
\Pr(B_m(\epsilon)) \leq \sum_{n=m}^\infty \Pr(A_n(\epsilon))
\]
so \(\lim_{m \to \infty} \Pr(B_m(\epsilon)) = 0\) whenever \(\sum_n \Pr(A_n(\epsilon)) < \infty\). See also Lemma \ref{asym.7.2.10} part (b).\end{proof}

%%% Section 7.2
\subsection{More on convergence (7.2 of Grimmet and Strikazer)}

\textbf{Other theorems to include:} Fatou's Lemma, Fubini's Theorem, Kolmogorov's Maximal Inequality, Kolmogorov Three-Series Test, Linberg Feller Central Limit Theorem, \textbf{this and more at beginning of Mike's 505A qual solutions.}

\begin{definition} \textbf{Cauchy Convergence.} We say that the sequence \(\{X_n: n \geq 1\}\) of random variables on the probability space \((\Omega, \mathcal{F}, \mathbb{P})\) is \textbf{almost surely Cauchy convergent} if

\[
\Pr \big( \{\omega \in \Omega: X_m(\omega) - X_n(\omega) \to 0 \text{ as } m, n \to \infty \} \big) = 1
\]

That is, the set of points \(\omega\) of the sample space for which the real sequence \(\{X_n(\omega): n \geq 1\}\) is Cauchy convergent is an event having probability 1.
\end{definition}

\begin{lemma}\label{asym.7.2.6} \textbf{(Lemma 7.2.6 from Grimmett and Stirzaker)} 
\begin{enumerate}[(a)]
\item If \(r > s \geq 1\) and \(X_n \xrightarrow{r} X\), then \(X_n \xrightarrow{s} X\).
\item If \(X_n \xrightarrow{1} X\) then \(X_n \xrightarrow{p} X\). 
\end{enumerate}
The converse assertions fail in general.
\end{lemma}

\begin{proof}
\begin{enumerate}[(a)]
% 7.2.6(a) proof
\item
Using Lyapunov's Inequality (Lemma \ref{asym.lyapunov}), if \(r > s \geq 1\)

\[
\big[ \E(|X_n - X|^s) \big]^{1/s} \leq \big[ \E(|X_n - X|^r) \big]^{1/r}
\]
Therefore if \(X_n \xrightarrow{r} X\) (meaning \(\lim_{n \to \infty} \E(|X_n - X|^r) = 0\)), ( then  \(\lim_{n \to \infty} \E(|X_n - X|^s) = 0\), so \(X_n \xrightarrow{s} X\). We show the converse fails by counterexample:

\[
X_n = \begin{cases}
n & \text{with probability } n^{(-1/2)(r+s)} \\
0 & \text{with probability } 1 -  n^{(-1/2)(r+s)}
\end{cases}
\]

Then \(\E|X_n^s| = n^{(1/2)(s-r)} \to 0\) and \(\E|X_n^r| = n^{(1/2)(r-s)} \to \infty\).

% 7.2.6(b) proof
\item By Markov's Inequality (Lemma \ref{asym.markov}),

\[
\Pr(|X_n - X| > \epsilon) \leq \frac{\E|X_n - X|}{\epsilon} \ \ \text{ for all } \epsilon > 0
\]

Therefore if \(X_n \xrightarrow{1} X\); that is, \(\lim_{n \to \infty} \E(|X_n - X|) = 0\), then \(\lim_{n \to \infty} \Pr(|X_n -X| > \epsilon) = 0\) for every \(\epsilon > 0\), so \(X_n \xrightarrow{p} X\).

To see the converse fails, define an independent sequence \(\{X_n\}\) by 

\[
X_n = \begin{cases}
n^3 & \text{with probability } n^{-2} \\
0 & \text{with probability } 1 - n^{-2}
\end{cases}
\]

Then \(\Pr(|X| > \epsilon) = n^{-2}\) for all large \(n\), and so \(X_n \xrightarrow{p} 0\). However, \(\E|X_n| = n \to \infty\).
\end{enumerate}
\end{proof}

\begin{lemma}\label{asym.7.2.10}
\textbf{(Lemma 7.2.10, Grimmett and Stirzaker.)} Let \(A_n(\epsilon) = \{|X_n - X| > \epsilon \}\) and \(B_m(\epsilon) = \cup_{n \geq m} A_n(\epsilon)\). Then:

\begin{enumerate}[(a)]

\item \(X_n \xrightarrow{a.s.} X\) if and only if \(\Pr(B_m(\epsilon)) \to 0\) as \(m \to \infty\) for all \(\epsilon > 0\).

\item \(X_n \xrightarrow{a.s.} X\) if \(\sum_n \Pr(A_n(\epsilon)) < \infty\) for all \(\epsilon > 0\).

\item If \(X_n \xrightarrow{a.s.} X\) then \(X_n \xrightarrow{p} X\), but the converse fails in general.

\end{enumerate}
\end{lemma}
\begin{proof}\begin{enumerate}[(a)]

\item

\item As for Theorem \ref{asym.7.2.4} part (c).

\item To see the converse fails, define an independent sequence \(\{X_n\}\) by

\[
X_n = \begin{cases}
1 & \text{with probability } n^{-1} \\
0 & \text{with probability } 1 - n^{-1}
\end{cases}
\]

Clearly \(X_n \xrightarrow{p} 0\). However, if \(0 < \epsilon < 1\),

\[
\Pr(B_m(\epsilon)) = 1 - \lim_{r \to \infty} \Pr(X_n = 0 \text{ for all } n \text{ such that } m \leq n \leq r) \text{ (by Lemma 1.3.5)}
\]

\[
=1 - \bigg( 1 - \frac{1}{m} \bigg)\bigg(1 - \frac{1}{m+1} \bigg) \cdots \text{ (by independence)}
\]

\[
= 1 - \lim_{M \to \infty}\bigg( \frac{m-1}{m} \cdot \frac{m}{m+1} \cdot \frac{m+1}{m+2} \cdots \frac{M}{M +1} \bigg)
\]

\[
= 1 - \lim_{M \to \infty} \frac{m-1}{M +1} = 1
\]

and so \(\{X_n\}\) does not converge almost surely.

\end{enumerate}\end{proof}

\begin{lemma}\textbf{(Lemma 7.2.12, Grimmett and Stirzaker.)} There exist sequences which

\begin{enumerate}[(a)]
\item converge almost surely but not in mean,
\item converge in mean but not almost surely.
\end{enumerate}
\end{lemma}
\begin{proof}
\begin{enumerate}[(a)]

\item As for Lemma \ref{asym.7.2.6} part (b).
\end{enumerate}
\end{proof}

\begin{theorem} \textbf{(Theorem 7.2.13, Grimmett and Stirzaker.)} If \(X_n \xrightarrow{p} X\), there exists a non-random increasing sequence of integers \(n_1, n_2, \ldots\) such that \(X_{n_i} \xrightarrow{a.s.} X\) as \(i \to \infty\). \end{theorem}

\begin{theorem}\textbf{Skorokhod's representation theorem (Theorem 7.2.14, Grimmett and Stirzaker).} If \(\{X_n\}\) and \(X\) with distribution functions \(\{F_n\}\) and \(F\) are such that \(X_n \xrightarrow{d} X\) (or equivalently, \(F_n \to F\)) as \(n \to \infty\), then there exists a probability space \((\Omega', \mathcal{F}', \mathbb{P}')\) and random variables \(\{Y_n\}\) and \(Y\) mapping \(\Omega'\) into \(\mathbb{R}\) such that

\begin{enumerate}[(a)]

\item \(\{Y_n\}\) and \(Y\) have distribution functions \(\{F_n\}\) and \(F\)

\item \(Y_n \xrightarrow{a.s.} Y\) as \(n \to \infty\)

\end{enumerate}

Therefore, although \(X_n\) may fail to converge to \(X\) in any mode other than in distribution, there exists a sequence \(\{Y_n\}\) such that \(Y_n\) is distributed identically to \(X_n\) for every \(n\), which converges almost surely to a copy of \(X\).
\end{theorem}

\begin{theorem}\textbf{(Theorem 7.2.18, Grimmett and Stirzaker.)} If \(X_n \xrightarrow{d} X\) and \(g: \mathbb{R} \to \mathbb{R}\) is continuous, then \(g(X_n) \xrightarrow{d} g(X)\). \end{theorem}

\begin{theorem}\textbf{(Theorem 7.2.19, Grimmett and Stirzaker; same as Portmanteau Theorem?)} The following three statements are equivalent:

\begin{enumerate}[(a)]

\item \(X_n \xrightarrow{d} X\)

\item \(\E[g(X_n)] \to \E[g(X)]\) for all bounded continuous functions \(g\).

\item \(\E[g(X_n)] \to \E[g(X)]\) for all functions \(g\) of the form \(g(x) = f(x) \boldsymbol{1}_{[a, b]}(x)\) where \(f\) is continuous on \([a, b]\) and \(a\) and \(b\) are points of continuity of the distribution function of the random variable \(X\). 

\end{enumerate}
\end{theorem}

\begin{theorem}\textbf{(Grimmett and Stirzaker Theorem 7.3.9.)} 
\begin{enumerate}[(a)]

\item If \(X_n \xrightarrow{a.s.} X\) and \(Y_n \xrightarrow{a.s.} Y\) then \(X_n + Y_n \xrightarrow{a.s.} X + Y\).

\item If \(X_n \xrightarrow{r} X\) and \(Y_n \xrightarrow{r} Y\) then \(X_n + Y_n \xrightarrow{r} X + Y\).

\item If \(X_n \xrightarrow{p} X\) and \(Y_n \xrightarrow{p} Y\) then \(X_n + Y_n \xrightarrow{p} X + Y\).

\item It is not in general true that  \(X_n + Y_n \xrightarrow{d} X + Y\) whenever  \(X_n \xrightarrow{d} X\) and \(Y_n \xrightarrow{d} Y\).

\end{enumerate}
\end{theorem}

%%% Borel-Cantelli lemmas 
\begin{theorem}\textbf{Borel-Cantelli lemmas (Grimmett and Stirzaker Theorem 7.3.10.)} Let \(\{A_n\}\) be an infinite sequence of events from some probability space \((\Omega, \mathcal{F}, \mathbb{P})\). Let \(A = \bigcap_n \bigcup_{m=n}^\infty A_m\ = \limsup_{n \to \infty} A_n = \{A_n \text{ i.o.}\}\) be the event that infinitely many of the \(A_n\) occur. Then:

\begin{enumerate}[(a)]

\item \(\Pr(A) = 0 \) if \(\sum_n \Pr(A_n) < \infty\)

\item \(\Pr(A) = 1\) if \(\sum_n \Pr(A_n) = \infty\) and \(A_1, A_2, \ldots\) are independent events.

\end{enumerate}
\end{theorem}

\begin{proof}
\begin{enumerate}[(a)]

\item We have that \(A \subseteq \bigcup_{m=n}^\infty A_n\) for all \(n\), so

\[
\Pr(A) \leq \sum_{m=n}^\infty \Pr(A_m) \to 0 \text{ as } n \to \infty
\]

whenever \(\sum_n \Pr(A_n) < \infty\).

\item One can confirm that 

\[
A^c = \bigcup_n \bigcap_{m=n}^\infty A_m^c
\]

But 

\[
\Pr \bigg( \bigcap_{m=n}^\infty A_m^c \bigg) = \lim_{r \to \infty}\Pr \bigg( \bigcap_{m=n}^r A_m^c \bigg) = \prod_{m=n}^\infty[ 1 - \Pr(A_m)] \text{ (by independence) } \leq  \prod_{m=n}^\infty \exp(- \Pr(A_m)) 
\]

\[
= \exp \bigg(- \sum_{m=n}^\infty \Pr(A_m) \bigg) = 0
\]

whenever \(\sum_n \Pr(A_n) = \infty\), where the fourth step follows since \(1 - x \leq e^{-x}\) if \(x \geq 0\). Thus

\[
\Pr(A^c) = \lim_{n \to \infty} \Pr \bigg( \bigcap_{m=n}^\infty A_m^c \bigg) = 0
\]

so \(\Pr(A) = 1\).

\end{enumerate}
\end{proof}

\begin{theorem}\label{asym:k2st}\textbf{Kolmogorov's Two-Series Theorem.} Let \(X_1, X_2, \ldots\) be independent random variables with \(\E(X_n) = \mu_n\) and \(\Var(X_n) = \sigma_n^2\) such that \(\sum_{n=1}^\infty \mu_n < \infty\) and \(\sum_{n=1}^\infty \sigma_n^2 < \infty\). Then \(\sum_{n=1}^\infty X_n\) converges in \(\mathbb{R}\) almost surely.
\end{theorem}

\begin{proof}Available on wikipedia, \url{https://en.wikipedia.org/wiki/Kolmogorov\%27s_two-series_theorem}.\end{proof}

%%% Slutsky's convergence theorems
\subsubsection{Slutsky's Convergence Theorems (8.4.1 of Pesaran, 7.3 of Grimmett and Stirzaker)}

\begin{theorem}\textbf{Theorem 6 of Pesaran, Section 8.4.1, p. 173.} Let \( \{x_t, y_t\}, t = 1, 2, \ldots\) be a sequence of pairs of random variables with \(y_t \xrightarrow{d} y\) and \(\left| y_t - x_t \right| \xrightarrow{p}  0\). Then \(x_t \xrightarrow{d} y\). \end{theorem}

\begin{theorem}\label{asym.slutsky} \textbf{Theorem 7 in Pesaran, on p.318 (section 7.3) of Grimmett and Stirzaker.} (Section 8.4.1, p. 174)  If \(x_t \xrightarrow{d} x\) and \(y_t \xrightarrow{p} c\) where \(c\) is a finite constant, then

\begin{enumerate}[(i)]

\item \(x_t + y_t \xrightarrow{d} x + c\)

\item \(y_tx_t \xrightarrow{d} cx\)

\item \(x_t/y_t \xrightarrow{d}  x/c, \text{ if } c \neq 0\).

\end{enumerate} \end{theorem}

\begin{theorem} \textbf{on p.318 (section 7.3) of Grimmett and Stirzaker.} Suppose that \(X_n \xrightarrow{d} 0\) and \(Y_n \xrightarrow{p} Y\), and let \(g: \mathbb{R}^2 \to \mathbb{R}\) be such that \(g(x,y)\) is a continuous function of \(y\) for all \(x\), and \(g(x, y)\) is continuous at \(x=0\) for all \(y\). Then \(g(X_n, Y_n) \xrightarrow{p} g(0, Y)\). \end{theorem}

 \textbf{Theorem 8.} (Grimmett and Stirzaker Section 8.4.1, p. 175) 

\begin{theorem} \textbf{Theorem 9 of Pesaran, Section 8.4.1, p. 176: convergence properties of transformed sequences.} Suppose \(\{\boldsymbol{x}_t\}\), \(\{\boldsymbol{y}_t\}\), \(\boldsymbol{x}\), and \(\boldsymbol{y}\) are \(m \times 1\) vectors of random variables on a probability space, and let \(\boldsymbol{g}(\cdot)\) be a continuous vector-valued function. Then

\begin{enumerate}[(i)]

\item \(\boldsymbol{x}_t \xrightarrow{a.s.} x \implies \boldsymbol{g}(\boldsymbol{x}_t) \xrightarrow{a.s.} \boldsymbol{g}(\boldsymbol{x})\)

\item \(\boldsymbol{x}_t \xrightarrow{p} x \implies \boldsymbol{g}(\boldsymbol{x}_t) \xrightarrow{p} \boldsymbol{g}(\boldsymbol{x})\)

\item \(\boldsymbol{x}_t \xrightarrow{d} x \implies \boldsymbol{g}(\boldsymbol{x}_t) \xrightarrow{d} \boldsymbol{g}(\boldsymbol{x})\)

\item \(\boldsymbol{x}_t  - \boldsymbol{y}_t \xrightarrow{p} \boldsymbol{0} \text{ and } \boldsymbol{y}_t \xrightarrow{d} \boldsymbol{y} \implies \boldsymbol{g}(\boldsymbol{x}_t) - \boldsymbol{g}(\boldsymbol{y}_t) \xrightarrow{d} \boldsymbol{0}(\boldsymbol{x})\)

\end{enumerate}

\end{theorem}

\begin{proof}See Serfling (1980) or Rao (1973).\end{proof}


%%% Section 8.5
\subsection{Stochastic orders \(\mathcal{O}_p(\cdot)\) and \(o_p(\cdot)\) (Pesaran 8.5)}

\begin{definition}\textbf{(Pesaran 8.5 Definition 6.)} Let \(\{a_t\}\) be a sequence of positive numbers and \(\{x_t\}\) be a sequence of random variables. Then

\begin{enumerate}[(i)]

\item \(x_t = \mathcal{O}_p(a_t)\), or \(x_t/a_t\) is bounded in probability, if for every \(\epsilon > 0\) there exist real numbers \(M_\epsilon\) and \(N_\epsilon\) such that

\[
\Pr \bigg( \frac{|x_t|}{a_t} > M_\epsilon \bigg) < \epsilon, \ \ \ \text{for } t > N_\epsilon
\]

\item \(x_t = o_p(a_t)\) if

\[
\frac{\boldsymbol{x}_t}{a_t} \xrightarrow{p} 0
\]

\end{enumerate}
\end{definition}

%%% Section 8.6
\subsection{Law of Large Numbers (8.6 of Pesaran)}


\begin{theorem} \textbf{(Khinchine) (Pesaran 8.6 Theorem 10.)} Suppose that \(\{x_t\}\) is a sequence of IID random variables with constant mean, i.e., \(\E(x_t) = \mu < \infty\). Then

\[
\overline{x}_T = \frac{1}{T} \sum_{t=1}^T x_t \xrightarrow{p} \mu
\]

\end{theorem}

\begin{theorem}\textbf{Weak Law of Large Numbers (Chebyshev) (Pesaran Section 8.6, p. 178, Theorem 11.)} Let \(\E(x_t) = \mu_t\), \(\Var(x_t) = \sigma_t^2\), and \(\Cov(x_t, x_t) = 0, \ t \neq s\). Then if 

\[
\lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T \sigma_t^2 < \infty
\]

we have \(\overline{x}_T - \overline{\mu}_T \xrightarrow{p} 0\), where \( \overline{\mu}_T = T^{-1} \sum_{t=1}^T \mu_t\).\end{theorem}

\begin{theorem}\textbf{Strong Law of Large Numbers 1 (Kolmogorov) (Pesaran 8.8 Theorem 12).} Let \(\{x_t\}\) be a sequence of independent random variables with \(\E(x_t) = \mu_t < \infty\) and \(\Var(x_t) =\sigma_t^2\) such that

\[
\sum_{t=1}^\infty \frac{\sigma_t^2}{t^2} < \infty
\]

Then \(\overline{x}_T - \overline{\mu}_T \xrightarrow{wp1} 0 \). If the independence assumption is replaced by a lack of correlation (i.e. \(\Cov(x_t, x_s) = 0, t \neq s\)), the convergence of \(\overline{x}_T - \overline{\mu}_T\) with probability one requires the stronger condition

\[
\sum_{t=1}^\infty \frac{\sigma_t^2 (\log t)^2}{t^2} < \infty
\]
\end{theorem}

\begin{theorem}\textbf{Strong Law of Large Numbers 2 (Pesaran 8.8 Theorem 13)} Suppose that \(x_1, x_2, \ldots\) are independent random variables, and that \(\E(x_i) = 0, \E(x_i^4) \leq K \ \forall \ i\) where \(K\) is an arbitrary positive constant. Then \(\overline{x}_T\) converges to 0 with probability 1.
\end{theorem}

\begin{theorem}\textbf{Central Limit Theorem (Grimmett and Stirzaker theorem 5.10.4.)} Let \(X_1, X_2, \ldots\) be a sequence of independent identically distributed random variables with finite mean \(\mu\) and finite non-zero variance \(\sigma^2\), and let \(S_n = \sum_{i=1}^n X_i\). Then

\[
\frac{S_n - n \mu}{\sqrt{n \sigma^2}} \xrightarrow{d} \mathcal{N}(0,1)
\]
\end{theorem}

\begin{theorem}\textbf{(Grimmett and Stirzaker theorem 5.10.5.)} Let \(X_1, X_2, \ldots\) be independent random variables satisfying \(\E(X_j) = 0\), \(\Var(X_j) = \sigma_j^2\), \(\E|X_j^3| < \infty\) such that

\[
\lim_{n \to \infty} \frac{1}{\sigma(n)^3} \sum_{j=1}^n \E|X_j^3| = 0
\]

where \(\sigma(n)^2 = \Var\big( \sum_{j=1}^n X_j \big) = \sum_{j=1}^n \sigma_j^2\). Then 

\[
\frac{1}{\sigma(n)} \sum_{j=1}^n X_j \xrightarrow{d} \mathcal{N}(0,1)
\]
\end{theorem}
\begin{proof}See Loeve (1977, p. 287) and Grimmett and Stirzaker Problem 5.12.40.\end{proof}

\begin{lemma}\textbf{Lindeberg's Condition:} Let \(\{X_k\}\) be a sequence of independent (not necessarily identically distributed) random variables with expectations \(\mu_k\) and finite variances \(\sigma_k^2\). Let \(s_n^2 = \sum_{k=1}^n \sigma_k^2\). If such a sequence of independent random variables \(X_k\) satisfies the condition

\[
\lim_{n \to \infty} \frac{1}{s_n^2} \sum_{k=1}^n \E \big[(X_k - \mu_k)^2) \cdot \boldsymbol{1}_{\{|X_k - \mu_k| > \epsilon s_n\}} \big] = 0
\]

for all \(\epsilon > 0\) then the central limit theorem holds; that is, the random variables

\[
Z_n = \frac{1}{s_n} \sum_{k=1}^n(X_k - \mu_k)
\]

converge in distrbution to \(\mathcal{N}(0, 1)\) as \(n \to \infty\).
\end{lemma}

%%% Section 8.8
\subsection{The case of dependent and heterogeneously distributed observations (Pesaran 8.8)}


\begin{theorem}\label{sec:tsch8.8} \textbf{Central limit theorem for martingale difference sequences (Pesaran 8.8 Theorem 28).} Let \(\{x_t\}\) be a martingale difference sequence with respect to the information set \(\Omega_t\). Let \(\overline{\sigma}_T^2 = \Var( \sqrt{T} \overline{x}_T) = T^{-1} \sum_{t=1}^T \sigma_t^2\). If \(\E(|x_t|^r) < K < \infty\), \(r > 2\) and for all \(t\), and

\[
\frac{1}{T} \sum_{t=1}^T x_t^2 - \overline{\sigma}_t^2 \xrightarrow{p} 0
\]

then \(\sqrt{T} \overline{x}_T / \overline{\sigma}_T \xrightarrow{d} \mathcal{N}(0, 1)\).
\end{theorem}

\subsection{Worked Examples from Math 505A Midterm 2}

\begin{enumerate}[(1)]

%%%%%%%%%%%% Midterm question 1 %%%%%%%%%%%%
\item 

%\textbf{Hints/Notes about this question:} Calculus limit (calculating limit). Law of large numbers/central limit theorem. We did this problem in class \textbf{(true for hw6 q1: went over part (a) on 10/31, p. 15 of notes)}. \textbf{Very likely: Homework 6 Question 1 (Fall 2010 qual, question 1)}.

%\textbf{Question:}

\begin{enumerate}[(a)]

\item Let \(X_k\), \(k \geq 1\), be i.i.d. random variables with mean 1 and variance 1. Show that the limit

\[
\lim_{n \to \infty} \frac{\sum_{k=1}^n X_k}{\sum_{k=1}^n X_k^2} 
\]

exists in an appropriate sense, and identify the limit.

\item Let \((X_j)_{j \geq 1}\) be i.i.d. uniform on \((-1, 1)\). Let 

\[
Y_n = \frac{\sum_{j=1}^n X_j}{\sum_{j=1}^n X_j^2 + \sum_{j=1}^n X_j^3} 
\]

Prove that \(\lim_{n \to \infty} \sqrt{n} Y_n\) exists in an appropriate sense, and identify the limit.

\end{enumerate}

\textbf{Solution.} \begin{enumerate}[(a)]

% 1a
\item 

%\textbf{on page 15 on notes, from 10/31}

\[
\lim_{n \to \infty} \frac{\sum_{k=1}^n X_k}{\sum_{k=1}^n X_k^2} = \lim_{n \to \infty} \frac{n^{-1}\sum_{k=1}^n X_k}{n^{-1}\sum_{k=1}^n X_k^2}
\]

Since \(X_1, X_2, \ldots\) are i.i.d., \(E(X_1^2) = \Var(X_1) + (\E(X_1))^2 = 2 < \infty\), we have \[ n^{-1}\sum_{k=1}^n X_k \xrightarrow{a.s.} \E(X_1) = 1 \text{ as } n \to \infty\] by Theorem 7.4.3 (Strong Law of Large Numbers). Also, \(X_1^2, X_2^2, \ldots\) are clearly identically distributed, and are independent by Theorem 4.2.3 (``If \(X\) and \(Y\) are independent, then so are \(g(X)\) and \(g(Y)\)."). It is clear also that \(\E(|X_1^2|) = \E(X_1^2) = \Var(X_1) + \E(X_1)^2 = 1 + 1 = 2 < \infty\). Therefore by the Strong Law of Large Numbers,  

\[
 n^{-1}\sum_{k=1}^n X_k^2 \xrightarrow{a.s.}\E(X_1^2) = 2 \text{ as } n \to \infty
 \]

(From here I had two different ways of finishing the problem.)

\begin{itemize}

\item \textbf{Approach suggested by Lototsky (possibly less rigorous?):}
 
 Because we have almost sure convergence in the numerator and denominator, the regular rules of calculus/real analysis apply. That is,
 
 \[
 \lim_{n \to \infty} \frac{n^{-1}\sum_{k=1}^n X_k}{n^{-1}\sum_{k=1}^n X_k^2} =  \frac{\lim_{n \to \infty} n^{-1}\sum_{k=1}^n X_k}{\lim_{n \to \infty} n^{-1}\sum_{k=1}^n X_k^2} \xrightarrow{a.s.} \boxed{\frac{1}{2}}
 \]

\item \textbf{By-the-book approach:}

Then, using one of  Slutsky's convergence theorems (Theorem \ref{asym.slutsky}: ``If \(x_t \xrightarrow{d} x\) and \(y_t \xrightarrow{p} c\) where \(c\) is a finite constant, then \(x_t/y_t \xrightarrow{d}  x/c, \text{ if } c \neq 0\)."), we have

%Then by Theorem 7.2.18, which states the following:
\[
\frac{n^{-1}\sum_{k=1}^n X_k}{n^{-1}\sum_{k=1}^n X_k^2} \xrightarrow{d} \frac{\E(X_1)}{\E(X_1^2)} =  \frac{\E(X_1)}{\Var(X_1) + \E(X_1)^2}  =  \frac{1}{1+1} = \frac{1}{2}
\]

But then, by Theorem \ref{asym.7.2.4} (Theorem 7.2.4(a) in Grimmett and Stirzaker: ``If \(X_n \xrightarrow{d} c\) where \(c\) is constant, then \(X_n \xrightarrow{p} c\)."), we have \(\frac{n^{-1}\sum_{k=1}^n X_k}{n^{-1}\sum_{k=1}^n X_k^2} \xrightarrow{p} 1/2\). 

\end{itemize}

% 1b
\item 

\[
Y_n = \frac{\sum_{j=1}^n X_j}{\sum_{j=1}^n X_j^2 + \sum_{j=1}^n X_j^3} =  \frac{n^{-1}\sum_{j=1}^n X_j}{n^{-1}\sum_{j=1}^n X_j^2 + n^{-1}\sum_{j=1}^n X_j^3} 
\]

Note that \(\E(X_1) = 0, \E(X_1^2) = \Var(X_1) + \E(X_1)^2 = (1 - -1)^2/12 + 0^2 = 1/3 , \E(X_1^3) = (1/2) \int_{-1}^1 x^3 dx = 0 \). (We derived the formulae for the first three moments of a uniform distribution on Homework 4 problem 2(2).) 

\[
\implies \sqrt{n} Y_n =  \frac{\sqrt{1/3} \big(\sum_{j=1}^n X_j - n\E(X_1) \big)/ \sqrt{n \cdot1/3} }{n^{-1}\sum_{j=1}^n X_j^2 + n^{-1}\sum_{j=1}^n X_j^3} 
\]
 
By the Central Limit Theorem,
 
\[
 \frac{\sum_{j=1}^n X_j - n\E(X_1) }{\sqrt{n \cdot1/3} } \xrightarrow{d} \mathcal{N}(0, 1)
\]

By the Law of Large Numbers, since \(\E(|X_1^2|) = \E(X_1^2) = 1/3 < \infty\),

\[
\frac{1}{n}\sum_{j=1}^n X_j^2 \xrightarrow{a.s.} \E(X_1^2) =   1/3
\]

By the Law of Large Numbers, since \(\E(|X_1^3|) = (1/2) \int_{-1}^1 |x^3| dx = \int_0^1 x^3 dx = 1/4 < \infty\),

\[
\frac{1}{n}\sum_{j=1}^n X_j^3 \xrightarrow{a.s.} \E(X_1^3) =   0
\]

In the denominator, since we have almost sure convergence, the regular rules of calculus/real analysis apply. That is, using the above results,

\[
n^{-1}\sum_{j=1}^n X_j^2 + n^{-1}\sum_{j=1}^n X_j^3 \xrightarrow{a.s.} 1/3
\]

Therefore

\[
\sqrt{n} Y_n =  \frac{\sqrt{1/3} \big(\sum_{j=1}^n X_j - n\E(X_1) \big)/ \sqrt{n \cdot1/3} }{n^{-1}\sum_{j=1}^n X_j^2 + n^{-1}\sum_{j=1}^n X_j^3} \xrightarrow{d} \frac{\sqrt{1/3} }{1/3} \mathcal{N}(0, 1) = \boxed{\mathcal{N}(0, 3)}
\]

\end{enumerate}


%%%%%%%%%%%% Midterm question 4 %%%%%%%%%%%%
\item 

%\textbf{Hints/Notes about this question:} Almost sure convergence. Computation. We did this problem in class \textbf{(true for hw6 q6: went over on 11/07, p. 22 of notes)}. \textbf{Very likely: Homework 6 Question 6(a) (Spring 2017 qual, question 3.}

%\textbf{Question:}

\begin{enumerate}[(a)]

\item Consider the sequence \(\{X_k, k \geq 1\}\) of random variables such that \(X_1\) is uniform on \((0, 1)\) and, given \(X_k\), the distribution of \(X_{k+1}\) is uniform on \((0, CX_k)\), where \(\sqrt{3} < C < 2\).

\begin{enumerate}[(i)]

\item Show that \(\lim_{x \to \infty} X_n = 0\) in \(\ell_1\) and with probability one, but not in \(\ell_2\). 

\item Investigate the same questions for all other values of \(C > 0\).

\end{enumerate}

\item Let \(a > 0\), let \(X_n, n \geq 1\) be i.i.d. random variables that are uniform on \((0, a)\), and let \(Y_n = \prod_{k=1}^n X_k\). Determine, with a proof, all values of \(a\) for which \(\lim_{n \to \infty} Y_n = 0\) with probability one.

\end{enumerate}

\textbf{Solution.} \begin{enumerate}[(a)]

% 6a
\item 

\begin{enumerate}[(i)]

% 6a(i)
\item We have that \(X_{n+1} \mid X_n \sim U(0, C X_n)\). Therefore

\[
\E(X_{n+1}^r \mid X_n) = \frac{1}{C X_n} \int_{0}^{CX_n} x^r dx = \frac{1}{CX_n} \cdot \frac{x^{r+1}}{r+1} \bigg|_0^{CX_n} = \frac{C^rX_n^r}{r+1}
\]

\[
\implies \E(X_{n+1}^r) = \E[\E(X_{n+1}^r \mid X_n)] = \frac{C^r}{r+1} \cdot \E( X_n^r)
\]

Note that \(E(X_1^r) = \int_0^1 x^r dr = 1/(r+1)\). Therefore 

\[
\E(X_{n+1}^r) = \frac{C^r}{r+1} \cdot \E( X_n^r) = \bigg( \frac{C^r}{r+1}\bigg)^n \cdot \E(X_1^r) = \bigg( \frac{C^r}{r+1}\bigg)^n \cdot \frac{1}{r+1}
\]

We would like to show that \(X_n \xrightarrow{w.p.1} 0\) and that \(X_n \xrightarrow{1} 0\), but that the same result does not follow for the \(\ell_2\) norm.

\begin{itemize}

\item \textbf{Convergence with probability one:} We seek to show that \(\Pr\big( \{\omega \in \Omega: \lim_{n \to \infty} X_n(\omega) = 0 \} \big) = 1\). By Markov's Inequality (Lemma \ref{asym.markov}), we have

\[
\Pr(|X_n| \geq a) \leq \frac{\E(X_n)}{a} \ \ \forall \ a > 0
\]

\[
\iff \Pr(|X_n| \geq a) \leq \bigg( \frac{C^1}{1+1}\bigg)^{n-1} \cdot \frac{1}{1+1} \cdot \frac{1}{a} =   \bigg( \frac{C}{2}\bigg)^{n-1} \cdot \frac{1}{2a}  \ \ \forall \ a > 0
\]
% = \frac{C^{n-1}}{2^na} 

Since \(\sqrt{3} < C < 2\), \(\sqrt{3}/2 < C/2 < 1\). Since \(X_n \in [0, C X_{n-1}]\), \(X_n \geq 0\), so \(|X_n| = X_n\). Therefore we have

\[
\Pr(\lim_{n \to \infty} |X_n| \geq a) = \Pr(\lim_{n \to \infty} X_n \geq a) \leq \lim_{n \to \infty}  \bigg( \frac{C}{2}\bigg)^{n-1} \cdot \frac{1}{2a} =  0 \ \ \forall \ a > 0
\]

Since \(|X_n| \geq 0\), this implies that \(\Pr(\lim_{n \to \infty} X_n = 0) = \Pr\big( \{\omega \in \Omega: \lim_{n \to \infty} X_n(\omega) = 0 \} \big) = 1\), so \(X_n\) converges to 0 with probability 1.

\item \textbf{Convergence in \(\ell_1\) norm:} We seek to show that \(\lim_{n \to \infty} \E(|X_n|) = 0\). Since \(X_n \in [0, C X_{n-1}]\), \(X_n \geq 0\), so \(|X_n| = X_n\). Therefore

\[
\lim_{n \to \infty} \E(|X_n|) = \lim_{n \to \infty} \E(X_n) = \lim_{n \to \infty} \bigg( \frac{C}{2}\bigg)^{n-1} \cdot \frac{1}{2}
\]

Since \(\sqrt{3} < C < 2\), \(\sqrt{3}/2 < C/2 < 1\), so \(C/2 < 1\). Therefore we have 

\[
\lim_{n \to \infty} \E(|X_n|)= \lim_{n \to \infty} \bigg( \frac{C}{2}\bigg)^{n-1} \cdot \frac{1}{2} = 0
\]

so \(X_n\) converges to 0 in 1st mean.

\item \textbf{Convergence in \(\ell_2\) norm:} We seek to show that \(\lim_{n \to \infty} \E(|X_n|^2) \neq 0\). We have

\[
\lim_{n \to \infty} \E(|X_n|^2) = \lim_{n \to \infty} \E(X_n^2) = \lim_{n \to \infty} \bigg( \frac{C^2}{3}\bigg)^{n-1} \cdot \frac{1}{3}
\]

Since \(\sqrt{3} < C < 2\), \(3/3 < C^2/3 < 4/3\), so \(C^2/3 > 1\). Therefore we have 

\[
\lim_{n \to \infty} \E(|X_n|^2) = \lim_{n \to \infty} \bigg( \frac{C^2}{3}\bigg)^{n-1} \cdot \frac{1}{3} = \infty \neq 0
\]

so \(X_n\) does not converge to 0 in 2nd mean.

\end{itemize} 

% 6a(ii)
\item From the above, it is clear that for convergence with probability one or in 1st mean we require \(0 < C/2 < 1\) and for convergence in second mean we require \(0 < C^2/3 < 1\).  For \(0 < C < \sqrt{3}\), we see that \(X_n\) would converge to zero in 2nd mean since this would imply that  \(0 < C^2/3 < 1\). It would also still converge to 0 in 1st mean (and with probability 1) since we would have \((0 < C/2 < \sqrt{3}/2 < 1\). 

For \(C = \sqrt{3}\), \(X_n\) would still converge to 0 with probability one and in 1st mean for the same reasons. However, it would not converge in 2nd mean because we would have

\[
\lim_{n \to \infty} \E(|X_n|^2) = \lim_{n \to \infty} \bigg( \frac{\sqrt{3}^2}{3}\bigg)^{n-1} \cdot \frac{1}{3} = \frac{1}{3} \neq 0
\]

For \(C \geq 2\), it would diverge in all three cases, since in this case \(C/2 \geq 2/2 = 1\) and \(C^2/3 \geq 4/3 > 1\).

\end{enumerate}

% 6b
\item \textbf{Probably won't be on midterm.} Note that 

\[
\lim_{n \to \infty} Y_n = \lim_{n \to \infty} \prod_{k=1}^n X_k = 0 \iff \log(Y_n) = \log \bigg( \prod_{k=1}^n X_k\bigg) = \sum_{k=1}^n \log (X_k) \to -\infty
\]

Note that

\[
\E[ \log(Y_n)] = \E \bigg(  \sum_{k=1}^n \log (X_k) \bigg) = \sum_{k=1}^n \E[\log(X_k)] = \sum_{k=1}^n \E[\log(X_1)] = \sum_{k=1}^n \int_0^a ( \log(x) /a) dx
\]

\[
=  \sum_{k=1}^n \frac{1}{a} \big[x \log x - x \big]_0^a =  \sum_{k=1}^n \frac{a \log a - a}{a} =  \sum_{k=1}^n ( \log(a) - 1) =n(\log(a) - 1)
\]

As \(n \to \infty\) we have

\[
\E[ \log(Y_n)]  = \begin{cases}
-\infty & a < e \\ 
0 & a = e \\ 
\infty & a > e 
\end{cases}
\]

Since \(\E[ \log(Y_n)] \to \infty\) for \(a < e\), we have \(\lim_{n \to \infty} Y_n = 0\) for \(a < 3\). Therefore \[\boxed{\lim_{n \to \infty}Y_n = \lim_{n \to \infty} \prod_{k=1}^n X_k = 0 \iff a < e.}\]

%The \(a = e\) case is the most interesting. 

\end{enumerate}

\end{enumerate}



%
%
%
%
%
%
%
%

%\end{document}
