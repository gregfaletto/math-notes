%\documentclass{article}
%
%\usepackage{fancyhdr}
%\usepackage{extramarks}
%\usepackage{amsmath}
%\usepackage{amsthm}
%\usepackage{amsfonts}
%\usepackage{tikz}
%\usepackage{enumerate}
%\usepackage{graphicx}
%\graphicspath{ {images/} }
%\usepackage[plain]{algorithm}
%\usepackage{algpseudocode}
%\usepackage[document]{ragged2e}
%\usepackage{textcomp}
%\usepackage{color}   %May be necessary if you want to color links
%\usepackage{import}
%\usepackage{hyperref}
%\hypersetup{
%    colorlinks=true, %set true if you want colored links
%    linktoc=all,     %set to all if you want both sections and subsections linked
%    linkcolor=black,  %choose some color if you want links to stand out
%}
%\usepackage{import}
%\usepackage{natbib}
%
%\usetikzlibrary{automata,positioning}
%
%%%%%%%
%%%%%%% Basic Document Settings
%%%%%%%
%
%\topmargin=-0.45in
%\evensidemargin=0in
%\oddsidemargin=0in
%\textwidth=6.5in
%\textheight=9.0in
%\headsep=0.25in
%\setlength{\parskip}{1em}
%
%\linespread{1.1}
%
%\pagestyle{fancy}
%\lhead{\hmwkAuthorName}
%\lfoot{\lastxmark}
%\cfoot{\thepage}
%
%\renewcommand\headrulewidth{0.4pt}
%\renewcommand\footrulewidth{0.4pt}
%
%\setlength\parindent{0pt}
%
%
%\newcommand{\hmwkTitle}{Math Review Notes---Asymptotics and Convergence}
%\newcommand{\hmwkAuthorName}{\textbf{G. Faletto} }
%
%%%%%%%
%%%%%%% Title Page
%%%%%%%
%
%\title{
%    \vspace{2in}
%    \textmd{\textbf{ \hmwkTitle}}\\
%}
%
%\author{Gregory Faletto}
%\date{}
%
%\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}
%
%%%%%%%
%%%%%%% Various Helper Commands
%%%%%%%
%
%%%%%%% Useful for algorithms
%\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}
%
%%%%%%% For derivatives
%\newcommand{\deriv}[2]{\frac{\mathrm{d} #1}{\mathrm{d} #2}}
%
%%%%%%% For partial derivatives
%\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
%
%%%%%%% Integral dx
%\newcommand{\dx}{\mathrm{d}x}
%
%%%%%%% Alias for the Solution section header
%\newcommand{\solution}{\textbf{\large Solution}}
%
%%%%%%% Probability commands: Expectation, Variance, Covariance, Bias
%\newcommand{\E}{\mathbb{E}}
%\newcommand{\Var}{\mathrm{Var}}
%\newcommand{\Cov}{\mathrm{Cov}}
%\newcommand{\Bias}{\mathrm{Bias}}
%\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
%\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
%\DeclareMathOperator{\Tr}{Tr}
%
%\theoremstyle{definition}
%\newtheorem{theorem}{Theorem}
%\theoremstyle{definition}
%\newtheorem{proposition}[theorem]{Proposition}
%\theoremstyle{definition}
%\newtheorem{lemma}[theorem]{Lemma}
%\theoremstyle{definition}
%\newtheorem{corollary}{Corollary}[theorem]
%\theoremstyle{definition}
%\newtheorem{definition}{Definition}[section]
%\newtheorem*{remark}{Remark}
%\theoremstyle{definition}
%\newtheorem{exercise}{Exercise}
%\theoremstyle{definition}
%\newtheorem{example}{Example}[section]
%
%%%%%%% Tilde
%\newcommand{\textapprox}{\raisebox{0.5ex}{\texttildelow}}
%
%\begin{document}
%
%\maketitle
%
%\pagebreak
%
%\tableofcontents
%
%\
%
%\
%
%\begin{center}
%Last updated \today
%\end{center}
%
%
%
%\newpage
%
%%
%%
%%
%%
%%
%%
%%
%%
%%
%
%%%% Chapter 8
\section{Asymptotics and Convergence}

These notes are based on my notes from chapter 8 of \textit{Time Series and Panel Data Econometrics} (1st edition) by M. Hashem Pesaran \citep{pesaran-2015-text} and coursework for Economics 613: Economic and Financial Time Series I at USC, as well as Math 505A and Math 541A at USC and chapter 7 from \textit{Probability and Random Processes} (Grimmett and Stirzaker) 3rd edition \citep{grimmett2001probability}.

%%%% Chapter 8
%\subsection{Chapter 8: Asymptotic Theory}

\subsection{Preliminaries (5.9 and 7.1, Grimmett and Stirzaker)}
\label{asym.preliminaries}

\begin{definition} \textbf{Definition 7.1.4, Grimmett and Stirzaker.} If for all \(x \in [0, 1]\) the sequence \(\{f_n(x)\}\) of real numbers satisfies \(f_n(x) \to f(x)\) as \(n \to \infty\) then we say \(f_n \to f\) \textbf{pointwise.}
\end{definition} 

\begin{remark} In practice pointwise convergence is often not useful for functions because a sequence of functions may be continuous while its limit is not. For instance, consider \(\{f_n: f_n = x^n \ \forall x \in [0, 1]\}\). Then \(f_n\) is continuous for all \(n\) but

\[
\lim_{n \to \infty} f_n = \begin{cases}
0 & x \leq 1 \\
1 & x = 1
\end{cases}
\]

Instead, the following definition is often more useful. \end{remark}

\begin{definition} \textbf{(from class notes.)} We say that \textbf{ \(f_n\) \textbf{uniformly converges} to \(f\) on \([a, b]\)} if for every \(\epsilon > 0\) there exists \(N\) such that for every \(n > N\),

\[
\forall x \in [a, b] \ |f_n(x) - f(x)| < \epsilon
\]

\end{definition}

\begin{definition} \textbf{(Definition 7.1.5, Grimmett and Stirzaker.)} Let \(V\) be a collection of functions mapping \([0, 1]\) into \(\mathbb{R}\) and assume \(V\) is endowed with a function \(\lVert \cdot \rVert : V \to \mathbb{R}\) satisfying

\begin{enumerate}[(a)]

\item \(\lVert f \rVert \geq 0\) for all \(f \in V\)

\item \(\lVert f \rVert = 0\) if and only if \(f\) is the zero function (or equivalent to it)

\item \(\lVert af \rVert = |a| \cdot \lVert f \rVert \) for all \(a \in \mathbb{R}\), \(f \in V\)

\item \(\lVert f + g \rVert \leq \lVert f \rVert + \lVert g \rVert \) (Triangle Inequality)

\end{enumerate}

The function \(\lVert \cdot \rVert\) is called a \textbf{norm}. If \(\{f_n\}\) is a sequence of members of \(V\) then we say that \textbf{\(f_n \to f\) with respect to the norm \(\lVert \cdot \rVert\)} if \(\lVert f_n - f \rVert \to 0 \) as \(n \to \infty\).

\end{definition}

\begin{definition} \textbf{(Definition 7.16, Grimmett and Stirzaker.)} Let \(\epsilon > 0\) be prescribed, and define the distance between two functions \(g, h: [0, 1] \to \mathbb{R}\) by

\[
d_\epsilon(g,h) = \int_E dx
\]

where \(E = \{ u \in[0, 1] : |g(u) - h(u)| > \epsilon \}\). We say that \textbf{ \(f_n \to f\) in measure} if 

\[
d_\epsilon(f_n, f) \to 0 \text{ as } n \to \infty \text{ for all } \epsilon > 0
\]

\end{definition}

\begin{theorem} \textbf{Inversion Theorem (Theorem 5.9.2, Grimmett and Stirzaker).} Let \(X\) have distribution function \(F\) and characteristic function \(\phi\). Define \(\overline{F}: \mathbb{R} \to [0, 1]\) by

\[
\overline{F}(x) = \frac{1}{2} \big[ F(x) + \lim_{y \to x^-} F(y) \big]
\]

Then

\[
\overline{F}(b) - \overline{F}(a) = \lim_{N \to \infty} \int_{-N}^N \frac{\exp(-iat) - \exp(-ibt)}{2\pi i t} \cdot \phi(t) dt
\]

\end{theorem}

\begin{proof} See \citet{kingman1966introduction}. \end{proof}

\begin{corollary} \textbf{Corollary 5.9.3.} Random variables \(X\) and \(Y\) have the same characteristic function if and only if they have the same distribution function.
\end{corollary}

\begin{proof} \textbf{Available in Grimmett and Stirzaker section 5.9, pp. 189 - 190.} \end{proof}

\begin{definition} \textbf{(Definition 5.9.4, Grimmett and Stirzaker.)} We say that the sequence \(F_1, F_2, \ldots\) of distribution functions \textbf{converges} to the distribution function \(F\) (written \(F_n \to F\)) if \(F(x) = \lim_{n\to \infty} F_n(x)\) at each point \(x\) where \(F\) is continuous.
\end{definition}

\begin{theorem}\label{asym.contin.thm}
\textbf{Continuity theorem (Thereom 5.9.5; in notes from Friday 10/26, Lecture 28).} Suppose that \(F_1, F_2, \ldots\) is a sequence of distribution functions with corresponding characteristic functions \(\phi_1, \phi_2, \ldots\). 

\begin{enumerate}[(a)]

\item If \(F_n(x) \to F(x)\) for some distribution function \(F\) with characteristic function \(\phi\) (at \(x\) where \(F\) is continuous), then \(\phi_n(t) \to \phi(t)\) for all \(t\).

\item Conversely, if \(\phi(t) = \lim_{n \to \infty} \phi_n(t)\) exists and \(\phi(t)\) is continuous at \(t = 0\), then \(\phi\) is the characteristic function of some distribution function \(F\), and \(F_n \to F\). 

\end{enumerate}
\end{theorem}

\begin{proof} See \citet{kingman1966introduction}. \end{proof}


%%% Section 8.6
\subsection{Inequalities (8.6 of Pesaran)}
\label{sec:tsch8.6}

\textbf{Inequalities}

\begin{itemize}

\item Probabilities

\begin{itemize}

\item \begin{lemma}\label{asym.markov} \textbf{Markov's Inequality (Grimmett and Stirzaker p. 311, 319) ):} Let \(X: \Omega \to [-\infty, \infty]\) be a random variable. Then for all \(a > 0\),

\[
\Pr(|X| \geq a) \leq \frac{\E(|X|)}{a}
\]
\end{lemma}
\begin{proof} Note \( t \cdot \boldsymbol{1}_{\{|X| \geq t\}} \leq |X|\), where \(\boldsymbol{1}\) is the indicator function. Dividing both sides by \(t\) and taking expectations, we have \

\[
\E( \boldsymbol{1}_{\{|X| \geq t\}}) \leq \frac{\E |X|}{t} \iff \Pr(|X|)\geq t)\leq\frac{\E |X|}{t},\qquad\forall\,t>0.
\] \end{proof}

\begin{corollary} If \(n\) is a positive integer, then

\[
\Pr(|X| \geq t) \leq \frac{\E(|X|^n)}{t^n} \ \ \forall t > 0
\]

\end{corollary}

\begin{proof}By Markov's Inequality (Theorem \ref{asym.markov}), 

\[
\Pr(|X| \geq t) = \Pr(|X|^n \geq t^n) \leq \frac{\E(|X|^n)}{t^n}
\]

\end{proof}

\item \begin{theorem}\label{asym.cheby}\textbf{Chebyshev's Inequality:} (probability p. 319) Let \(X: \Omega \to [-\infty, \infty]\) be an (integrable) random variable with \(\E(X^2) < \infty\).Then for any real number \(k > 0\)

\[
\Pr \big(\left| X - \E(X) \right| \geq k \sqrt{\Var(X)} \big) \leq \frac{1}{k^2}
\] 

This can also be written as

\[
\Pr(|X - \E(X)| \geq k) \leq \frac{\Var(X)}{k^2}
\]
\end{theorem}

(Can be used to demonstrate consistency of estimators: if we can show that as \(T \to \infty\) \(\Var(X) = \sigma^2 \to 0\), then this implies \(\Pr \big(\left| X - \mu \right| \geq k \sigma \big) \to 0\) as \(T \to \infty\), showing consistency.)

\item \begin{theorem} \textbf{Chernoff} For \(x \geq 0\), \(a > 0\), \(\forall \ t > 0\),

\[
\Pr(X \geq a) = \Pr( e^{tX} \geq e^{ta} ) \leq \frac{\E(e^{tX})}{e^{ta}}
\]
\end{theorem} 
\end{itemize}

\item \textbf{Moments}

\begin{itemize}

\item \begin{theorem}[ \textbf{Cauchy-Schwarz} (and \textbf{Bunyakovsky).}] \label{asym.cauchy.schwarz} If \(X\) and \(Y\) are random variables with finite variance then

\[
\E(XY)^2 \leq \E(X^2) \E(Y^2)
\]
\end{theorem}

Note that this can is a corollary of Theorem \ref{asym.thm.holder.ineq} with \(p = q = 2\). We can also prove this theorem on its own in a different one. We first prove a useful result.

\begin{lemma}\label{asym.cauchy.lemma}If \(\Var(X) = 0\) then \(X\) is almost surely constant; that is, \(\Pr(X = a) = 1\) for some \(a \in \mathbb{R}\).
\end{lemma}

\begin{proof}
Note that because \(\Var(X) = 0 < \infty\), we know that \(\E(X)\) and \(\E(X^2)\) exist. We have

\[
\Var(X) = \E\big[ (X - \E(X))^2 \big] = 0
\]

Let \(Y = (X - \E(X))^2 \). Note that \(Y = (X - \E(X))^2 \geq 0\) and that \(\E(Y) = \Var(X) = 0\). Therefore \(\Pr(Y=0) = 1\), so \(\Pr(Y \neq 0) = 0\). To see why, in the case that \(X\) is discrete,

\[
\E(Y) = \sum_{k=0}^\infty k \cdot \Pr(Y=k) = \Var(X) = 0 
\]

which is true if and only if \(\Pr(Y=k) = 0\) for all \(k > 0\). Since we already showed that \(Pr(Y < 0) = 0\), it follows that \(\Pr(Y=0) = 1\). In the continuous case,

\[
\E(Y) = \int_{0}^\infty y \cdot f_Y(y) dy = \Var(X) = 0 
\]

which implies that \(f_Y(x) = 0\) for all \(x > 0\). Again, since \(\Pr(Y < 0) = 0\), we have \(\Pr(Y \neq 0) = 0\). But \(Y = 0 \iff X = \E(X)\) so we have \(\Pr(X = \E(X)) = 1\).
\end{proof}

\begin{remark}Note that Lemma \ref{asym.cauchy.lemma} along with Proposition \ref{prob.const.var} imply that \(X\) has variance 0 if and only if it is (almost surely) constant.
\end{remark}

We are now ready to prove the Cauchy-Schwarz Inequality.

\begin{proof}
if \(\E(X^2) = 0\) or \(\E(Y^2)=0\), the Cauchy-Schwarz Inequality follows immediately. To see why, suppose without loss of generality that \(\E(X^2) = 0\). Then the right side is 0. Also, \(0 \leq \Var(X) = \E(X^2) - \E(X)^2 = - \E(X)^2\). Since \(\E(X)^2 \geq 0\), we must have \(E(X)^2 =0\) and therefore \(\Var(X) = 0\). Therefore by Lemma \ref{asym.cauchy.lemma}, \(X\) is almost surely constant, which means that \(\Cov(X,Y) = 0\).

\

 In the case that \(\E(X^2) > 0\) and \(\E(Y^2) > 0\), for \(a, b \in \mathbb{R}\), let \(Z = aX - bY\). Then 

\begin{equation}\label{asym.cauchyschwarz.1}
0 \leq \E(Z^2) = a^2 \E(X^2) - 2ab \E(XY) + b^2 \E(Y^2)
\end{equation}

The right side of (\ref{asym.cauchyschwarz.1}) is quadratic in \(a\). Because it is greater than or equal to zero, it has at most one real root, which means its discriminant must be non-positive. That is, if \(b \neq 0\),

\[
(-2b \E(XY))^2 - 4 b^2\E(X^2)\E(Y^2) \leq 0 \iff \E(XY)^2 - \E(X^2)\E(Y^2) \leq 0
\]

which yields the result. Note that equality holds if and only if \(\Pr(aX = bY) = 1\) because the discriminant is zero if and only if the quadratic has a real root, which occurs if and only if 

\[
\E\big[ (aX -bY)^2 \big] = 0
\]

which is true if and only if \(\Pr(aX = bY) = 1\) by by Lemma \ref{asym.cauchy.lemma} and Proposition \ref{prob.const.var}.
\end{proof}

\item \textbf{Krylov}

\item \begin{definition}Let \(\phi: \mathbb{R} \to \mathbb{R}\). We say that \(\phi\) is \textbf{convex} if for any \(x, y \in \mathbb{R}\) and for any \(t \in [0,1]\), we have

\[
\phi(tx + (1-t)y) \leq t \phi(x) + (1-t) \phi(y)
\]
 \end{definition}
 
% \begin{proposition}\label{asym.conv.prop}  Let \(\phi: \mathbb{R} \to \mathbb{R}\). Then \(\phi\) is convex if and only if for any \(y \in \mathbb{R}\) there exists a constant \(a\) and there exists a function \(L : \mathbb{R} \to \mathbb{R}\) defined by \(L(x) = a(x-y) + \phi(y), x \in \mathbb{R}\) such that \(L(y) = \phi(y)\) and such that \(L(x) \leq \phi(x)\) for all \(x \in \mathbb{R}\).
% 
% \end{proposition}
% 
% \begin{proof} Hint: Suppose \(\phi\) is convex. If \(x\) is fixed and \(y\) varies, show that \(\frac{\phi(y)- \phi(x)}{y-x}\) increases as \(y\) increases. Draw a picture. What slope \(a\) should \(L\) have at \(x\)?
% 
% \end{proof}

% \begin{theorem} \textbf{Jensen's Inequality} () Let \(X: \Omega \to [-\infty, \infty]\) be a random variable. Let \(u: \mathbb{R} \to \mathbb{R}\) be convex. If \(\E |X| < \infty\) and \(\E|u(X)| < \infty\), then
%
%\[
%u(\E(X)) \leq \E(u(X)) 
%\]
%\end{theorem}

\begin{theorem}[\textbf{Jensen's Inequality, from Math 541A. Also Grimmett and Stirzaker p.181, 349}]\label{asym.jensen.general}Let $X:\Omega\to[-\infty,\infty]$ be a random variable.  Let $\phi:\mathbb{R}\to\mathbb{R}$ be convex. If $\E|X|<\infty$ and $\E|\phi(X)|<\infty$, then
$$\phi(\E X)\leq \E \phi(X).$$ (See also Theorem \ref{cvx.jensen.general}.)
\end{theorem}

For the definition of convexity, see Definition \ref{cvx.defn.convex}.

\begin{proof}Note that from Theorem \ref{cvx.convex.tangent.line}, for any \(y \in \mathbb{R}\) there exists a constant \(a\) and a function \(L\) such that

\[
a(x-y)+\phi(y) \leq \phi(x) \ \ \ \forall x \in \mathbb{R}
\]

Letting \(y = \E(X)\) we have

\[
a (X-\E X)+\phi(\E X) \leq \phi(X)
\]

Since expectations preserve inequalities,

\[
\E[a (X-\E X)+\phi(\E X)] \leq \E \phi(X)
\]

But

\[
\E[a(X-\E X)+\phi(\E X)]  = a(\E X - \E X) + \E(\phi(\E X)) = \phi(\E X)
\]

which yields

\[
\phi( \E X) \leq \E \phi(X).
\]

\end{proof}

%\begin{proof} Note that from Exercise 4 in Math 541A Homework 2, we have 
%
%\[
%u'(y) (x-y)+u(y) \leq u(x) \ \ \ \forall x \in \mathbb{R}
%\]
%
%Letting \(y = \E(X)\) we have
%
%\[
%u'(\E X) (X-\E X)+u(\E X) \leq u(X)
%\]
%
%% \iff u(\E X) \leq u(X) - u'(\E X) (X-\E X)
%
%Since expectations preserve inequalities,
%
%\[
%\E[u'(\E X) (X-\E X)+u(\E X)] \leq \E u(X)
%\]
%
%But
%
%\[
%\E[u'(\E X) (X-\E X)+u(\E X)]  = u'(\E X) (\E X - \E X) + \E(u(\E X)) = u(\E X)
%\]
%
%which yields
%
%\[
%u( \E X) \leq \E u(X).
%\]
%
%\end{proof}

For some corollaries, see section \ref{cvx.sec.jensen.etc}.

%\begin{proof} Follows immediately from Jensen's Inequality using \(u(X) = X^2 \).\end{proof}

\item \begin{theorem}\label{asym.thm.holder.ineq}[\textbf{H\"{o}lder (Grimmett and Stirzaker p. p. 143, 319; Theorem 1.99 in Math 541A lecture notes) Generalization of Cauchy-Schwarz}] Let \(X, Y : \Omega \to \mathbb{R}\) be random variables. For \(p, q \geq 1\) satisfying \(1/p + 1/q =1\) we have

\[
\E(|XY|) \leq (\E(|X^p|))^{1/p}(\E(|X^q|))^{1/q} = \lVert X \rVert_p \lVert Y \rVert_q.
\]

The equality case happens only if \(X\) is a constant multiple of \(Y\) with probability 1. Note that the case \(p=q=2\) recovers the Cauchy-Schwarz Inequality (Theorem \ref{asym.cauchy.schwarz}).
\end{theorem}

\begin{proof} Assume without loss of generality that \(\lVert X\rVert_p = \lVert Y \rVert_q = 1\). Also, the case \(p=1, q=\infty\) follows from the triangle inequality, so we assume \(1 < p < \infty\). From concavity of the log function, we have

\[
\log\big((x^p)^{1/p}(y^q)^{1/q} \big) = (1/p) \log \big( x^p\big) + (1/q) \log \big(y^q\big)
\]

\[
 \leq \log \bigg( \frac{1}{p} x^p + \frac{1}{q} y^q\bigg) 
 \]
 
 \[
 \implies (x^p)^{1/p}(y^q)^{1/q} \leq \frac{1}{p} x^p + \frac{1}{q} y^q
 \]
 
 Fixing an \(\omega \in \Omega\), we have
 
 \[
|X(\omega) Y(\omega)| =  (|X(\omega)|^p)^{1/p}(|Y(\omega)|^q)^{1/q} \leq \frac{1}{p}  |X(\omega)|^p + \frac{1}{q}  |Y(\omega)|^q
 \]
 
Integrating we have...
 
\end{proof}

\item \begin{theorem} \textbf{Minkowski} (Grimmett and Stirzaker p. p. 143) For \(p \geq 1\),

\[
[\E(|X + Y|^p)]^{1/p} \leq (\E|X^p|)^{1/p} + (\E|Y^p|)^{1/p}
\]
\end{theorem}

\item Useful for showing lower order moments are finite (e.g. finite variance implies finite mean). \begin{lemma}\textbf{Lyapunov's Inequality (Grimmett and Stirzaker p. 143).}\label{asym.lyapunov} For \(0 < r \leq s < \infty\),

\[
\E(|X|^r)^{1/r} \leq \E(|X|^s)^{1/s} 
\]
\end{lemma}

\item \begin{theorem}\label{asym.tri.ineq.norm} \textbf{Triangle Inequality:} Let \(X, Y: \Omega \to \mathbb{R}\) be random variables. Let \(1 \leq p \leq \infty\). Then

\[
\lVert X+Y\rVert_p \leq \Vert X\rVert_p + \lVert Y \rVert_p, 1 \leq p \leq \infty
\]

\begin{proof} The case \(p = \infty\) follows from the scalar triangle inequality, so assume \(1 \leq p < \infty\). By scaling, we may assume \(\lVert X \rVert_p = 1 - t, \lVert Y \rVert_p = t\), for some \(t \in (0,1)\) (zeroes and infinities being trivial). Define \(V:=X/(1-t), W:=Y/t\). Then by convexity of \(x \to |x|^p\) on \(\mathbb{R}\), 

\[
|(1-t)V(\omega) + t(W(\omega)|^p \leq (1-t)|V(\omega)|^p + t|W(\omega)|^p
\]

Take expectation of both sides:
\[
\E|X+Y|^p \leq (a-t)^{1-p}\E(|X|^p) + t^{1-p}\E(|Y|^p)
\]

Since \(\lVert X\rVert_p = t, \lVert Y \rVert_p=1-t\), we have that the right side is \(1 - t + t = 1\). (Note: \(\lVert Y\rVert_p = t, \E|Y|^p = t^p, \lVert X\rVert_p = 1-t\) Therefore

\[
(\E|X+Y|^p)^{1/p} = \lVert X+Y\rVert_p \leq 1
\]

\end{proof}

\begin{remark} See also Theorem \ref{ra.tri.ineq.1} and Corollary \ref{ra.tri.ineq.2}. \end{remark}

\end{theorem}
\end{itemize}

\end{itemize}

\begin{theorem}[\textbf{Chernoff Bound}]
Let $X$ be a random variable and let $r>0$.  Define $M_{X}(t):= \E e^{tX}$ for any $t\in\mathbb{R}$. Then for any $t>0$,

\[
\mathbb{P}(X>r)\leq e^{-tr}M_{X}(t).
\]

\end{theorem}

\begin{proof} Using Markov's Inequality (Theorem \ref{asym.markov}) on \(e^{tX}\), we have

\[
\Pr (X\geq r) = \Pr (e^{tX} \geq e^{t r}) \leq \frac{\E e^{tX}}{e^{t r}}  = e^{-tr}M_{X}(t), \ \ \ \forall\,t>0.
\]

\end{proof}

\begin{remark}Consequently, if $X_{1},\ldots,X_{n}$ are independent random variables with the same CDF, and if $r,t>0$,
$$\mathbb{P}\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}>r\right)\leq e^{-trn}(M_{X_{1}}(t))^{n}.$$
For example, if $X_{1},\ldots,X_{n}$ are independent Bernoulli random variables with parameter $0<p<1$,  and if $r,t>0$,
$$\mathbb{P}\left(\frac{X_{1}+\cdots+X_{n}}{n}-p>r\right)\leq e^{-trn}( e^{-tp}[pe^{t}+(1-p)])^{n}.$$
And if we choose $t$ appropriately, then the quantity $\mathbb{P}\left(\frac{1}{n}\sum_{i=1}^{n}(X_{i}-p)>r\right)$ becomes exponentially small as either $n$ or $r$ become large.  That is, $\frac{1}{n}\sum_{i=1}^{n}X_{i}$ becomes very close to its mean.  Importantly, the Chernoff bound is much stronger than either Markov's or Cheyshev's inequality, since they only respectively imply that
$$\mathbb{P}\left( \left|\frac{X_{1}+\cdots+X_{n}}{n}-p \right|>r\right)\leq \frac{2p(1-p)}{r},   % E|X-p| = p (1-p) + (1-p)p=2p(1-p)
\quad\mathbb{P}\left(\left|\frac{X_{1}+\cdots+X_{n}}{n}-p \right|>r\right)\leq \frac{p(1-p)}{nr^{2}}.$$ % var(X-p)= E(X-p)^2 = p(1-p)^2+(1-p)p^2= p(1-p)[1-p+p]=p(1-p)
\end{remark}

\textbf{Monotone convergence theorem.}

\textbf{Dominated Convergence Theorem} (Theorem \ref{prob.dom.convg.thm}).

%%% Section 8.2
\subsection{Modes of Convergence (7.2 of Grimmett and Stirzaker, 8.2 and 8.4 of Pesaran)}

 Let \(\{X_n\} = \{X_1, X_2, \ldots\}\) and \(X\) be random variables defined on a probability space \((\Omega, \mathcal{F}, \mathbb{P})\).

%%%% Convergence in probability
\begin{definition} \textbf{Convergence in probability.} \(\{X_n\}\) is said to \textbf{converge in probability} to \(X\) if
\begin{itemize}

\item Grimmett and Strizaker definition:
\[
\lim_{n \to \infty} \Pr(|X_n -X| > \epsilon) = 0, \text{ for every } \epsilon > 0
\]

\item Pesaran definition:
\[
\lim_{n \to \infty} \Pr(|X_n -X| < \epsilon) = 1, \text{ for every } \epsilon > 0
\]

\item More formal (from Math 541A):

\[
\forall \epsilon > 0, \ \lim_{n \to \infty} \Pr(\{\omega \in \Omega : |X_n(\omega) - X(\omega)| > \epsilon) = 0
\]

\end{itemize}

\begin{remark}This mode of convergence is also often denoted by \(X_n \xrightarrow{p} X\) and when \(X\) is a fixed constant it is referred to as the \textbf{probability limit of \(X_n\)}, written as \(Plim(X_n) = x\), as \(n \to \infty\).

The above concept is readily extended to multivariate cases where \(\{ \boldsymbol{X}_n, n = 1, 2, \ldots \}\) denote \(m\)-dimensional vectors of random variables. Then the condition is

\[
\lim_{n \to \infty} \Pr(\lVert \boldsymbol{X}_n -\boldsymbol{X}\rVert  < \epsilon) = 1, \text{ for every } \epsilon > 0
\]

where \(\lVert \cdot \rVert \) denotes an appropriate norm (say \(\ell_2\)). Convergence in probability is often referred to as "weak convergence" (in contrast to convergence with probability 1, below).
\end{remark}
\end{definition} 

%%%% Convergence with probability 1 or almost surely
\begin{definition}
\textbf{Convergence with probability 1 or almost surely.} The sequence of random variables \(\{X_n\}\) is said to \textbf{converge with probability 1} (or \textbf{almost surely}) to \(X\) if 

\begin{itemize}

\item (505A class notes definition)

\[
\Pr\big( \{\omega \in \Omega: \lim_{n \to \infty} X_n(\omega) = X(\omega) \} \big) = 1
\]

(Note: pointwise convergence can hardly ever be shown here and is not useful.)

\item Grimmett and Stirzaker textbook definition:
\[
\Pr \big( \{\omega \in \Omega: X_n(\omega) \to X(\omega) \text{ as } n \to \infty \} \big) = 1
\]

\item Pesaran textbook definition:

\[
\Pr \bigg( \lim_{n \to \infty} X_n = X \bigg) = 1
\]

\end{itemize}

\begin{remark}This is often written as \(X_n \xrightarrow{w.p.1} X\) or \(X_n \xrightarrow{a.s.} X\). An equivalent condition for convergence with probability 1 is given by

\[
\lim_{n \to \infty} \Pr( |X_m - X| < \epsilon, \text{ for all } m \geq =n) = 1, \text{ for every } \epsilon > 0
\]

which shows that convergence in probability is a special case of convergence with probability 1 (obtained by setting \(m = n\)). Convergence with probability 1 is stronger than convergence in probability and is often referred to as ``strong convergence." \end{remark}

\end{definition}

%%%% Convergence in r-th mean
\begin{definition}
\textbf{Convergence in \(r\)-th mean} or \textbf{convergence in \(\ell_p\)}. \(X_n \to X\) \textbf{in \(r\)th mean}  (or \textbf{in \(\ell_p\)}) where \(r \geq 1\) (or \(0 < p \leq \infty\)) if \(\E|X_n^r| < \infty\) for all \(n\) and

\[
\lim_{n \to \infty} \E(|X_n - X|^r) = 0
\]

or if \(\lVert X \rVert_p < \infty\) and

\[
\lim_{n \to \infty} \lVert X_n - X \rVert _p = 0
\]

\begin{remark}
Recall that \(\lVert X \rVert_p := (\E(X)^p)^{1/p}\) if \(0 < p < \infty\) and \(\lVert X \rVert_\infty := \inf \{c >0: \Pr(|X| \leq c ) = 1\}\). Note that if \(p < 1\), \(\lVert \cdot \rVert_p\) is no longer a norm because it does not satisfy the Triangle Inequality (Corollary \ref{asym.tri.ineq.exp} and Theorem \ref{asym.tri.ineq.norm}), but this property still holds. Convergence in \(r\)th mean is often written \(X_n \xrightarrow{r} X\).
\end{remark}
\end{definition}

%%%%%%% Convergence in Distribution
\begin{definition}
\textbf{Convergence in Distribution.} Let \(X_1, X_2, \ldots\) have distribution functions \(F_1(\cdot), F_2(\cdot), \ldots \) respectively. Then \(X_n\) is said to \textbf{converge in distribution to \(X\)} if

\[
\lim_{n \to \infty} \Pr(X_n \leq u) = \Pr(X \leq u)
\]

for all \(u\) at which \(F_X(x) = \Pr(X \leq x)\) is continuous. This can also be written

\[
\lim_{n \to \infty} F_n(u) = F(u)
\]

for all \(u\) at which \(F\) is continuous.

\begin{remark}Convergence in distribution is usually denoted by \(X_n \xrightarrow{d} X\), \(X_n \xrightarrow{L} X\), or \(F_n \implies F\). By the Continuity Theorem (Theorem \ref{asym.contin.thm}, section \ref{asym.preliminaries}), this is equivalent to

\[
\lim_{n \to \infty} \phi_{X_n}(t) = \phi_X(t), \ \ t \in \mathbb{R}.
\]

Note that the random variables are allowed to have different domains.
\end{remark}
\end{definition}

\begin{definition} \textbf{(Convergence in distribution for vector-valued random variables.)} We say that random variables \(Y^{(1)}, \ldots, : \Omega \to \mathbb{R}^d\) \textbf{converge in distribution} to \(Y: \Omega \to \mathbb{R}^d\) if for all \(v \in \mathbb{R}^d\), \(\langle v, Y^{(1)} \rangle, \langle v, Y^{(2)} \rangle, \ldots\) converges in distribution to \(\langle v, Y \rangle\).

\end{definition}

\begin{theorem}
\textbf{(Theorem 7.2.3, Grimmett and Stirzaker.)} The following implications hold:

\begin{itemize}

\item \( (X_n \xrightarrow{a.s.} X) \implies (X_n \xrightarrow{p} X) \)

\item \( (X_n \xrightarrow{r} X) \implies (X_n \xrightarrow{p} X) \text{ for any } r \geq 1 \)

\item \( (X_n \xrightarrow{p} X) \implies (X_n \xrightarrow{d} X) \)

\end{itemize}

Also, if \(r > s \geq 1\), then \((X_n \xrightarrow{r} X) \implies (X_n \xrightarrow{s} X)\). No other implications hold in general.
\end{theorem}

\begin{theorem}\label{asym.7.2.4}\textbf{Some exceptions (Theorem 7.2.4).}

\begin{itemize}

\item If \(X_n \xrightarrow{d} c\) where \(c\) is constant, then \(X_n \xrightarrow{p} c\).

\item If \(X_n \xrightarrow{p} X\) and \(\Pr(|X_n| \leq k) = 0\) for all \(n\) and some \(k\), then \(X_n \xrightarrow{r} X\) for all \(r \geq 1\).

\item If \(P_n(\epsilon) = \Pr(|X_n - X| > \epsilon)\) satisfies \(\sum_n P_n(\epsilon) < \infty\) for all \(\epsilon > 0\), then \(X_n \xrightarrow{a.s.} X\). 

\end{itemize}
\end{theorem}

\begin{proof}(Part (c).) Let \(A_n(\epsilon) = \{|X_n - X| > \epsilon\}\) (so that \(P_n(\epsilon) = \Pr[A_n(\epsilon))]\), and let \(B_m(\epsilon) = \bigcup_{n \geq m} A_n(\epsilon)\). Then

\[
\Pr(B_m(\epsilon)) \leq \sum_{n=m}^\infty \Pr(A_n(\epsilon))
\]
so \(\lim_{m \to \infty} \Pr(B_m(\epsilon)) = 0\) whenever \(\sum_n \Pr(A_n(\epsilon)) < \infty\). See also Lemma \ref{asym.7.2.10} part (b).\end{proof}

%%% Section 7.2
\subsection{More on convergence (7.2 of Grimmet and Stirzaker)}

\textbf{Other theorems to include:} Fatou's Lemma, Fubini's Theorem, Kolmogorov's Maximal Inequality, Kolmogorov Three-Series Test, Lindeberg Feller Central Limit Theorem, \textbf{this and more at beginning of Mike's 505A qual solutions.}

\begin{definition} \textbf{Cauchy Convergence.} We say that the sequence \(\{X_n: n \geq 1\}\) of random variables on the probability space \((\Omega, \mathcal{F}, \mathbb{P})\) is \textbf{almost surely Cauchy convergent} if

\[
\Pr \big( \{\omega \in \Omega: X_m(\omega) - X_n(\omega) \to 0 \text{ as } m, n \to \infty \} \big) = 1
\]

That is, the set of points \(\omega\) of the sample space for which the real sequence \(\{X_n(\omega): n \geq 1\}\) is Cauchy convergent is an event having probability 1.
\end{definition}

\begin{lemma}\label{asym.7.2.6} \textbf{(Lemma 7.2.6 from Grimmett and Stirzaker)} 
\begin{enumerate}[(a)]
\item If \(r > s \geq 1\) and \(X_n \xrightarrow{r} X\), then \(X_n \xrightarrow{s} X\).
\item If \(X_n \xrightarrow{1} X\) then \(X_n \xrightarrow{p} X\). 
\end{enumerate}
The converse assertions fail in general.
\end{lemma}

\begin{proof}
\begin{enumerate}[(a)]
% 7.2.6(a) proof
\item
Using Lyapunov's Inequality (Lemma \ref{asym.lyapunov}), if \(r > s \geq 1\)

\[
\big[ \E(|X_n - X|^s) \big]^{1/s} \leq \big[ \E(|X_n - X|^r) \big]^{1/r}
\]
Therefore if \(X_n \xrightarrow{r} X\) (meaning \(\lim_{n \to \infty} \E(|X_n - X|^r) = 0\)), ( then  \(\lim_{n \to \infty} \E(|X_n - X|^s) = 0\), so \(X_n \xrightarrow{s} X\). We show the converse fails by counterexample:

\[
X_n = \begin{cases}
n & \text{with probability } n^{(-1/2)(r+s)} \\
0 & \text{with probability } 1 -  n^{(-1/2)(r+s)}
\end{cases}
\]

Then \(\E|X_n^s| = n^{(1/2)(s-r)} \to 0\) and \(\E|X_n^r| = n^{(1/2)(r-s)} \to \infty\).

% 7.2.6(b) proof
\item By Markov's Inequality (Lemma \ref{asym.markov}),

\[
\Pr(|X_n - X| > \epsilon) \leq \frac{\E|X_n - X|}{\epsilon} \ \ \text{ for all } \epsilon > 0
\]

Therefore if \(X_n \xrightarrow{1} X\); that is, \(\lim_{n \to \infty} \E(|X_n - X|) = 0\), then \(\lim_{n \to \infty} \Pr(|X_n -X| > \epsilon) = 0\) for every \(\epsilon > 0\), so \(X_n \xrightarrow{p} X\).

To see the converse fails, define an independent sequence \(\{X_n\}\) by 

\[
X_n = \begin{cases}
n^3 & \text{with probability } n^{-2} \\
0 & \text{with probability } 1 - n^{-2}
\end{cases}
\]

Then \(\Pr(|X| > \epsilon) = n^{-2}\) for all large \(n\), and so \(X_n \xrightarrow{p} 0\). However, \(\E|X_n| = n \to \infty\).
\end{enumerate}
\end{proof}

\begin{lemma}\label{asym.7.2.10}
\textbf{(Lemma 7.2.10, Grimmett and Stirzaker.)} Let \(A_n(\epsilon) = \{|X_n - X| > \epsilon \}\) and \(B_m(\epsilon) = \cup_{n \geq m} A_n(\epsilon)\). Then:

\begin{enumerate}[(a)]

\item \(X_n \xrightarrow{a.s.} X\) if and only if \(\Pr(B_m(\epsilon)) \to 0\) as \(m \to \infty\) for all \(\epsilon > 0\).

\item \(X_n \xrightarrow{a.s.} X\) if \(\sum_n \Pr(A_n(\epsilon)) < \infty\) for all \(\epsilon > 0\).

\item If \(X_n \xrightarrow{a.s.} X\) then \(X_n \xrightarrow{p} X\), but the converse fails in general.

\end{enumerate}
\end{lemma}
\begin{proof}\begin{enumerate}[(a)]

\item

\item As for Theorem \ref{asym.7.2.4} part (c).

\item To see the converse fails, define an independent sequence \(\{X_n\}\) by

\[
X_n = \begin{cases}
1 & \text{with probability } n^{-1} \\
0 & \text{with probability } 1 - n^{-1}
\end{cases}
\]

Clearly \(X_n \xrightarrow{p} 0\). However, if \(0 < \epsilon < 1\),

\[
\Pr(B_m(\epsilon)) = 1 - \lim_{r \to \infty} \Pr(X_n = 0 \text{ for all } n \text{ such that } m \leq n \leq r) \text{ (by Lemma 1.3.5)}
\]

\[
=1 - \bigg( 1 - \frac{1}{m} \bigg)\bigg(1 - \frac{1}{m+1} \bigg) \cdots \text{ (by independence)}
\]

\[
= 1 - \lim_{M \to \infty}\bigg( \frac{m-1}{m} \cdot \frac{m}{m+1} \cdot \frac{m+1}{m+2} \cdots \frac{M}{M +1} \bigg)
\]

\[
= 1 - \lim_{M \to \infty} \frac{m-1}{M +1} = 1
\]

and so \(\{X_n\}\) does not converge almost surely.

\end{enumerate}\end{proof}

\begin{lemma}\textbf{(Lemma 7.2.12, Grimmett and Stirzaker.)} There exist sequences which

\begin{enumerate}[(a)]
\item converge almost surely but not in mean,
\item converge in mean but not almost surely.
\end{enumerate}
\end{lemma}
\begin{proof}
\begin{enumerate}[(a)]

\item As for Lemma \ref{asym.7.2.6} part (b).
\end{enumerate}
\end{proof}

\begin{theorem} \textbf{(Theorem 7.2.13, Grimmett and Stirzaker.)} If \(X_n \xrightarrow{p} X\), there exists a non-random increasing sequence of integers \(n_1, n_2, \ldots\) such that \(X_{n_i} \xrightarrow{a.s.} X\) as \(i \to \infty\). \end{theorem}

\begin{theorem}\textbf{Skorokhod's representation theorem (Theorem 7.2.14, Grimmett and Stirzaker).} If \(\{X_n\}\) and \(X\) with distribution functions \(\{F_n\}\) and \(F\) are such that \(X_n \xrightarrow{d} X\) (or equivalently, \(F_n \to F\)) as \(n \to \infty\), then there exists a probability space \((\Omega', \mathcal{F}', \mathbb{P}')\) and random variables \(\{Y_n\}\) and \(Y\) mapping \(\Omega'\) into \(\mathbb{R}\) such that

\begin{enumerate}[(a)]

\item \(\{Y_n\}\) and \(Y\) have distribution functions \(\{F_n\}\) and \(F\)

\item \(Y_n \xrightarrow{a.s.} Y\) as \(n \to \infty\)

\end{enumerate}

Therefore, although \(X_n\) may fail to converge to \(X\) in any mode other than in distribution, there exists a sequence \(\{Y_n\}\) such that \(Y_n\) is distributed identically to \(X_n\) for every \(n\), which converges almost surely to a copy of \(X\).
\end{theorem}



\begin{theorem}\textbf{(Theorem 7.2.19, Grimmett and Stirzaker; same as Portmanteau Theorem?)} The following three statements are equivalent:

\begin{enumerate}[(a)]

\item \(X_n \xrightarrow{d} X\)

\item \(\E[g(X_n)] \to \E[g(X)]\) for all bounded continuous functions \(g\).

\item \(\E[g(X_n)] \to \E[g(X)]\) for all functions \(g\) of the form \(g(x) = f(x) \boldsymbol{1}_{[a, b]}(x)\) where \(f\) is continuous on \([a, b]\) and \(a\) and \(b\) are points of continuity of the distribution function of the random variable \(X\). 

\end{enumerate}
\end{theorem}

\begin{theorem}\textbf{(Grimmett and Stirzaker Theorem 7.3.9.)} 
\begin{enumerate}[(a)]

\item If \(X_n \xrightarrow{a.s.} X\) and \(Y_n \xrightarrow{a.s.} Y\) then \(X_n + Y_n \xrightarrow{a.s.} X + Y\).

\item If \(X_n \xrightarrow{r} X\) and \(Y_n \xrightarrow{r} Y\) then \(X_n + Y_n \xrightarrow{r} X + Y\).

\item If \(X_n \xrightarrow{p} X\) and \(Y_n \xrightarrow{p} Y\) then \(X_n + Y_n \xrightarrow{p} X + Y\).

\item It is not in general true that  \(X_n + Y_n \xrightarrow{d} X + Y\) whenever  \(X_n \xrightarrow{d} X\) and \(Y_n \xrightarrow{d} Y\).

\end{enumerate}
\end{theorem}

%%% Borel-Cantelli lemmas 
\begin{theorem}\textbf{Borel-Cantelli lemmas (Grimmett and Stirzaker Theorem 7.3.10.)} \label{asym.borel} Let \(\{A_n\}\) be an infinite sequence of events from some probability space \((\Omega, \mathcal{F}, \mathbb{P})\). Let \(A = \bigcap_n \bigcup_{m=n}^\infty A_m\ = \limsup_{n \to \infty} A_n = \{A_n \text{ i.o.}\}\) be the event that infinitely many of the \(A_n\) occur. Then:

\begin{enumerate}[(a)]

\item \(\Pr(A) = 0 \) if \(\sum_n \Pr(A_n) < \infty\)

\item \(\Pr(A) = 1\) if \(\sum_n \Pr(A_n) = \infty\) and \(A_1, A_2, \ldots\) are independent events.

\end{enumerate}
\end{theorem}

\begin{proof}
\begin{enumerate}[(a)]

\item We have that \(A \subseteq \bigcup_{m=n}^\infty A_n\) for all \(n\), so

\[
\Pr(A) \leq \sum_{m=n}^\infty \Pr(A_m) \to 0 \text{ as } n \to \infty
\]

whenever \(\sum_n \Pr(A_n) < \infty\).

\item One can confirm that 

\[
A^c = \bigcup_n \bigcap_{m=n}^\infty A_m^c
\]

But 

\[
\Pr \bigg( \bigcap_{m=n}^\infty A_m^c \bigg) = \lim_{r \to \infty}\Pr \bigg( \bigcap_{m=n}^r A_m^c \bigg) = \prod_{m=n}^\infty[ 1 - \Pr(A_m)] \text{ (by independence) } \leq  \prod_{m=n}^\infty \exp(- \Pr(A_m)) 
\]

\[
= \exp \bigg(- \sum_{m=n}^\infty \Pr(A_m) \bigg) = 0
\]

whenever \(\sum_n \Pr(A_n) = \infty\), where the fourth step follows since \(1 - x \leq e^{-x}\) if \(x \geq 0\). Thus

\[
\Pr(A^c) = \lim_{n \to \infty} \Pr \bigg( \bigcap_{m=n}^\infty A_m^c \bigg) = 0
\]

so \(\Pr(A) = 1\).

\end{enumerate}
\end{proof}

\begin{theorem}\label{asym:k2st}\textbf{Kolmogorov's Two-Series Theorem.} Let \(X_1, X_2, \ldots\) be independent random variables with \(\E(X_n) = \mu_n\) and \(\Var(X_n) = \sigma_n^2\) such that \(\sum_{n=1}^\infty \mu_n < \infty\) and \(\sum_{n=1}^\infty \sigma_n^2 < \infty\). Then \(\sum_{n=1}^\infty X_n\) converges in \(\mathbb{R}\) almost surely.
\end{theorem}

\begin{proof}Available on wikipedia, \url{https://en.wikipedia.org/wiki/Kolmogorov\%27s_two-series_theorem}.\end{proof}

%%% Slutsky's convergence theorems
\subsubsection{Slutsky's Convergence Theorems (8.4.1 of Pesaran, 7.3 of Grimmett and Stirzaker)}

\begin{theorem}\textbf{Theorem 6 of Pesaran, Section 8.4.1, p. 173.} Let \( \{x_t, y_t\}, t = 1, 2, \ldots\) be a sequence of pairs of random variables with \(y_t \xrightarrow{d} y\) and \(\left| y_t - x_t \right| \xrightarrow{p}  0\). Then \(x_t \xrightarrow{d} y\). \end{theorem}

\begin{theorem}\label{asym.slutsky} \textbf{Theorem 7 in Pesaran, on p.318 (section 7.3) of Grimmett and Stirzaker.} (Section 8.4.1, p. 174)  If \(x_t \xrightarrow{d} x\) and \(y_t \xrightarrow{p} c\) where \(c\) is a finite constant, then

\begin{enumerate}[(i)]

\item \(x_t + y_t \xrightarrow{d} x + c\)

\item \(y_tx_t \xrightarrow{d} cx\)

\item \(x_t/y_t \xrightarrow{d}  x/c, \text{ if } c \neq 0\).

\end{enumerate} \end{theorem}

\begin{theorem} \textbf{on p.318 (section 7.3) of Grimmett and Stirzaker.} Suppose that \(X_n \xrightarrow{d} 0\) and \(Y_n \xrightarrow{p} Y\), and let \(g: \mathbb{R}^2 \to \mathbb{R}\) be such that \(g(x,y)\) is a continuous function of \(y\) for all \(x\), and \(g(x, y)\) is continuous at \(x=0\) for all \(y\). Then \(g(X_n, Y_n) \xrightarrow{p} g(0, Y)\). \end{theorem}

% \textbf{Theorem 8.} (Pesaran Section 8.4.1, p. 175) 

\begin{theorem}[\textbf{Continuous Mapping Theorem (Theorem 9 of Pesaran, Section 8.4.1, p. 176: convergence properties of transformed sequences.)}]\label{asym.contmappthm}Suppose \(\{\boldsymbol{x}_j\}\), \(\{\boldsymbol{y}_j\}\), \(\boldsymbol{x}\), and \(\boldsymbol{y}\) are \(k \times 1\) vectors of random variables on a probability space, and let \(\boldsymbol{g}(\cdot)\) be a continuous vector-valued function. (Alternatively, suppose \(g\) has the set of discontinuity points \(D_g\) such that \(\Pr(X \in D_g) = 0\).) Then

\begin{enumerate}[(i)]

\item \(\boldsymbol{x}_j \xrightarrow{a.s.} x \implies \boldsymbol{g}(\boldsymbol{x}_j) \xrightarrow{a.s.} \boldsymbol{g}(\boldsymbol{x})\)

\item \(\boldsymbol{x}_j \xrightarrow{p} x \implies \boldsymbol{g}(\boldsymbol{x}_j) \xrightarrow{p} \boldsymbol{g}(\boldsymbol{x})\)

\item \(\boldsymbol{x}_j \xrightarrow{d} x \implies \boldsymbol{g}(\boldsymbol{x}_j) \xrightarrow{d} \boldsymbol{g}(\boldsymbol{x})\)

\item \(\boldsymbol{x}_j  - \boldsymbol{y}_j \xrightarrow{p} \boldsymbol{0} \text{ and } \boldsymbol{y}_j \xrightarrow{d} \boldsymbol{y} \implies \boldsymbol{g}(\boldsymbol{x}_j) - \boldsymbol{g}(\boldsymbol{y}_j) \xrightarrow{d} \boldsymbol{0}(\boldsymbol{x})\)

\end{enumerate}

where \(x = (c_1, \ldots, c_k) \in \mathbb{R}^k\).

\end{theorem}

\begin{proof}[Proof (part (b), continuous case, one-dimensional codomain)] Let \(\boldsymbol{x}_j = (M_{j,1}, \ldots, M_{j,k})\). We have that

\[
\forall \epsilon_j > 0, \ \lim_{n \to \infty} \Pr(\{\omega \in \Omega : |M_{j,n}(\omega) - c_j| > \epsilon_j) = 0, \qquad \forall j \in \{1, \ldots, k\}.
\]

\begin{equation}\label{mathstats.541a.hw7.1a}
\iff \forall \epsilon_j > 0, \ \lim_{n \to \infty} \Pr(\{\omega \in \Omega : |M_{j,n}(\omega) - c_j| < \epsilon_j) = 1, \qquad \forall j \in \{1, \ldots, k\}.
\end{equation}

Because \(g\) is continuous, we have that for every \(\epsilon^* > 0\) there exists a \(\delta^* > 0\) such that


\begin{equation}\label{mathstats.541a.hw7.1c}
0 < \lVert (M_{1,n}(\omega),\ldots,M_{j,n}(\omega) \big) \rVert_2 < \delta^* \implies |  g(M_{1,n},\ldots,M_{j,n}) - g(c_1, \ldots, c_j)| < \epsilon^*.
\end{equation}

Note that since in \(\mathbb{R}\) the \(L_2\) and \(L_1\) norms are equivalent,

\[
 |M_{j,n}(\omega) - c_j| < \epsilon_j \iff  \lVert M_{j,n}(\omega) - c_j\rVert_2 < \epsilon_j  \implies \sum_{j=1}^k   \lVert M_{j,n}(\omega) - c_j\rVert_2  < \sum_{j=1}^k \epsilon_j 
 \]
 
%\begin{equation}\label{mathstats.541a.hw7.1b}
\[
\implies \lVert (M_{1,n}(\omega),\ldots,M_{j,n}(\omega) \big) \rVert_2 <  \sum_{j=1}^k \epsilon_j 
\]
%\end{equation}

%where the last step follows by the Triangle Inequality. Let \(\delta^* =  \sum_{j=1}^k \epsilon_j\) and note that since we can choose any value for each \(\epsilon_j\), (\ref{mathstats.541a.hw7.1b}) 
%
%\[
%\vdots
%\]

where the last step follows by the Triangle Inequality. Therefore letting \(\delta^* =  \sum_{j=1}^k \epsilon_j \), we have

\[
\Pr(\{\omega \in \Omega : |M_{j,n}(\omega) - c_j| < \epsilon_j)  \leq \Pr \big(0 < \lVert (M_{1,n}(\omega),\ldots,M_{j,n}(\omega) \big) \rVert_2 < \delta^*  \big)
\]

\[
 \leq \Pr(\{\omega \in \Omega : |g\big(M_{1,n}(\omega),\ldots,M_{j,n}(\omega) \big)- g(c_1, \ldots, c_j)| < \epsilon^*)
\]

where the last step follows from (\ref{mathstats.541a.hw7.1c}). So

\begin{equation}\label{mathstats.541a.hw7.1d}
\Pr(\{\omega \in \Omega : |M_{j,n}(\omega) - c_j| < \epsilon_j)  \leq \Pr(\{\omega \in \Omega : |g\big(M_{1,n}(\omega),\ldots,M_{j,n}(\omega) \big)- g(c_1, \ldots, c_j)| < \epsilon^*).
\end{equation}

Taking limits of (\ref{mathstats.541a.hw7.1d}) and substituting in (\ref{mathstats.541a.hw7.1a}), we have

%\[
%\vdots
%\]
%
%
%But 
%
%\[
%|M_{j,n}(\omega) - c_j| > \epsilon \iff \ldots \iff |h\big(M_{1,n}(\omega),\ldots,M_{j,n}(\omega) \big)- h(c_1, \ldots, c_j)| > \epsilon
%\]
%
%\[
%\vdots
%\]

\[
\forall \epsilon^* > 0, \ \lim_{n \to \infty} \Pr(\{\omega \in \Omega : |g\big(M_{1,n}(\omega),\ldots,M_{j,n}(\omega) \big)- g(c_1, \ldots, c_j)| < \epsilon^*) \geq 1
\]

\[
\iff \forall \epsilon^* > 0, \ \lim_{n \to \infty} \Pr(\{\omega \in \Omega : |g\big(M_{1,n}(\omega),\ldots,M_{j,n}(\omega) \big)- g(c_1, \ldots, c_j)| > \epsilon^*) = 0
\]

\[
\iff g(M_{1,n},\ldots,M_{j,n}) \xrightarrow{p} g(c_{1},\ldots,c_{j}).
\]


\

For remaining parts, see \citet{serfling1980} or \citet{rao1973linear}.\end{proof}

See also: 

\begin{theorem} \label{asym.thm.7.2.18} \textbf{(Theorem 7.2.18, Grimmett and Stirzaker.)} If \(X_n \xrightarrow{d} X\) and \(g: \mathbb{R} \to \mathbb{R}\) is continuous, then \(g(X_n) \xrightarrow{d} g(X)\). \end{theorem}

%%% Section 8.5
\subsection{Stochastic orders \(\mathcal{O}_p(\cdot)\) and \(o_p(\cdot)\) (Pesaran 8.5)}

\begin{definition}[\textbf{Pesaran 8.5 Definition 6.}] Let \(\{a_t\}\) be a sequence of positive numbers and \(\{x_t\}\) be a sequence of random variables. Then

\begin{enumerate}[(i)]

\item \(x_t = \mathcal{O}_p(a_t)\), or \(x_t/a_t\) is bounded in probability, if for every \(\epsilon > 0\) there exist real numbers \(M_\epsilon\) and \(N_\epsilon\) such that

\[
\Pr \bigg( \frac{|x_t|}{a_t} > M_\epsilon \bigg) < \epsilon, \ \ \ \text{for } t > N_\epsilon
\]

\item \(x_t = o_p(a_t)\) if

\[
\frac{\boldsymbol{x}_t}{a_t} \xrightarrow{p} 0
\]

\end{enumerate}
\end{definition}

\begin{definition}[Ross ISE 620 Definition] We say that \(f(x)\) is \(o(h)\) if \(\lim_{h \to 0} f(h)/h =0\).

\end{definition}

%%% Section 8.6
\subsection{Laws of Large Numbers and Central Limit Theorems (Pesaran 8.6; Grimmett and Stirzaker 7.4, 7.5)}

\begin{theorem}\label{asym.wlln} \textbf{Weak Law of Large Numbers (Khinchine) (Pesaran 8.6 Theorem 10, Grimmett and Stirzaker Theorem 7.4.7, 541A notes Theorem 2.10).} Suppose that \(\{X_k\}\) is a sequence of (i) independent (ii) identically distributed random variables with (iii) constant means, i.e., \(\E(X_k) = \mu < \infty\). Then

\[
\overline{X}_k = \frac{1}{n} \sum_{k=1}^n X_k \xrightarrow{p} \mu
\]

\end{theorem}

\begin{theorem}\textbf{Weak Law of Large Numbers (Chebyshev) (Pesaran Section 8.6, p. 178, Theorem 11.)} Let \(\{X_k\}\) be a sequence of random variables. If (i) \(\E(X_k) = \mu_k\), (ii) \(\Var(X_k) = \sigma_k^2\), and (iii) \(\Cov(X_k, X_j) = 0, \ k \neq j\), and (iv)

\[
\lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^n \sigma_k^2 < \infty
\]

then we have \(\overline{X}_n - \overline{\mu}_n \xrightarrow{p} 0\), where \( \overline{\mu}_n = n^{-1} \sum_{k=1}^n \mu_k\).\end{theorem}

\begin{theorem}\label{asym.lln1} \textbf{Strong Law of Large Numbers (Grimmett and Stirzakker Theorem 7.4.3).} Let \(\{X_k\}\) be a sequence of (i) independent (ii) identically distributed random variables with (iii) \(\E(X_k) = \mu \) and (iv) \(\E(X_k^2) < \infty \). Then 

\[
\frac{1}{n} \sum_{k=1}^n X_k \rightarrow \mu \text{ almost surely and in mean square.}
\]

\end{theorem}

\begin{theorem}[\textbf{Strong Law of Large Numbers (Grimmett and Stirzakker Theorem 7.5.1, 541A notes Theorem 2.11).}]\label{asym.lln2} Let \(\{X_k\}\) be a sequence of (i) independent (ii) identically distributed random variables. Then if and only if (iii) \(\E|X_k|  < \infty \),

\[
\frac{1}{n} \sum_{k=1}^n X_k \xrightarrow{a.s.} \mu
\]

\end{theorem}

\begin{theorem}\textbf{Strong Law of Large Numbers 1 (Kolmogorov) (Pesaran 8.8 Theorem 12).} Let \(\{X_k\}\) be a sequence of (i) independent random variables with (ii) \(\E(X_k) = \mu_k < \infty\) and (ii) \(\Var(X_k) =\sigma_k^2\) such that (iii)

\[
\sum_{k=1}^\infty \frac{\sigma_k^2}{k^2} < \infty
\]

Then \(\overline{X}_n - \overline{\mu}_n \xrightarrow{wp1} 0 \). If the independence assumption (i) is replaced by a lack of correlation (i.e. \(\Cov(X_k, X_j) = 0, k \neq j\)), the convergence of \(\overline{X}_n - \overline{\mu}_n\) with probability one requires the stronger condition

\[
\sum_{k=1}^\infty \frac{\sigma_k^2 (\log k)^2}{k^2} < \infty
\]
\end{theorem}

\begin{theorem}\textbf{Strong Law of Large Numbers 2 (Pesaran 8.8 Theorem 13)} Suppose that \(X_1, X_2, \ldots\) are (i) independent random variables, and that (ii) \(\E(X_k) = 0\), (iii) \(\E(X_k^4) \leq M \ \forall \ k\) where \(M\) is an arbitrary positive constant. Then 

\[
\overline{X}_n = \frac{1}{n} \sum_{k=1}^n X_k \xrightarrow{a.s.} 0
\] 

\end{theorem}

\begin{theorem} \label{asym.clt} \textbf{Central Limit Theorem (Grimmett and Stirzaker theorem 5.10.4.)} Let \(X_1, X_2, \ldots\) be a sequence of independent identically distributed random variables with finite mean \(\mu\) and finite non-zero variance \(\sigma^2\), and let \(S_n = \sum_{i=1}^n X_i\). Then

\[
\frac{S_n - n \mu}{\sqrt{n \sigma^2}} \xrightarrow{d} \mathcal{N}(0,1)
\]
\end{theorem}

\begin{theorem}\label{asym.berry.esseen} \textbf{(Berry-Esseen Central Limit Theorem.)} There exists \(c > 0\) such that the following holds. Let \(X_1, X_2, \ldots\) be i.i.d. real-valued random variables with mean zero, variance 1, and \(\E(|X_1|)^3 < \infty\). Let \(Z\) be a standard Gaussian random variable. Then for any \(n \geq 1\), 

\[
\sup_{t \in \mathbb{R}} \left| \Pr\bigg( \frac{X_1 + \ldots + X_n}{\sqrt{n}} \leq t \bigg) - \Pr( Z \leq t) \right| \leq c \cdot \frac{\E(|X_1|^3) }{\sqrt{n}}
\]

\end{theorem}

\begin{remark}You can look up what the \(c\) is; Heilman doesn't think it's any bigger than around 10.

\end{remark}

\begin{theorem} \label{asym.clt.vector} \textbf{(Central Limit Theorem in \(\mathbb{R}^d\), Heilman notes Theorem 2.33.)} Let \(X^{(1)}, X^{(2)}, \ldots\) be a sequence of independent identically distributed \(\mathbb{R}^d\)-valued random variables. (Notation: we write \(X^{(1)} = (X^{(1)}_1, \ldots, X^{(1)}_d)\).) Assume \(\E(X^{(n)} = \boldsymbol{\mu}\) for all \(n \geq 1\) and for any \(1 \leq i < j \leq d\), all of the covariances

\[
a_{ij} = \E\big[(X_i^{(1)} - \E(X_i^{(1)})(X_j^{(1)} - \E(X_j^{(1)}) \big]
\]

are finite. Let \(\boldsymbol{S}_n = \sum_{i=1}^n X^{(i)}\). Then as \(n \to \infty\), 

\[
\frac{\boldsymbol{S}_n - n \boldsymbol{\mu}}{\sqrt{n \sigma^2}} \xrightarrow{d} \mathcal{N}(\boldsymbol{\mu},[a_{ij}])
\]
\end{theorem}

\begin{theorem}\textbf{(Grimmett and Stirzaker theorem 5.10.5.)} Let \(X_1, X_2, \ldots\) be independent random variables satisfying \(\E(X_j) = 0\), \(\Var(X_j) = \sigma_j^2\), \(\E|X_j^3| < \infty\) such that

\[
\lim_{n \to \infty} \frac{1}{\sigma(n)^3} \sum_{j=1}^n \E|X_j^3| = 0
\]

where \(\sigma(n)^2 = \Var\big( \sum_{j=1}^n X_j \big) = \sum_{j=1}^n \sigma_j^2\). Then 

\[
\frac{1}{\sigma(n)} \sum_{j=1}^n X_j \xrightarrow{d} \mathcal{N}(0,1)
\]
\end{theorem}
\begin{proof}See \citet[p. 287]{loeve1977probability} and Grimmett and Stirzaker Problem 5.12.40.\end{proof}

\begin{lemma}\textbf{Lindeberg's Condition:}[\label{asym.lindeberg.cond} Let \(\{X_k\}\) be a sequence of independent (not necessarily identically distributed) random variables with expectations \(\mu_k\) and finite variances \(\sigma_k^2\). Let \(s_n^2 = \sum_{k=1}^n \sigma_k^2\). If such a sequence of independent random variables \(X_k\) satisfies the condition

\[
\lim_{n \to \infty} \frac{1}{s_n^2} \sum_{k=1}^n \E \big[(X_k - \mu_k)^2) \cdot \boldsymbol{1}_{\{|X_k - \mu_k| > \epsilon s_n\}} \big] = 0
\]

for all \(\epsilon > 0\) then the central limit theorem holds; that is, the random variables

\[
Z_n = \frac{1}{s_n} \sum_{k=1}^n(X_k - \mu_k)
\]

converge in distrbution to \(\mathcal{N}(0, 1)\) as \(n \to \infty\).
\end{lemma}

%%% Section 8.8
\subsection{The case of dependent and heterogeneously distributed observations (Pesaran 8.8)}


\begin{theorem}\label{asym.clt.mart.ds} \textbf{Central limit theorem for martingale difference sequences (Pesaran 8.8 Theorem 28).} Let \(\{x_t\}\) be a martingale difference sequence with respect to the information set \(\Omega_t\). Let \(\overline{\sigma}_T^2 = \Var( \sqrt{T} \overline{x}_T) = T^{-1} \sum_{t=1}^T \sigma_t^2\). If \(\E(|x_t|^r) < K < \infty\) for any \(r > 2\) and for all \(t\), and

\[
\frac{1}{T} \sum_{t=1}^T x_t^2 - \overline{\sigma}_T^2 \xrightarrow{p} 0
\]

then \(\sqrt{T} \overline{x}_T / \overline{\sigma}_T \xrightarrow{d} \mathcal{N}(0, 1)\).
\end{theorem}

\subsection{Worked Examples from Math 505A Midterm 2}

\begin{enumerate}[(1)]

%%%%%%%%%%%% Midterm question 1 %%%%%%%%%%%%
\item

%\textbf{Hints/Notes about this question:} Calculus limit (calculating limit). Law of large numbers/central limit theorem. We did this problem in class \textbf{(true for hw6 q1: went over part (a) on 10/31, p. 15 of notes)}. \textbf{Very likely: Homework 6 Question 1 (Fall 2010 qual, question 1)}.

%\textbf{Question:}

\begin{enumerate}[(a)]

\item  \textbf{Fall 2010 Problem 1.} Let \(X_k\), \(k \geq 1\), be i.i.d. random variables with mean 1 and variance 1. Show that the limit

\[
\lim_{n \to \infty} \frac{\sum_{k=1}^n X_k}{\sum_{k=1}^n X_k^2} 
\]

exists in an appropriate sense, and identify the limit.

\item \textbf{Not included on midterm or final.} Let \((X_j)_{j \geq 1}\) be i.i.d. uniform on \((-1, 1)\). Let 

\[
Y_n = \frac{\sum_{j=1}^n X_j}{\sum_{j=1}^n X_j^2 + \sum_{j=1}^n X_j^3} 
\]

Prove that \(\lim_{n \to \infty} \sqrt{n} Y_n\) exists in an appropriate sense, and identify the limit.

\end{enumerate}

\textbf{Solution.} \begin{enumerate}[(a)]

% 1a
\item 

%\textbf{on page 15 on notes, from 10/31}

\[
\lim_{n \to \infty} \frac{\sum_{k=1}^n X_k}{\sum_{k=1}^n X_k^2} = \lim_{n \to \infty} \frac{n^{-1}\sum_{k=1}^n X_k}{n^{-1}\sum_{k=1}^n X_k^2}
\]

Since \(X_1, X_2, \ldots\) are i.i.d., \(E(X_1^2) = \Var(X_1) + (\E(X_1))^2 = 2 < \infty\), we have \[ n^{-1}\sum_{k=1}^n X_k \xrightarrow{a.s.} \E(X_1) = 1 \text{ as } n \to \infty\] by Theorem \ref{asym.lln1} (Strong Law of Large Numbers). Also, \(X_1^2, X_2^2, \ldots\) are clearly identically distributed, and are independent by Theorem 4.2.3 (``If \(X\) and \(Y\) are independent, then so are \(g(X)\) and \(g(Y)\)."). It is clear also that \(\E(|X_1^2|) = \E(X_1^2) = \Var(X_1) + \E(X_1)^2 = 1 + 1 = 2 < \infty\). Therefore by Theorem \ref{asym.lln2} (Strong Law of Large Numbers),  

\[
 n^{-1}\sum_{k=1}^n X_k^2 \xrightarrow{a.s.}\E(X_1^2) = 2 \text{ as } n \to \infty
 \]

(From here I had two different ways of finishing the problem.)

\begin{itemize}

\item 
 
Because we have almost sure convergence in the numerator and denominator, by the Continuous Mapping Theorem (Theorem \ref{asym.contmappthm}),
 
 \[
 \lim_{n \to \infty} \frac{n^{-1}\sum_{k=1}^n X_k}{n^{-1}\sum_{k=1}^n X_k^2} =  \frac{\lim_{n \to \infty} n^{-1}\sum_{k=1}^n X_k}{\lim_{n \to \infty} n^{-1}\sum_{k=1}^n X_k^2} \xrightarrow{a.s.} \boxed{\frac{1}{2}}
 \]

\item 

Then, using one of  Slutsky's convergence theorems (Theorem \ref{asym.slutsky}: ``If \(x_t \xrightarrow{d} x\) and \(y_t \xrightarrow{p} c\) where \(c\) is a finite constant, then \(x_t/y_t \xrightarrow{d}  x/c, \text{ if } c \neq 0\)."), we have

%Then by Theorem 7.2.18, which states the following:
\[
\frac{n^{-1}\sum_{k=1}^n X_k}{n^{-1}\sum_{k=1}^n X_k^2} \xrightarrow{d} \frac{\E(X_1)}{\E(X_1^2)} =  \frac{\E(X_1)}{\Var(X_1) + \E(X_1)^2}  =  \frac{1}{1+1} = \frac{1}{2}
\]

But then, by Theorem \ref{asym.7.2.4} (Theorem 7.2.4(a) in Grimmett and Stirzaker: ``If \(X_n \xrightarrow{d} c\) where \(c\) is constant, then \(X_n \xrightarrow{p} c\)."), we have \(\frac{n^{-1}\sum_{k=1}^n X_k}{n^{-1}\sum_{k=1}^n X_k^2} \xrightarrow{p} 1/2\). 

\end{itemize}

% 1b
\item \textbf{(Not included on midterm or final.)}

\[
Y_n = \frac{\sum_{j=1}^n X_j}{\sum_{j=1}^n X_j^2 + \sum_{j=1}^n X_j^3} =  \frac{n^{-1}\sum_{j=1}^n X_j}{n^{-1}\sum_{j=1}^n X_j^2 + n^{-1}\sum_{j=1}^n X_j^3} 
\]

Note that \(\E(X_1) = 0, \E(X_1^2) = \Var(X_1) + \E(X_1)^2 = (1 - -1)^2/12 + 0^2 = 1/3 , \E(X_1^3) = (1/2) \int_{-1}^1 x^3 dx = 0 \). (We derived the formulae for the first three moments of a uniform distribution on Homework 4 problem 2(2).) 

\[
\implies \sqrt{n} Y_n =  \frac{\sqrt{1/3} \big(\sum_{j=1}^n X_j - n\E(X_1) \big)/ \sqrt{n \cdot1/3} }{n^{-1}\sum_{j=1}^n X_j^2 + n^{-1}\sum_{j=1}^n X_j^3} 
\]
 
By the Central Limit Theorem (Theorem \ref{asym.clt}),
 
\[
 \frac{\sum_{j=1}^n X_j - n\E(X_1) }{\sqrt{n \cdot1/3} } \xrightarrow{d} \mathcal{N}(0, 1)
\]

By the Law of Large Numbers (Theorem \ref{asym.lln2}), since \(\E(|X_1^2|) = \E(X_1^2) = 1/3 < \infty\),

\[
\frac{1}{n}\sum_{j=1}^n X_j^2 \xrightarrow{a.s.} \E(X_1^2) =   1/3
\]

By the Law of Large Numbers (Theorem \ref{asym.lln2}), since \(\E(|X_1^3|) = (1/2) \int_{-1}^1 |x^3| dx = \int_0^1 x^3 dx = 1/4 < \infty\),

\[
\frac{1}{n}\sum_{j=1}^n X_j^3 \xrightarrow{a.s.} \E(X_1^3) =   0
\]

In the denominator, since we have almost sure convergence, the regular rules of calculus/real analysis apply. That is, using the above results,

\[
n^{-1}\sum_{j=1}^n X_j^2 + n^{-1}\sum_{j=1}^n X_j^3 \xrightarrow{a.s.} 1/3
\]

Therefore

\[
\sqrt{n} Y_n =  \frac{\sqrt{1/3} \big(\sum_{j=1}^n X_j - n\E(X_1) \big)/ \sqrt{n \cdot1/3} }{n^{-1}\sum_{j=1}^n X_j^2 + n^{-1}\sum_{j=1}^n X_j^3} \xrightarrow{d} \frac{\sqrt{1/3} }{1/3} \mathcal{N}(0, 1) = \boxed{\mathcal{N}(0, 3)}
\]

\end{enumerate}

%%%%%%%%%%%% Midterm question 2 %%%%%%%%%%%%
\item 

%\textbf{Hints/Notes about this question:} Has to do with generating functions and Taylor series. This is the only nontrivial generating function problem. We either did this problem in class or discussed one like it. \textbf{Guess: HW 6 question 4(a) (went over on 11/02, p. 17 of notes; Fall 2010 qual, question 2)? Melike solution involves generating functions; probably the one I should use.}

\textbf{Fall 2010 Problem 2.} Fix \(p \in (0, 1)\) and consider independent Poisson random variables \(X_k, k \geq 1\) with

\[
\E X_k = \frac{p^k}{k}
\]

Verify that the sum \(\sum_{k=1}^\infty k X_k\) converges with probability one and determine the distribution of the random variable \(Y = \sum_{k=1}^\infty k X_k\).

\textbf{Solution.} %%%%%%%%%% Melike's solution (use for midterm) %%%%%%%%%%%%%
\textbf{Melike's solution (use for midterm):} We have \(\E[k X_k] = p^k\) and \(\sum_{k=1}^\infty p^k = p/(1-p) < \infty\), and \(\Var(k X_k) = kp^k\) and 

\[
\sum_{k=1}^\infty k p^k= p\sum_{k=1}^\infty k p^{k-1} = p \deriv{}{p}\sum_{k=1}^\infty  p^k = p \deriv{}{p}\frac{p}{1-p}= p \cdot \frac{(1-p) - p(-1)}{(1-p)^2} = \frac{p}{(1-p)^2} < \infty
\]

Since the sequence \(\{Y_k|\}_{k \geq 1}\) is independent, by Kolmogorov's Two Series Theorem (Theorem \ref{asym:k2st}: ``Let \(X_1, X_2, \ldots\) be independent random variables with \(\E(X_n) = \mu_n\) and \(\Var(X_n) = \sigma_n^2\) such that \(\sum_{n=1}^\infty \mu_n < \infty\) and \(\sum_{n=1}^\infty \sigma_n^2 < \infty\). Then \(\sum_{n=1}^\infty X_n\) converges in \(\mathbb{R}\) almost surely."), we conclude that \(\sum_{k=1}^\infty k X_k\) converges almost surely. 

To find the distribution of \(Y\), let \(X\) be a Poisson random variable and consider its probability generating function:

\[
G_X(s) = \E(s^X) = \sum_{k=0}^\infty s^k e^{- \lambda} \frac{\lambda^k}{k!} = e^{-\lambda} \sum_{k=0}^\infty \frac{(\lambda s)^k}{k!} = e^{-\lambda}e^{\lambda s} = e^{\lambda(s-1)}
\]

So \(\E(s^{X_k}) = \exp \bigg( \frac{p^k}{k}(s-1) \bigg)\) and \(\E(s^{kX_k}) = \E[(s^k)^{X_k}] = \exp\bigg( \frac{p^k}{k}(s^k - 1) \bigg)\). Then define \(Y_n = \sum_{k=1}^n k X_k\) and consider

\[
G_{Y_n}(s) = \E(s^{Y_n}) = \E\bigg( \prod_{k=1}^n s^{kX_k}\bigg) = \prod_{k=1}^n\E\big(  s^{kX_k}\big) = \prod_{k=1}^n \exp \bigg( \frac{p^k}{k} (s^k - 1)\bigg) = \exp \bigg( \sum_{k=1}^n  \frac{p^k}{k} (s^k - 1)\bigg) 
\]

\[
= \exp \bigg( \sum_{k=1}^n  \frac{(ps)^k}{k} -  \sum_{k=1}^n  \frac{p^k}{k} \bigg)
\]

Now, by taking limits as \(n \to \infty\) (since we are allowed to take limit inside of expectation here), we get

\[
G_Y(s) = \E(s^Y) = \exp \bigg( \sum_{k=1}^\infty  \frac{(ps)^k}{k} -  \sum_{k=1}^\infty  \frac{p^k}{k} \bigg) = \exp \bigg( \int \sum_{k=1}^\infty (ps)^{k-1} dp -  \int \sum_{k=1}^\infty  p^{k-1} dp \bigg) 
\]

\[
= \exp \bigg( \int \frac{1}{1-ps} dp-  \int \frac{1}{1-p} dp \bigg) = \exp (-\log(1 - ps) + \log(1 - p)), \ \ -1 \leq ps < 1 \text{ and } -1 \leq p < 1
\]

\[
 = \frac{1 - p}{1 - ps}, \ \ \ -1 \leq ps < 1
\]

Since we know \(\Pr(X =k) = \frac{G_X^{(k)}(0)}{k!} \), we have

\[
G_Y(s) = \frac{1-p}{1 -sp}, \ G'(s) = \frac{p(1-p)}{(1-sp)^2}, \ G''(s) = \frac{2p^2(1-p)}{(1-sp)^3}, \ G^{(3)}(s) = \frac{3 \cdot 2p^3(1-p)}{(1 - sp)^3}, \ldots
\]

\[
G^{(k)}(s) = \frac{k!p^k (1-p)}{(1-sp)^k} \text{ for } k = 0, 1, 2, \ldots
\]

So we have

\[
\Pr(Y=k) = (1-p)p^k, \ \ k= 0, 1, 2, \ldots
\]

\[
= \Pr(G_1(1-p) = k +1) = \Pr(G_1(1-p) - 1 = k)
\]

which means \(Y \sim G_1(1-p) - 1\).


%%%%%%%%%%%% Midterm question 4 %%%%%%%%%%%%
\item \textbf{Spring 2017 Problem 3.}

%\textbf{Hints/Notes about this question:} Almost sure convergence. Computation. We did this problem in class \textbf{(true for hw6 q6: went over on 11/07, p. 22 of notes)}. \textbf{Very likely: Homework 6 Question 6(a) (Spring 2017 qual, question 3.}

%\textbf{Question:}

\begin{enumerate}[(a)]

\item Consider the sequence \(\{X_k, k \geq 1\}\) of random variables such that \(X_1\) is uniform on \((0, 1)\) and, given \(X_k\), the distribution of \(X_{k+1}\) is uniform on \((0, CX_k)\), where \(\sqrt{3} < C < 2\).

\begin{enumerate}[(i)]

\item For \(n \geq 1\), compute the conditional expectation \(\E(X_{n+1}^r \mid X_n)\).

\item For \(n \geq 1\), compute \(\E(X_n^r)\).

\item Show that \(\lim_{x \to \infty} X_n = 0\) in \(\ell_1\) and with probability one, but not in \(\ell_2\). 

\item Investigate the same questions for all other values of \(C > 0\).

\end{enumerate}

\item Let \(a > 0\), let \(X_n, n \geq 1\) be i.i.d. random variables that are uniform on \((0, a)\), and let \(Y_n = \prod_{k=1}^n X_k\). Determine, with a proof, all values of \(a\) for which \(\lim_{n \to \infty} Y_n = 0\) with probability one.

\end{enumerate}

\textbf{Solution.} \begin{enumerate}[(a)]

% 6a
\item 

\begin{enumerate}[(i)]

% 6a(i)
\item We have that \(X_{n+1} \mid X_n \sim U(0, C X_n)\). Therefore

\[
\E(X_{n+1}^r \mid X_n) = \frac{1}{C X_n} \int_{0}^{CX_n} x^r dx = \frac{1}{CX_n} \cdot \frac{x^{r+1}}{r+1} \bigg|_0^{CX_n} = \frac{C^rX_n^r}{r+1}
\]

\[
\implies \E(X_{n+1}^r) = \E[\E(X_{n+1}^r \mid X_n)] = \frac{C^r}{r+1} \cdot \E( X_n^r)
\]

\[
\implies \boxed{ \E(X_{n+1}^r \mid X_n) = \frac{C^r}{r+1} X_n^r}
\]

\item Note that \(E(X_1^r) = \int_0^1 x^r dr = 1/(r+1)\). Therefore 

\[
\E(X_{n+1}^r) = \frac{C^r}{r+1} \cdot \E( X_n^r) = \bigg( \frac{C^r}{r+1}\bigg)^n \cdot \E(X_1^r) =  \boxed{\bigg( \frac{C^r}{r+1}\bigg)^n \cdot \frac{1}{r+1}}
\]

\item We would like to show that \(X_n \xrightarrow{w.p.1} 0\) and that \(X_n \xrightarrow{1} 0\), but that the same result does not follow for the \(\ell_2\) norm.

\begin{itemize}

\item \textbf{Convergence with probability one:} We seek to show that \(\Pr\big( \{\omega \in \Omega: \lim_{n \to \infty} X_n(\omega) = 0 \} \big) = 1\). By Markov's Inequality (Lemma \ref{asym.markov}), we have

\[
\Pr(|X_n| \geq a) \leq \frac{\E(X_n)}{a} \ \ \forall \ a > 0
\]

\[
\iff \Pr(|X_n| \geq a) \leq \bigg( \frac{C^1}{1+1}\bigg)^{n-1} \cdot \frac{1}{1+1} \cdot \frac{1}{a} =   \bigg( \frac{C}{2}\bigg)^{n-1} \cdot \frac{1}{2a}  \ \ \forall \ a > 0
\]
% = \frac{C^{n-1}}{2^na} 

Since \(\sqrt{3} < C < 2\), \(\sqrt{3}/2 < C/2 < 1\). Since \(X_n \in [0, C X_{n-1}]\), \(X_n \geq 0\), so \(|X_n| = X_n\). Therefore we have

\[
\Pr(\lim_{n \to \infty} |X_n| \geq a) = \Pr(\lim_{n \to \infty} X_n \geq a) \leq \lim_{n \to \infty}  \bigg( \frac{C}{2}\bigg)^{n-1} \cdot \frac{1}{2a} =  0 \ \ \forall \ a > 0
\]

Since \(|X_n| \geq 0\), this implies that \(\Pr(\lim_{n \to \infty} X_n = 0) = \Pr\big( \{\omega \in \Omega: \lim_{n \to \infty} X_n(\omega) = 0 \} \big) = 1\), so by the Borel-Cantelli Lemma (Theorem \ref{asym.borel}), \(X_n\) converges to 0 with probability 1.

\item \textbf{Convergence in \(\ell_1\) norm:} We seek to show that \(\lim_{n \to \infty} \E(|X_n|) = 0\). Since \(X_n \in [0, C X_{n-1}]\), \(X_n \geq 0\), so \(|X_n| = X_n\). Therefore

\[
\lim_{n \to \infty} \E(|X_n|) = \lim_{n \to \infty} \E(X_n) = \lim_{n \to \infty} \bigg( \frac{C}{2}\bigg)^{n-1} \cdot \frac{1}{2}
\]

Since \(\sqrt{3} < C < 2\), \(\sqrt{3}/2 < C/2 < 1\), so \(C/2 < 1\). Therefore we have 

\[
\lim_{n \to \infty} \E(|X_n|)= \lim_{n \to \infty} \bigg( \frac{C}{2}\bigg)^{n-1} \cdot \frac{1}{2} = 0
\]

so \(X_n\) converges to 0 in 1st mean.

\item \textbf{Convergence in \(\ell_2\) norm:} We seek to show that \(\lim_{n \to \infty} \E(|X_n|^2) \neq 0\). We have

\[
\lim_{n \to \infty} \E(|X_n|^2) = \lim_{n \to \infty} \E(X_n^2) = \lim_{n \to \infty} \bigg( \frac{C^2}{3}\bigg)^{n-1} \cdot \frac{1}{3}
\]

Since \(\sqrt{3} < C < 2\), \(3/3 < C^2/3 < 4/3\), so \(C^2/3 > 1\). Therefore we have 

\[
\lim_{n \to \infty} \E(|X_n|^2) = \lim_{n \to \infty} \bigg( \frac{C^2}{3}\bigg)^{n-1} \cdot \frac{1}{3} = \infty \neq 0
\]

so \(X_n\) does not converge to 0 in 2nd mean.

\end{itemize} 

% 6a(ii)
\item From the above, it is clear that for convergence with probability one or in 1st mean we require \(0 < C/2 < 1\) and for convergence in second mean we require \(0 < C^2/3 < 1\).  For \(0 < C < \sqrt{3}\), we see that \(X_n\) would converge to zero in 2nd mean since this would imply that  \(0 < C^2/3 < 1\). It would also still converge to 0 in 1st mean (and with probability 1) since we would have \((0 < C/2 < \sqrt{3}/2 < 1\). 

For \(C = \sqrt{3}\), \(X_n\) would still converge to 0 with probability one and in 1st mean for the same reasons. However, it would not converge in 2nd mean because we would have

\[
\lim_{n \to \infty} \E(|X_n|^2) = \lim_{n \to \infty} \bigg( \frac{\sqrt{3}^2}{3}\bigg)^{n-1} \cdot \frac{1}{3} = \frac{1}{3} \neq 0
\]

For \(C \geq 2\), it would diverge in all three cases, since in this case \(C/2 \geq 2/2 = 1\) and \(C^2/3 \geq 4/3 > 1\).

\end{enumerate}

% 6b
\item \textbf{Probably won't be on midterm.} Note that 

\[
\lim_{n \to \infty} Y_n = \lim_{n \to \infty} \prod_{k=1}^n X_k = 0 \iff \log(Y_n) = \log \bigg( \prod_{k=1}^n X_k\bigg) = \sum_{k=1}^n \log (X_k) \to -\infty
\]

Note that

\[
\E[ \log(Y_n)] = \E \bigg(  \sum_{k=1}^n \log (X_k) \bigg) = \sum_{k=1}^n \E[\log(X_k)] = \sum_{k=1}^n \E[\log(X_1)] = \sum_{k=1}^n \int_0^a ( \log(x) /a) dx
\]

\[
=  \sum_{k=1}^n \frac{1}{a} \big[x \log x - x \big]_0^a =  \sum_{k=1}^n \frac{a \log a - a}{a} =  \sum_{k=1}^n ( \log(a) - 1) =n(\log(a) - 1)
\]

As \(n \to \infty\) we have

\[
\E[ \log(Y_n)]  = \begin{cases}
-\infty & a < e \\ 
0 & a = e \\ 
\infty & a > e 
\end{cases}
\]

Since \(\E[ \log(Y_n)] \to \infty\) for \(a < e\), we have \(\lim_{n \to \infty} Y_n = 0\) for \(a < 3\). Therefore \[\boxed{\lim_{n \to \infty}Y_n = \lim_{n \to \infty} \prod_{k=1}^n X_k = 0 \iff a < e.}\]

%The \(a = e\) case is the most interesting. 

\end{enumerate}

\end{enumerate}

\subsection{Estimators and Central Limit Theorems (DSO 607)}

Lyapunov (?) condition: can prove central limit theorem if we check 3rd moment. Lindeberg's Condition (Lemma \ref{asym.lindeberg.cond})

%
%
%
%
%
%
%
%

%\bibliographystyle{abbrvnat}
%\bibliography{mybib2fin}
%\end{document}