%\documentclass{article}
%
%\usepackage{fancyhdr}
%\usepackage{extramarks}
%\usepackage{amsmath}
%\usepackage{amsthm}
%\usepackage{amsfonts}
%\usepackage{tikz}
%\usepackage{enumerate}
%\usepackage{graphicx}
%\graphicspath{ {images/} }
%\usepackage[plain]{algorithm}
%\usepackage{algpseudocode}
%\usepackage[document]{ragged2e}
%\usepackage{textcomp}
%\usepackage{color}   %May be necessary if you want to color links
%\usepackage{import}
%\usepackage{hyperref}
%\hypersetup{
%    colorlinks=true, %set true if you want colored links
%    linktoc=all,     %set to all if you want both sections and subsections linked
%    linkcolor=black,  %choose some color if you want links to stand out
%}
%
%\usetikzlibrary{automata,positioning}
%
%
%% Basic Document Settings
%
%
%\topmargin=-0.45in
%\evensidemargin=0in
%\oddsidemargin=0in
%\textwidth=6.5in
%\textheight=9.0in
%\headsep=0.25in
%\setlength{\parskip}{1em}
%
%\linespread{1.1}
%
%\pagestyle{fancy}
%\lhead{\hmwkAuthorName}
%\lfoot{\lastxmark}
%\cfoot{\thepage}
%
%\renewcommand\headrulewidth{0.4pt}
%\renewcommand\footrulewidth{0.4pt}
%
%\setlength\parindent{0pt}
%
%
%\newcommand{\hmwkTitle}{Math Review Notes---Mathematical Statistics}
%\newcommand{\hmwkAuthorName}{\textbf{G. Faletto} }
%
%
%%%%%% Title Page
%
%
%\title{
%    \vspace{2in}
%    \textmd{\textbf{ \hmwkTitle}}\\
%}
%
%\author{Gregory Faletto}
%\date{}
%
%\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}
%
%
%%%%%% Various Helper Commands
%
%
%%%%%% Useful for algorithms
%\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}
%
%%%%%% For derivatives
%\newcommand{\deriv}[2]{\frac{\mathrm{d} #1}{\mathrm{d} #2}}
%
%%%%%% For partial derivatives
%\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
%
%%%%%% Integral dx
%\newcommand{\dx}{\mathrm{d}x}
%
%%%%%% Alias for the Solution section header
%\newcommand{\solution}{\textbf{\large Solution}}
%
%%%%%% Probability commands: Expectation, Variance, Covariance, Bias
%\newcommand{\E}{\mathbb{E}}
%\newcommand{\Var}{\mathrm{Var}}
%\newcommand{\Cov}{\mathrm{Cov}}
%\newcommand{\Bias}{\mathrm{Bias}}
%\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
%\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
%\DeclareMathOperator{\Tr}{Tr}
%
%\theoremstyle{definition}
%\newtheorem{theorem}{Theorem}
%\theoremstyle{definition}
%\newtheorem{proposition}[theorem]{Proposition}
%\theoremstyle{definition}
%\newtheorem{lemma}[theorem]{Lemma}
%\theoremstyle{definition}
%\newtheorem{corollary}{Corollary}[theorem]
%\theoremstyle{definition}
%\newtheorem{definition}{Definition}[section]
%\newtheorem*{remark}{Remark}
%\theoremstyle{definition}
%\newtheorem{exercise}{Exercise}
%\theoremstyle{definition}
%\newtheorem{example}{Example}[section]
%
%%%%%% Tilde
%\newcommand{\textapprox}{\raisebox{0.5ex}{\texttildelow}}
%
%\begin{document}
%
%\maketitle
%
%\pagebreak
%
%\tableofcontents
%
%\
%
%\
%
%\begin{center}
%Last updated \today
%\end{center}
%
%
%
%\newpage
%%
%%%
%%%
%%%
%%%
%%%
%%%
%%%
%%%
%%%%
%%%% Mathematical Statistics

\section{Mathematical Statistics}

These are my notes from taking Math 541A at USC taught by Steven Heilman as well as \textit{Statistical Inference} (2nd edition) by Casella and Berger, Statistics 100B at UCLA taught by Nicolas Christou, ISE 620 at USC taught by Sheldon Ross, and a few other sources I cite within the text.

\subsection{Order Statistics (ISE 620)}

\begin{definition}[\textbf{Order statistics (from Math 541A, more precise)}] Let \(X:\Omega\to\mathbb{R}\) be a random variable.  Let $X_{1},\ldots,X_{n}$ be a random sample of size $n$ from $X$.  Define $X_{(1)}:=\min_{1\leq i\leq n}X_{i}$, and for any $2\leq i\leq n$, inductively define
$$X_{i}:=\min\Big\{\{X_{1},\ldots,X_{n}\}\setminus\{X_{(1)},\ldots,X_{(i-1)}\}\Big\},$$
so that
$$X_{(1)}\leq X_{(2)}\leq\cdots\leq X_{(n)}=\max_{1\leq i\leq n}X_{i}.$$
The random variables $X_{(1)},\ldots,X_{(n)}$ are called the \textbf{order statistics} of $X_{1},\ldots,X_{n}$.


\end{definition}

\begin{definition}[\textbf{Order statistics (from ISE 620, more informal)}] Let \(X_1, \ldots X_n \sim iid \ \ F\) with \(F' = f\). Define \(X_{(1)}\) as the smallest among \(X_1, \ldots, X_n\), \(X_{(2)}\) as the 2nd smallest, and so on, up to \(X_{(n)}\), the largest of the group. We call \(X_{(1)}, \ldots, X_{(n)}\) the \textbf{order statistics} of \(X_1, \ldots, X_n\).

\end{definition}

\begin{proposition}[\textbf{Order statistics distribution function; from Math 541A}]\label{mathstats.order.stats.dist} Suppose $X$ is a discrete random variable and we can order the values that $X$ takes as $x_{1}<x_{2}<\cdots$.  For any $i\geq1$, define $p_{i}:= \Pr(X\leq x_{i})$. Then for any $1\leq i,j\leq n$,
$$\Pr(X_{(j)}\leq x_{i})=\sum_{k=j}^{n}\binom{n}{k}p_{i}^{k}(1-p_{i})^{n-k}.$$
\end{proposition}

\begin{proof} Note that \(\{X_{(j)} \leq x_i\}\) is equivalent to the event that \(j\) or more of the \(X_i\) are less than or equal to \(x_i\) regardless of order; that is, \(x_i\) is the \(k\)th smallest observed value. Let \(A_k\) be the event that exactly \(k\) of the \(X_i\) are less than or equal to \(x_i\) regardless of order. Then

\[
\{X_{(j)} \leq x_i\} = \bigcup_{k=j}^n A_k.
\]

Then since (by definition of \(p_i\))

\[
\Pr(A_k) = \binom{n}{k} p_i^k(1-p_i)^{n-k}
\]

and using the fact that the \(\{A_k\}\) are disjoint, we have

\[
\Pr(\{X_{(j)} \leq x_i\}) = \Pr(\bigcup_{k=j}^n A_k) = \sum_{k=1}^n \Pr(A_k) = \sum_{k=1}^n \binom{n}{k} p_i^k(1-p_i)^{n-k}.
\]

\end{proof}

\begin{corollary}\label{mathstats.order.stats.density.541a} if $X$ is a continuous random variable with density $f_{X}$ and cumulative distribution function $F_{X}$, then for any $1\leq j\leq n$, $F_{X_{(j)}}$ has density
$$f_{X_{(j)}}(x):=\frac{n!}{(j-1)!(n-j)!}f_{X}(x)(F_{X}(x))^{j-1}(1-F_{X}(x))^{n-j},\qquad\forall\,x\in\mathbb{R}.$$

\end{corollary}

\begin{proof} This follows by differentiating the identity from Proposition \ref{mathstats.order.stats.dist} for the cumulative distribution function.

\end{proof}

\begin{proposition}[\textbf{Order statistics joint density function; result from ISE 620}]\label{mathstats.order.stats.density} The joint density of the order statistics is

\[
f_{X_{(1)}, \ldots, X_{(n)}}(x_1, \ldots, x_n) = n! \prod_{i=1}^n f(x_i).
\]

\end{proposition}

\begin{proof} Start with \(n=2\). We seek \(f_{X_{(1)}, X_{(2)}}(x_1,  x_2) \). Note that \(X_{(1)} = x_1, X_{(2)} = x_2\) if \(X_1 = x_1, X_2 = x_2\) or if \(X_1 = x_2, X_2 = x_1\). These are mutually exclusive events, so their density is equal to the sums of the two densities. That is,

\[
f_{X_{(1)}, X_{(2)}}(x_1,  x_2) = f_{X_1, X_2} (x_1, x_2) +  f_{X_1, X_2} (x_2, x_1) = 2 f(x_1) f(x_2)
\]

where the last step follows from the i.i.d. distributions. Generalizing, we have

\[
f_{X_{(1)}, \ldots, X_{(n)}}(x_1, \ldots, x_n) = n! \prod_{i=1}^n f(x_i).
\]

\end{proof}

\begin{proposition}[\textbf{Distribution of order statistics of uniform random variable; from 541A}]\label{mathstats.orders.unif.dist}Let $X$ be a random variable uniformly distributed in $[0,1]$. Then for any $1\leq j\leq n$, $X_{(j)}$ is a beta distributed random variable with parameters $j$ and $n-j$. 
\end{proposition}

\begin{proof} Note that for a uniform distribution on \([0,1]\), \(f_X(x) = 1, x \in [0,1]\) and \(F_X(x) = x, x \in [0,1]\). Therefore by Corollary \ref{mathstats.order.stats.density.541a} we have

\[
f_{X_{(j)}}(x) =\frac{n!}{(j-1)!(n-j)!} (x)^{j-1}(1-x)^{n-j},\qquad x\in [0,1]
\]

\[
 =\frac{\Gamma(n+1)}{\Gamma(j) \Gamma(n+1-j)} x^{j-1}(1-x)^{n-j}  =\frac{\Gamma(j + n + 1 - j)}{\Gamma(j) \Gamma(n + 1 - j)} x^{j-1}(1-x)^{n+1-j - 1}
 \]


which is the pdf for a beta distribution with parameters \(j\) and \(n + 1 -j\). 

\end{proof}

\begin{corollary}Let $X$ be a random variable uniformly distributed in $[0,1]$. Then \(\E X_{(j)}=\frac{j}{n+1}\)

\end{corollary}

\begin{proof} Follows from Proposition \ref{mathstats.orders.unif.dist} since the mean of such a beta distribution is \(\frac{j}{n+1}\).

\end{proof}

\begin{proposition}[\textbf{Result from 541A}] Let $a,b\in\mathbb{R}$ with $a<b$.  Let $U$ be the number of indices $1\leq j\leq n$ such that $X_{j}\leq a$.  Let $V$ be the number of indices $1\leq j\leq n$ such that $a<X_{j}\leq b$.  Then the vector $(U,V,n-U-V)$ is a multinomial random variable, so that for any nonnegative integers $u,v$ with $u+v\leq n$, we have
    \begin{flalign*}
    &\P(U=u,V=v,n-U-V=n-u-v)\\
    &\qquad=\frac{n!}{u!v!(n-u-v)!}F_{X}(a)^{u}(F_{X}(b)-F_{X}(a))^{v}(1-F_{X}(v))^{n-u-v}.
    \end{flalign*}
    Consequently, for any $1\leq i,j\leq n$,
    $$\P(X_{(i)}\leq a,X_{(j)}\leq b)=\P(U\geq i,U+V\geq j)=\sum_{k=i}^{j-1}\sum_{m=j-k}^{n-k}\P(U=k,V=m)+\P(U\geq j).$$
    So, it is possible to write an explicit formula for the joint distribution of $X_{(i)}$ and $X_{(j)}$.
    
    \end{proposition}
    
    \begin{proof} We can define a multinomial distribution as follows (from Sheldon Ross \textit{Stochastic Processes}, see Definition \ref{prob.ross.multinomial.dist.def}): ``Suppose that \(n\) independent trials, each of which results in either outcome \(1, 2, \ldots, r\) with respective probabilities \(p_1, p_2, \ldots, p_r\) (with \(\sum_{i} p_i = 1\)), are performed. Let \(N_i\) denote the number of trials resulting in outcome \(i\). Then the joint distribution of \(N_1, \ldots, N_r\) is called the \textbf{multinomial distribution.}" In this case \(r=3\). If we define outcome 1 to be \(X_j \leq a\), outcome 2 to be \(a < X_j \leq b\), and outcome 3 to be \(X_j > b\), then the counts \((U, V, n-U-V)\) meet this definition exactly, with \(p_1 = \Pr(X_j \leq a) = F_X(a)\), \(p_2 = \Pr(a < X_j \leq b) = F_X(b) - F_X(a)\), \(p_3 = \Pr(X_j > b) = 1 - F_X(b)\). Since the pmf of a multinomial distribution with \(r=3\) is 

\[
\Pr((N_1, N_2, N_3) = (n_1, n_2, n_3) ) = \binom{n}{n_1, n_2, n_3} p_1^{n_1}  p_2^{n_2}  p_3^{n_3 }  = \frac{n!}{n_1! n_2! n_3!}  p_1^{n_1}  p_2^{n_2}  p_3^{n_3 }
\]

we have in this case

\[
\Pr (U=u,V=v,n-U-V=n-u-v) =\frac{n!}{u!v!(n-u-v)!}F_{X}(a)^{u}(F_{X}(b)-F_{X}(a))^{v}(1-F_{X}(v))^{n-u-v}
    \]
    
    as desired.
    
    \end{proof}

\subsection{Random Samples}

\begin{definition}[\textbf{Random Sample}] Let \(n >0\) be an integer.  A \textbf{random sample} of size \(n\) is a sequence \(X_1, \ldots, X_n\) of independent identically distributed random variables.

\end{definition}

\begin{definition}[\textbf{Statistic}] Let \(n, k\) be positive integers. Let \(X_1, \ldots, X_n\) be a random sample of size \(n\). Let \(f: \mathbb{R}^n \to \mathbb{R}^k\) be a (measureable) function. A \textbf{statistic} is a random variable of the form \(Y:= f(X_1, \ldots, X_n)\). The distribution of \(Y\) is called a \textbf{sampling distribution.}

\end{definition}

\begin{definition}[\textbf{Sample mean}] The \textbf{sample mean} of a random sample \(X_1, \ldots, X_n\) of size \(n\), denoted \(\overline{X}\), is the following statistic:

\[
\overline{X} := \frac{1}{n} \sum_{i=1}^n X_i.
\]

\end{definition}

\begin{proposition}\label{mathstats.sample.mean.props} Suppose we have a random sample of size \(n\) from an i.i.d. distribution \(X_1, X_2, \ldots, X_n\) with \(\E(X_1) = \mu \ in \mathbb{R}\), \(\Var(X_1) = \sigma^2 < \infty\). Then

\begin{enumerate}[(a)]

\item \(\E(\overline{X}) = \E(X_1)\).

\item \( \Var(\overline{X}) = \sigma^2/n\).

\end{enumerate}

\end{proposition}

\begin{proof}

\begin{enumerate}[(a)]

\item

\[
\E(\overline{X}) = \E \bigg( \frac{1}{n} \sum_{i=1}^n X_i\bigg) = \frac{1}{n} \sum_{i=1}^n \E   X_i  = \frac{1}{n} \cdot n \mu = \mu
\]

\item

Using the independence of the \(X_i\),

\[
\Var (\overline{X}) = \Var \bigg( \frac{1}{n} \sum_{i=1}^n X_i\bigg) =  \frac{1}{n^2} \Var \bigg( \sum_{i=1}^n X_i\bigg) =  \frac{1}{n^2} \sum_{i=1}^n  \Var ( X_i ) = \frac{1}{n^2}  \cdot n \sigma^2 = \boxed{\frac{\sigma^2}{n}}
\]

\end{enumerate}

\end{proof}

\begin{proposition}[\textbf{Stats 100B homework problem}] Suppose that \(X_1, \ldots , X_m\) and \(Y_1, \ldots , Y_n\) are two samples, with \(X \sim  \mathcal{N}(\mu_1, \sigma_1)\) and \(Y \sim  \mathcal{N}(\mu_2, \sigma_2)\). The difference between the sample means, \(\bar{X} - \bar{Y}\) , is then a linear combination of \(m + n\) normal random variables. Then

\begin{enumerate}[a.]

\item \(\E(\bar{X} - \bar{Y}) =  \mu_1 - \mu_2\).

\item \(\Var (\bar{X} - \bar{Y}) = \frac{\sigma_1^2}{m} + \frac{\sigma_2^2}{n}  \).

\item The distribution of \(\bar{X} - \bar{Y}\) is normal with mean and variance equal to the previous results.

\end{enumerate}

\end{proposition}

\begin{proof}

\begin{enumerate}[a.]

\item
% Question 6 part (a)

\[
\bar{X} = \frac{1}{m} \sum_{i=1}^m X_i, \ \bar{Y} = \frac{1}{n} \sum_{j=1}^n Y_j 
\]


\[
\E(\bar{X} - \bar{Y}) = \E\bigg(\frac{1}{m} \sum_{i=1}^m X_i - \frac{1}{n} \sum_{j=1}^n Y_j \bigg) = \frac{1}{m} \E\bigg( \sum_{i=1}^m X_i\bigg) - \frac{1}{n} \E \bigg(\sum_{j=1}^n Y_j \bigg)
\]

\[
= \frac{1}{m}  \sum_{i=1}^m \E (X_i)  - \frac{1}{n} \sum_{j=1}^n \E(Y_j) = \frac{1}{m}  \sum_{i=1}^m \mu_1  - \frac{1}{n} \sum_{j=1}^n \mu_2 = \frac{1}{m} m \cdot \mu_1 - \frac{1}{n} n \cdot \mu_2
\]

\[
\implies
\E(\bar{X} - \bar{Y}) = \mu_1 - \mu_2
\]

\item 
% Question 6 part (b)

Since \(X\) and \(Y\) are independent,

\[
\Var (\bar{X} - \bar{Y}) = \Var(\bar{X}) + \Var(\bar{Y})
\]

\[
= \E\big[ (\bar{X} - \E[\bar{X}])^2 \big] + \E\big[ (\bar{Y} - \E[\bar{Y}])^2 \big]
\]

\[
= \E\bigg( \frac{1}{m} \sum_{i=1}^m X_i - \mu_1 \bigg)^2 + \E\bigg( \frac{1}{n} \sum_{j=1}^n Y_j - \mu_2 \bigg)^2
\]

\[
= \E\bigg( \frac{1}{m} \sum_{i=1}^m \big(X_i - m \frac{1}{m} \mu_1\big) \bigg)^2 + \E\bigg( \frac{1}{n} \sum_{j=1}^n \big( Y_j - n \frac{1}{n} \mu_2 \big) \bigg)^2
\]

\[
= \frac{1}{m^2 }\E\bigg(\sum_{i=1}^m \big(X_i - \mu_1\big) \bigg)^2 + \frac{1}{n^2} \E\bigg(  \sum_{j=1}^n \big( Y_j - \mu_2 \big) \bigg)^2
\]

Since \(X_i\) and \(X_j\) are independent for \(i \neq j\) (and likewise for \(Y\)), \(\Cov(X_i, X_j) = 0\) for \(i \neq j\), so 

\[
\E[(X_i - \mu_1)(X_j - \mu_1)] = 0
\] 

for \(i \neq j\) (and likewise for \(Y\)). Therefore the above equation can be written as

\[
\frac{1}{m^2 }\E\bigg(\sum_{i=1}^m \big(X_i - \mu_1\big)^2 \bigg) + \frac{1}{n^2} \E\bigg(  \sum_{j=1}^n \big( Y_j - \mu_2 \big)^2 \bigg)
\]

\[
\frac{1}{m^2 }\sum_{i=1}^m \E \big(X_i - \mu_1\big)^2  + \frac{1}{n^2}   \sum_{j=1}^n \E \big( Y_j - \mu_2 \big)^2
\]

\[
= \frac{1}{m^2 }\bigg(\sum_{i=1}^m \sigma_1^2 \bigg) + \frac{1}{n^2} \bigg(  \sum_{j=1}^n \sigma_2^2 \bigg) = \frac{1}{m^2}m \cdot \sigma_1^2 + \frac{1}{n^2} n \cdot \sigma_2^2
\]

\[
\implies
\Var (\bar{X} - \bar{Y}) = \frac{\sigma_1^2}{m} + \frac{\sigma_2^2}{n} 
\]

\item
% Question 6 part (c)

\[
M_{X_i}(t) = \exp\bigg(\mu_1 t + \frac{t^2 \sigma_1^2}{2} \bigg), \ M_{Y_i}(t) = \exp\bigg(\mu_2 t + \frac{t^2 \sigma_2^2}{2} \bigg)
\]

Since individual observations from \(X\) and \(Y\) are independent,

\[
M_{\bar{X}}(t) = \prod_{i=1}^m M_{X_i}\bigg(\frac{1}{m}t\bigg), \ M_{\bar{Y}}(t) = \prod_{j=1}^n M_{Y_j}\bigg(\frac{1}{n}t\bigg)
\]

and

\[
M_{\bar{X} - \bar{Y}}(t) = M_{\bar{X}}(t) M_{\bar{-Y}}(t) = M_{\bar{X}}(t) M_{\bar{Y}}(-t) = \prod_{i=1}^m M_{X_i}\bigg(\frac{1}{m}t\bigg) \prod_{j=1}^n M_{Y_j}\bigg(\frac{-1}{n}t\bigg)
\]

\[
= \bigg[ M_{X_i}\bigg(\frac{t}{m} \bigg) \bigg] ^m  \bigg[ M_{Y_j}\bigg( \frac{-t}{n} \bigg)  \bigg]^n = \bigg[ \exp\bigg(\frac{\mu_1 t}{m} + \frac{t^2 \sigma_1^2}{2m^2} \bigg) \bigg] ^m  \bigg[ \exp\bigg(\frac{-\mu_2 t}{n} + \frac{(-t)^2 \sigma_2^2}{2n^2} \bigg) \bigg]^n
\]

\[
= \exp\bigg(\frac{m \mu_1 t}{m} + \frac{m t^2 \sigma_1^2}{2m^2} \bigg)   \exp\bigg(\frac{-n \mu_2 t}{n} + \frac{n t^2 \sigma_2^2}{2n^2} \bigg)
\]

\[
\implies \boxed{
M_{\bar{X} - \bar{Y}}(t) = \exp \bigg[ (\mu_1 - \mu_2)t + \frac{1}{2}t^2 \bigg( \frac{\sigma_1^2}{m}  + \frac{\sigma_2^2}{n} \bigg) \bigg]}
\]

This is the moment generating function of a normal distribution with mean \( \mu_1 - \mu_2\) and variance \(\frac{\sigma_1^2}{m}  + \frac{\sigma_2^2}{n}\), consistent with the results from parts (a) and (b).


\end{enumerate}

\end{proof}

\begin{definition}[\textbf{Sample variance}] Let \(n > 1\). The \textbf{sample variance} of a random sample \(X_1, \ldots, X_n\) of size \(n\), denoted \(S^2\), is the following statistic:

\[
S^2 := \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2.
\]

The \textbf{sample standard deviation} of a random sample of size \(n\) is \(\sqrt{S^2}\).

\end{definition}

\begin{proposition}[\textbf{Unbiasedness of sample variance}] Suppose we have a random sample of size \(n\) from an i.i.d. distribution \(X_1, X_2, \ldots, X_n\) with \(\E(X_1) = \mu \ in \mathbb{R}\), \(\Var(X_1) = \sigma^2 < \infty\). Then \(\E(S^2) = \sigma^2\). Further, \(S^2\) is a consistent estimator of \(\sigma^2\).

\end{proposition}

\begin{proof}

We have

\[
\E(S^2) = \E \bigg( \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2 \bigg) = \frac{1}{n-1}  \E \bigg( \sum_{i=1}^n X_i^2 - 2X_i\overline{X} + \overline{X}^2 \bigg)
\]

\[
= \frac{1}{n-1} \bigg( \sum_{i=1}^n  \E(X_i^2)  - 2 \E \bigg( \overline{X}  \sum_{i=1}^n  X_i \bigg) + n \E\overline{X}^2 \bigg)  = \frac{1}{n-1} \bigg(  n \E(X_i^2)  - 2n \E  \overline{X}^2  + n \E\overline{X}^2 \bigg)
\]

\[
= \frac{n}{n-1} \big(   \E(X_i^2)  -  \E  \overline{X}^2 \big) = \frac{n}{n-1} \big(  \Var(X_i) + \E(X_i)^2 - [\Var(\overline{X}) + \E( \overline{X})^2] \big)
\]

Using the results from Proposition \ref{mathstats.sample.mean.props}, we have

\[
\E(S^2) = \frac{n}{n-1} \big(  \sigma^2 + \mu^2 - [ \sigma^2/n + \mu^2] \big)  = \frac{n}{n-1} \cdot \frac{(n-1) \sigma^2}{n}  = \boxed{\sigma^2}
\]

\end{proof}

\begin{proof}[Alternative proof from Stats 100B homework.]

\[
\E\bigg( \frac{1}{n} \sum_{i=1}^n(x_i - \mu)^2  \bigg) = \frac{1}{n}  \E\bigg( \sum_{i=1}^n(x_i - \mu)^2  \bigg)
\]

Assuming independence of samples, this can be written as

\[
\frac{1}{n}  \sum_{i=1}^n  \E( (x_i - \mu)^2) = \frac{1}{n} n \sigma^2 = \boxed{\sigma^2}
\]

Since \(S^2\) is unbiased, it is a consistent estimator if we can show \(\Var(S^2) \to 0\)  as \( n \to \infty\). We have

\[
\Var(\frac{(n-1)S^2}{\sigma^2}) = 2(n-1)
\]

\[
\frac{(n-1)^2}{\sigma^4}\Var(S^2) = 2(n-1)
\]

\[
\Var(S^2) = \frac{2\sigma^4}{n-1}
\]

\[
\implies \lim_{n \to \infty} \Var(S^2) =  \lim_{n \to \infty}  \frac{2\sigma^4}{n-1} = \boxed{0}
\]

Therefore \(S^2\) is a consistent estimator of \(\sigma^2\).

\end{proof}

\begin{lemma}\label{stats.hw3.ex.3.16} Let \(X := (X_1, \ldots, X_n)\) be i.i.d. mean zero, variance 1 Gaussian random variables. Let \(v_1, \ldots, v_n \in \mathbb{R}^n\). Then \(\langle X, v_1 \rangle, \ldots, \langle X, v_n \rangle\) are independent if and only if \(v_1, \ldots, v_m \) are pairwise orthogonal; that is,  \(\langle v_i, v_j \rangle = 0 \ \forall \ 1 \leq i < j \leq m\). 

\end{lemma}

\begin{proof} By Theorem \ref{prob.thm.sums.gaussian}, we have that for any $v\in\mathbb{R}^{n}$, $\langle X, v\rangle$ is a mean zero Gaussian with variance $\langle v,v\rangle$. For notational convenience, let \(\langle X, v_k \rangle = A_k\). Because all the \( A_k \) are Gaussian random variables by Theorem \ref{prob.thm.sums.gaussian}, the \(A_k\) are uncorrelated if and only if they are independent. That is, we would like to show that their covariances


\[
\E[(A_k - \E A_k)(A_\ell - \E A_\ell)] 
\]

equal zero for all \( \{(k, \ell) : k, \ell \in \{1, 2, \ldots, m\}, k \neq \ell \}\) if and only if the vectors $v_1,\dots,v_m$ are pairwise orthogonal; that is, \(\langle v_k, v_\ell \rangle = 0 \) for all \( \{(k, \ell) : k, \ell \in \{1, 2, \ldots, m\}, k \neq \ell \}\). Note that since \(A_k = \sum_{i=1}^n X_i v_{ki}\), \(\E (A_k) =   \sum_{i=1}^n  v_{ki} \E(X_i)\). So for any \( \{(k, \ell) : k, \ell \in \{1, 2, \ldots, m\}, k \neq \ell \}\) we have

\[
\E[(A_k - \E A_k)(A_\ell - \E A_\ell)] = \E \bigg[ \bigg( \sum_{i=1}^n X_i v_{ki} -   \sum_{i=1}^n  v_{ki} \E(X_i) \bigg) \bigg(\sum_{i=1}^n X_i v_{\ell i} -   \sum_{i=1}^n  v_{\ell i} \E(X_i) \bigg) \bigg] 
\]

\[
= \E \bigg[ \bigg( \sum_{i=1}^n X_i v_{ki} \bigg)  \bigg(\sum_{i=1}^n X_i v_{\ell i} \bigg) -  \bigg( \sum_{i=1}^n X_i v_{ki} \bigg) \bigg( \sum_{i=1}^n  v_{\ell i} \E(X_i) \bigg)   
\]

\[
-   \bigg( \sum_{i=1}^n  v_{ki} \E(X_i) \bigg) \bigg(\sum_{i=1}^n X_i v_{\ell i} \bigg)  +    \bigg( \sum_{i=1}^n  v_{ki} \E(X_i) \bigg)  \bigg(  \sum_{i=1}^n  v_{\ell i} \E(X_i) \bigg) \bigg] 
\]




\[
= \E \bigg(  \sum_{i=1}^n X_i^2 v_{ki} v_{\ell i} + \sum_{\{a, b \in \{1, \ldots, n\}, a \neq b \} }   X_a X_b v_{ka}  v_{\ell b}\bigg)
-  2 \E \bigg(  \sum_{i=1}^n X_i \E(X_i) v_{ki} v_{\ell i} + \sum_{\{a, b \in \{1, \ldots, n\}, a \neq b\} }   X_a \E(X_b) v_{ka}  v_{\ell b} \bigg)   
\]

\[  
+   \E  \bigg(  \sum_{i=1}^n \E(X_i)^2 v_{ki} v_{\ell i} + \sum_{\{a, b \in \{1, \ldots, n\},a \neq b \} }   \E(X_a) \E(X_b) v_{ka}  v_{\ell b} \bigg)
\]

Recall that \(\E(X_i)= 0\) for all \(i\). Also, due to independence of the \(X_i\), all of the terms that involve \(\E (X_a X_b), \ a \neq b\) disappear. This leaves only

 \begin{equation}\label{prob.541a.hw3.5}
= \E \bigg(  \sum_{i=1}^n X_i^2 v_{ki} v_{\ell i} \bigg) = \sum_{i=1}^n  \E (X_i^2 ) v_{ki} v_{\ell i}  = \E (X_1^2 ) \sum_{i=1}^n   v_{ki} v_{\ell i} 
\end{equation}

where the last step follows from the i.i.d. distributions of \(X_i\). Recall

\[
\langle v_k, v_\ell \rangle = 0 \iff  \sum_{i=1}^n   v_{ki} v_{\ell i}  = 0.
\]

Since \(\E(X_i^2) \neq 0\), (\ref{prob.541a.hw3.5}) equals 0 for all \( \{(k, \ell) : k, \ell \in \{1, 2, \ldots, m\}, k \neq \ell \}\) if and only if \(\langle v_k, v_\ell \rangle = 0 \) for all \( \{(k, \ell) : k, \ell \in \{1, 2, \ldots, m\}, k \neq \ell \}\). Therefore the random variables $\langle X, v_1 \rangle, \dots, \langle X, v_m \rangle$ are independent if and only if the vectors $v_1,\dots,v_m$ are pairwise orthogonal.

\end{proof}

\begin{proposition}[\textbf{Proposition 4.7 in 541A notes}] Let \(n \geq 2\) be an integer. Let \(X_1, \ldots, X_n\) be a random sample from the Gaussian distribution with mean \(\mu \in \mathbb{R}\) and variance \(\sigma^2 > 0\). Let \(\overline{X}\) be the sample mean and let \(S\) be the sample standard deviation. Then

\begin{enumerate}[(i)]

\item \(\overline{X}\) and \(S\) are independent random variables.

\item \(\overline{X}\) is a Gaussian random variable with mean \(\mu\) and variance \(\sigma^2/n\).

\item \((n-1)S^2/\sigma^2\) is a \(\chi^2\)-distributed random variable with \(n-1\) degrees of freedom.

\end{enumerate}

\end{proposition}

\begin{proof}

\begin{enumerate}[(i)]

\item Replace \(X_1, \ldots, X_n\) with \(X_1 - \mu, \ldots, X_n - \mu\) so that \(\mu = 0\). Also divide by \(\sigma\) so that \(\sigma=1\). Note that \(\overline{X}\) is independent of all random variables \(X_2 - \overline{X}, \ldots, X_n - \overline{X}\) by Lemma \ref{stats.hw3.ex.3.16} because for example

\[
X_2 - \overline{X} = \langle X_2, e_2 - \frac{1}{n} (1, 1, \ldots, 1) \rangle
\]

where the second vector in the inner product is orthogonal to \((1, 1, \ldots, 1 )\) (in fact, \((1, \ldots, 1)\) is orthogonal to anything in the span of these vectors). Likewise for all the remaining vectors you could use to construct \(X_i\). (Note that the other random variables [e.g. \(X_2 - \overline{X}\) and \(X_3 - \overline{X}\)] are not independent.)

\

So the proof will be complete if we can write \(S\) as a function of \(X_2 - \overline{X}, \ldots, X_n - \overline{X}\). Observe

\[
(n-1)S^2 = \sum_{i=1}^n (X_i - \overline{X})^2 = (X_1 - \overline{X})^2 + \sum_{i=2}^n (X_i - \overline{X})^2 = \bigg( n \overline{X} - \bigg[ \sum_{i=2}^n X_i \bigg] - \overline{X} \bigg) ^2 + \sum_{i=2}^n \bigg( X_i - \overline{X} \bigg)^2 
\]

\[
= \bigg(\sum_{i=2}^n (X_i - \overline{X}  \bigg)^2 + \sum_{i=2}^n (X_i - \overline{X})^2 
\]

\item Follows from Proposition 1.45, Example 1.108, and Exercise 1.58 in 541A notes (condense later?)

\item Like above, replace \(X_1, \ldots, X_n\) with \(X_1 - \mu, \ldots, X_n - \mu\) so that \(\mu = 0\). Also divide by \(\sigma\) so that \(\sigma=1\). We will prove by induction. Let \(\overline{X}_n:= \frac{1}{n} \sum_{i=1}^n X_i\) and let \(S_n^2 := \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X}_n)^2\). In the case \(n=2\) we have

\[
S_2^2 = \bigg(X_1 - \frac{1}{2}(X_1 + X_2) \bigg)^2 + \bigg(X_2 - \frac{1}{2}(X_1 + X_2) \bigg)^2 = \frac{1}{4}(X_2 - X_1)^2 + \frac{1}{4}(X_2 - X_1)^2 = \frac{1}{2}(X_2 - X_1)^2 
\]

\[
= \bigg( \frac{1}{\sqrt{2}} (X-2 - X_1) \bigg)^2
\]

Note that \(1/\sqrt{2} (X_2 - X_1)\) is a mean zero Gaussian random variable with variance 1 (see example 1.108 in 541A notes for details). So \(S_2^2\) is \(\chi_1^2\) by Definition 1.33 in 541A notes.

\

We now induct on \(n\). From Lemma 4.8 in 541A notes (will prove later),

\[
n S_{n+1}^2 = (n-1) S_n^2 + \frac{n}{n+1}(X_{n+1} - \overline{X}_n)^2, \ \ \ \forall n \geq 2
\]

From the first item, \(S_n\) is independent of \(\overline{X}_n\). Also, \(X_{n+1}\) is independent of \(S_n\) by Proposition 1.61 in Math 541A notes, since \(S_n\) is a function of \(X_1, \ldots, X_n\), which are independent of \(X_{n+1}\). So \(S_n\) is independent of \((X_{n+1} - \overline{X}_n)^2\). By the inductive hypothesis, \((n-1)S_n^2\) is a \(\chi_{n-1}^2\) random variable. From Example 1.108 in Math 541A notes, \(X_{n+1} - \overline{X}_n\) is a Gaussian random variable with mean zero and variance \(1 + 1/n = (n+1)/n\) so that \(\sqrt{n/(n+1)} (X_{n+1} - \overline{X}_n)\) is a mean zero Gaussian with variance 1, implying \(n/(n+1) (X_{n+1} - \overline{X}_n)^2\) is \(\chi^2\). Definition 1.33 in 541A notes then implies that \(nS_{n+1}\) is a \(\chi_n^2\) random variable, completing the inductive step.

\end{enumerate}

\end{proof}

\begin{lemma}[\textbf{Lemma 4.8 in 541A notes.}]

\end{lemma} Let \(X_1, X_2, \ldots\) be random variables. For any \(n \geq 2\), let \(\overline{X}_n:= (1/n)\sum_{i=1}^n X_i\) and let \(S_n^2:= 1/(n-1) \sum_{i=1}^n (X_i - \overline{X}_n)^2\). Then

\[
n S_{n+1}^2  - (n-1)S_n^2 =\frac{n}{n+1}(X_{n+1} - \overline{X}_n)^2.
\]

\begin{proof}

\[
n S_{n+1}^2  - (n-1)S_n^2 = \sum_{i=1}^{n+1} (X_i - \overline{X}_{n+1})^2 - \sum_{i=1}^n (X_i - \overline{X}_n)^2 
\]

Note:

\[
(a-b)^2 - (a-c)^2 = a^2 - 2ab + b^2 - a^2 - c^2 + 2ac = b^2 - c^2 + 2a(c-b) 
\]

\[
= (b-c)[(b+c)  - 2a]= (b-c)(b+c-2a)
\]

for all real \(a, b,c\). Using \(a=X_n, b= \overline{X}_{n+1}, c= \overline{X_n}\) we have

\[
= (X_{n+1} - \overline{X}_{n+1})^2 + \sum_{i=1}^n(\overline{X}_{n+1}- \overline{X}_n)(\overline{X}_{n+1} + \overline{X}_n - 2 X_i) 
\]

\[
= (X_{n+1} - \overline{X}_{n+1})^2 + (\overline{X}_{n+1} - \overline{X}_n) \sum_{i=1}^n (\overline{X}_{n+1} + \overline{X}_n - 2 X_i)
\]

\[
= (X_{n+1} - \overline{X}_{n+1})^2 + (\overline{X}_{n+1} - \overline{X}_n) \cdot n (\overline{X}_{n+1} + \overline{X}_n - 2 \overline{X}_n)
\]

\[
=(X_{n+1}(1 - 1/(n+1)) - \frac{n}{n++1} \overline{X}_n)^2 + n(\overline{X}_{n+1} - \overline{X}_n)^2
\]

\[
= \frac{n^2}{(n+1)^2} (X_{n+1} - \overline{X}_n)^2 + n ( \frac{X_{n+1}}{n+1} + ( \frac{1}{n+1} - \frac{1}{n} ) \sum_{i=1}^n X_i )^2
\]

Algebra: \(1/(n+1) - 1/n = \frac{n - (n+1)}{n(n+1)} = - \frac{1}{n(n+1)}\). So we have

\[
= \frac{n^2}{(n+1)^2}(X_{n+1}-\overline{X}_n)^2  + \frac{n}{(n+1)^2} (X_{n+1} - \frac{1}{n} \sum_{i=1}^n X_i )^2
\]

\[
= \frac{n^2}{(n+1)^2} (X_{n+1}-\overline{X}_n)^2 + \frac{n}{(n+1)^2} (X_{n+1} - \overline{X}_n)^2
\]

\[
=\frac{n^2+n}{(n+1)^2} (X_{n+1} - \overline{X}_n)^2 = \frac{n}{n+1}(X_{n+1} - \overline{X}_n)^2
\]


\end{proof}

\begin{proposition}[\textbf{Proposition 4.9 in 541A notes}] Let \(X\) be a standard Gaussian random variable. Let \(Y\) be a \(\chi_p^2\) random variable. Assume that \(X\) and \(Y\) are independent. Then \(X/\sqrt{Y/p}\) has the following density, known as \textbf{Student's \(t\)-distribution} with \(p=\) degrees of freedom: (\(p=n+1\)?)

\[
f_{X/(Y/\sqrt{p})}(t):= \frac{ \Gamma((p+1)/2)}{\sqrt{p} \sqrt{\pi} \Gamma(p/2) } \bigg( 1 + \frac{t^2}{p} \bigg)^{-(p+1)/2}, \ \ \ \forall t \in \mathbb{R}
\]

(should have \(p+1\) in a bunch of the expressions above? that's what was written on board, not in notes.)

\end{proposition}

\begin{proof} Let \(Z:= \sqrt{Y/p}\). We find the density of \(Z\) as follows. Let \(t > 0\). Then

\[
f_Z(y) = \deriv{}{y}\big|_{y=0} \Pr(Z \leq y) = \deriv{}{y} \big|_{y=0} \Pr(Y \leq y^2p) 
\]

\[
=  \deriv{}{y} \big|_{y=0} \int_0^{y^2p} \frac{x^{(p/2)-1}e^{-x/2}}{2^{p/2} \Gamma(p/2)} dx = 2yp \cdot p^{(p/2)-1} y^{p-2}e^{-y^2p/2} \cdot \frac{1}{2^{p/2} \Gamma(p/2)}
\]

\[
=p^{p/2}y^{p-1}e^{-y^2 p/2} \cdot \frac{1}{2^{p/2-1} \Gamma(p/2)}
\] 

\[
\vdots \text{ skipped this stuff in class proof}
\]

\[
\Pr(X/Z \leq t) = \Pr(X \leq tZ) 
\]

\[
= \text{ (by definition of joint density) }  \int \int_{\{ (x,y) \in \mathbb{R}^2: x \leq ty \} } f_X(x) f_Z(y) dx dy
\]

We use the change of variables formula:

\[
\int \int_{\phi(U)} f(x,y) dxdy = \int \int_U f(\phi(a,b)) | \operatorname{Jac} \phi(a,b)| da db
\]

\[
\phi: \mathbb{R}^2 \to \mathbb{R}^2
\]

\[
\phi(a,b) = (ab,a)
\]

\[
\phi^{-1}(x,y) =(y, x/y)
\]

We chose \(x/y\) as the second variable so that an upper limit of the variable will end up being \(t\) after the transformation. We need the Jacobian of \(\phi\):'

\[
| \operatorname{Jac} \phi(a,b) | = \left| \operatorname{det} \begin{pmatrix} b & a \\ 1 & 0 \end{pmatrix} \right| = |a|
\]

By the change in variables formula,

\[
\int \int_{\phi(U)} f(x,y) dx dy = \int \int_U f(\phi(a,b)) \left| \operatorname{Jac} \phi(a,b) \right| da db
\]

\[
 = \int \int_{\{ (a,b) \in \mathbb{R}^2: a \geq 0, b \leq t \} }  f_X(ab) f_Z(a) \left| a \right| da db
\]

\[
\implies \Pr(X/Z \leq t) = \int_{-\infty}^t \int_0^\infty |a|  f_X(ab) f_Z(a) da db
\]

By the Fundamental Theorem of Calculus,

\[
f_{X/Z}(t) = \deriv{}{t} \Pr(X/Z \leq t) = \int_0^\infty |a|  f_X(at) f_Z(a)) da = \int_0^\infty a f_X(at) f_Z(a)) da
\]

By the definitions of \(X\) and \(Z\), 

\[
= \frac{1}{2^{-/2-1} \Gamma(p/2)}\int_0^\infty a  \cdot \frac{1}{\sqrt{2 \pi}} e^{-(a^2t^2)/2} \cdot p^{p/2} a^{p-1} e^{-a^2p/2}  da
\]

\[
= \frac{p^{p/2} }{2^{-/2-1} \Gamma(p/2) \sqrt{2 \pi}}\int_0^\infty   e^{-[a^2(t^2 + p)]/2} \cdot a^{p}  da
\]

Change of variables: let \(x = a^2, dx = 2ada, da= \frac{1}{2a} dx = 1/(2 \sqrt{x}) dx\). Then this integral is

\[
= c \int_0^\infty   e^{-[x(t^2 + p)]/2} \cdot x^{p/2 - 1/2}  da, \ \ \ \text{where } c = \frac{p^{p/2}}{2^{p/2} \sqrt{2 \pi} \Gamma(p/2)}
\]

So the integrand is a Gamma density function with parameters \(\alpha, \beta\): \(\alpha  - 1 = p/2 - 1/2 \iff \alpha = p/2 + 1/2\), \(\beta = 2/(t^2 + p)\). So if we multiply and divide \(\beta^\alpha \Gamma(\alpha)\)

\

So

\[
f_{X/Z}(t) = \frac{p^{p/2}}{2^{p/2} \sqrt{2 \pi} \Gamma(p/2)} \cdot \beta^\alpha \Gamma(\alpha) \cdot 1 = \frac{p^{p/2} \Gamma((p_1)/2)}{2^{p/2} \sqrt{2 \pi} \Gamma(p/2)} \cdot \bigg( \frac{2}{t^2 + p} \bigg)^{(p-1)/2}
\]

\[
= \frac{p^{p/2} \Gamma((p+1)/2)}{\sqrt{\pi} \Gamma(p/2)} \cdot (t^2 + p)^{-(p+1)/2} = \frac{\Gamma((p+1)/2)}{\sqrt{\pi p} \Gamma(p/2)} \cdot (1 + t^2/p)^{-(p+1)/2}
\]




\end{proof}

\begin{remark}[Remark 4.10 in 541A notes] If \(X_1, \ldots, X_n\) is a random sample from a Gaussian distribution with mean \(\mu \in \mathbb{R}\), standard deviation \(\sigma < 0\), then

\[
\frac{\overline{X} - \mu}{S / \sqrt{n}}
\]

also has Student's \(t\) distribution. (\(\overline{X} := n^{-1} \sum_{i=1}^n X_i, S = \sqrt{(n-1)^{n-1} \sum_{i=1}^n (X_i - \overline{X})^2}\).)

\end{remark}

\begin{proposition}[\textbf{Stats 100B homework 3 problem}] Let \(X_1, X_2\) be a random sample from a normal distribution with a mean \(\mu\) and standard deviation \(\sigma\). Then \((n-1)s^2/\sigma^2\) has a \(\chi_1^2\) distribution.

\end{proposition}

\begin{proof}

\[
s^2 = \frac{1}{2-1}\sum_{i=1}^2(X_i - \bar{X})^2  = (X_1 - \bar{X})^2 + (X_2 - \bar{X})^2 = (X_1 - \frac{X_1 + X_2}{2})^2 +  (X_2 - \frac{X_1 + X_2}{2})^2
\]

\[
= X_1^2 - 2X_1(\frac{X_1 + X_2}{2}) + (\frac{X_1 + X_2}{2})^2 + X_2^2 - 2X_2(\frac{X_1 + X_2}{2}) + (\frac{X_1 + X_2}{2})^2
\]

\[
= X_1^2 +  X_2^2  - X_1(X_1 + X_2)  - X_2(X_1 + X_2) + 2(\frac{X_1 + X_2}{2})^2
\]

\[
= X_1^2 + X_2^2 - (X_1 + X_2)(X_1 + X_2) + \frac{(X_1 + X_2)^2}{2}
\]

\[
= \frac{1}{2}(2X_1^2 + 2X_2^2) - \frac{1}{2}(X_1^2 + 2X_1 X_2 + X_2^2)
\]

\[
= \frac{1}{2}(X_1^2 - 2X_1 X_2 + X_2^2)
\]

\[
\boxed{
s^2 = \frac{1}{2}(X_1 - X_2)^2}
\]

\[
\implies \frac{(n-1)s^2}{\sigma^2} = (2-1)\frac{1}{2\sigma^2}(X_1 - X_2)^2 = \bigg(\frac{X_1-X_2}{\sigma \sqrt{2}} \bigg)^2
\]

Since \(X_1\) and \(X_2\) are normal, 

\[
X_1 - X_2 \sim \mathcal{N}(\mu - \mu, \sqrt{\sigma^2 + \sigma^2}) = \mathcal{N}(0, \sigma\sqrt{2}) \implies \frac{X_1 - X_2}{\sigma \sqrt{2}} \sim \mathcal{N}(0, 1)
\]

\[
\implies \bigg(\frac{X_1-X_2}{\sigma \sqrt{2}} \bigg)^2 = \boxed{\frac{(n-1)s^2}{\sigma^2} \sim \chi_1^2}
\]


\end{proof}

\begin{proposition}[\textbf{Stats 100B homework problem}] Suppose two independent random samples of \(n_1\) and \(n_2\) observations are selected from two normal populations. Further, assume that the populations possess a common variance \(\sigma^2\) which is unknown. Let the sample variances be \(S_1^2\) and \(S_2^2\) and assume they are unbiased. Then the pooled estimator for \(\sigma^2\)

\[
S^2 = \frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}
\]

is unbiased and has variance \( \frac{2 \sigma^4}{n_1 + n_2 -2}\).

\end{proposition}

\begin{proof} First we show \(S^2\) is unbiased. 

\[
\E(S^2) = \E \bigg( \frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 -2} \bigg) =   \frac{n_1 - 1}{n_1 + n_2 -2}\E (S_1^2 ) +  \frac{n_2 - 1}{n_1 + n_2 -2}\E (S_2^2 )
\]

\[
=   \frac{n_1 - 1}{n_1 + n_2 -2}\sigma^2 +  \frac{n_2 - 1}{n_1 + n_2 -2}\sigma^2 = \frac{(n_1 + n_2 -2)\sigma^2}{n_1 + n_2 -2} = \boxed{\sigma^2}
\]

Now we derive its variance.

\[
\Var(S^2) = \Var \bigg(  \frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 -2}  \bigg) 
\]

Since \(S_1\) and \(S_2\) are independent, this can be written as

\[
 \frac{1}{(n_1 + n_2 -2)^2} \bigg(\Var[(n_1 - 1)S_1^2]  +   \Var[(n_2 - 1)S_2^2 ] \bigg)
\]

Since the populations are normal, we know

\[
\frac{(n_i-1)S_i^2}{\sigma^2} \textapprox \chi_{n_i-1}^2 \implies \Var \bigg( \frac{(n_i-1)S_i^2}{\sigma^2} \bigg) = 2(n_i-1)
\]

\[
\Var(S^2) =  \frac{\sigma^4}{(n_1 + n_2 -2)^2} \bigg(\Var\bigg[ \frac{(n_1 - 1)S_1^2}{\sigma^2}\bigg]  +   \Var\bigg[ \frac{(n_2 - 1)S_2^2}{\sigma^2} \bigg] \bigg)
\]

\[
\frac{\sigma^4}{(n_1 + n_2 -2)^2}(2(n_1 - 1) + 2(n_2 - 1)) = \sigma^4 \frac{2(n_1 + n_2 -2)}{(n_1 + n_2 -2)^2}
\]

\[
=  \frac{2 \sigma^4}{n_1 + n_2 -2}
\]

\end{proof}

\begin{proposition}[\textbf{Stats 100B Homework problem}] Suppose that \(X_1, \ldots , X_m\) and \(Y_1, \ldots , Y_n\) are two samples, with \(X \textapprox  \mathcal{N}(\mu_1, \sigma_1)\) and \(Y \textapprox  \mathcal{N}(\mu_2, \sigma_2)\). The difference between the sample means, \(\bar{X} - \bar{Y}\) , is then a linear combination of \(m + n\) normal random variables.

\begin{enumerate}[a.]

\item \(\E(\bar{X} - \bar{Y})\).

\item \(\Var (\bar{X} - \bar{Y})\)

\item The distribution of \(\bar{X} - \bar{Y}\) is normal.

\end{enumerate}

\end{proposition}

\begin{proof}

\begin{enumerate}[a.]

\item
% Question 6 part (a)

\[
\bar{X} = \frac{1}{m} \sum_{i=1}^m X_i, \ \bar{Y} = \frac{1}{n} \sum_{j=1}^n Y_j 
\]


\[
\E(\bar{X} - \bar{Y}) = \E\bigg(\frac{1}{m} \sum_{i=1}^m X_i - \frac{1}{n} \sum_{j=1}^n Y_j \bigg) = \frac{1}{m} \E\bigg( \sum_{i=1}^m X_i\bigg) - \frac{1}{n} \E \bigg(\sum_{j=1}^n Y_j \bigg)
\]

\[
= \frac{1}{m}  \sum_{i=1}^m \E (X_i)  - \frac{1}{n} \sum_{j=1}^n \E(Y_j) = \frac{1}{m}  \sum_{i=1}^m \mu_1  - \frac{1}{n} \sum_{j=1}^n \mu_2 = \frac{1}{m} m \cdot \mu_1 - \frac{1}{n} n \cdot \mu_2
\]

\[
\boxed{
\E(\bar{X} - \bar{Y}) = \mu_1 - \mu_2}
\]

\item 
% Question 6 part (b)

Since \(X\) and \(Y\) are independent,

\[
\Var (\bar{X} - \bar{Y}) = \Var(\bar{X}) + \Var(\bar{Y})
\]

\[
= \E\big[ (\bar{X} - \E[\bar{X}])^2 \big] + \E\big[ (\bar{Y} - \E[\bar{Y}])^2 \big]
\]

\[
= \E\bigg( \frac{1}{m} \sum_{i=1}^m X_i - \mu_1 \bigg)^2 + \E\bigg( \frac{1}{n} \sum_{j=1}^n Y_j - \mu_2 \bigg)^2
\]

\[
= \E\bigg( \frac{1}{m} \sum_{i=1}^m \big(X_i - m \frac{1}{m} \mu_1\big) \bigg)^2 + \E\bigg( \frac{1}{n} \sum_{j=1}^n \big( Y_j - n \frac{1}{n} \mu_2 \big) \bigg)^2
\]

\[
= \frac{1}{m^2 }\E\bigg(\sum_{i=1}^m \big(X_i - \mu_1\big) \bigg)^2 + \frac{1}{n^2} \E\bigg(  \sum_{j=1}^n \big( Y_j - \mu_2 \big) \bigg)^2
\]

Since \(X_i\) and \(X_j\) are independent for \(i \neq j\) (and likewise for \(Y\)), \(\Cov(X_i, X_j) = 0\) for \(i \neq j\), so 

\[
\E[(X_i - \mu_1)(X_j - \mu_1)] = 0
\] 

for \(i \neq j\) (and likewise for \(Y\)). Therefore the above equation can be written as

\[
\frac{1}{m^2 }\E\bigg(\sum_{i=1}^m \big(X_i - \mu_1\big)^2 \bigg) + \frac{1}{n^2} \E\bigg(  \sum_{j=1}^n \big( Y_j - \mu_2 \big)^2 \bigg)
\]

\[
\frac{1}{m^2 }\sum_{i=1}^m \E \big(X_i - \mu_1\big)^2  + \frac{1}{n^2}   \sum_{j=1}^n \E \big( Y_j - \mu_2 \big)^2
\]

\[
= \frac{1}{m^2 }\bigg(\sum_{i=1}^m \sigma_1^2 \bigg) + \frac{1}{n^2} \bigg(  \sum_{j=1}^n \sigma_2^2 \bigg) = \frac{1}{m^2}m \cdot \sigma_1^2 + \frac{1}{n^2} n \cdot \sigma_2^2
\]

\[
\boxed{
\Var (\bar{X} - \bar{Y}) = \frac{\sigma_1^2}{m} + \frac{\sigma_2^2}{n} }
\]

\item
% Question 6 part (c)

\[
M_{X_i}(t) = \exp\bigg(\mu_1 t + \frac{t^2 \sigma_1^2}{2} \bigg), \ M_{Y_i}(t) = \exp\bigg(\mu_2 t + \frac{t^2 \sigma_2^2}{2} \bigg)
\]

Since individual observations from \(X\) and \(Y\) are independent,

\[
M_{\bar{X}}(t) = \prod_{i=1}^m M_{X_i}\bigg(\frac{1}{m}t\bigg), \ M_{\bar{Y}}(t) = \prod_{j=1}^n M_{Y_j}\bigg(\frac{1}{n}t\bigg)
\]

and

\[
M_{\bar{X} - \bar{Y}}(t) = M_{\bar{X}}(t) M_{\bar{-Y}}(t) = M_{\bar{X}}(t) M_{\bar{Y}}(-t) = \prod_{i=1}^m M_{X_i}\bigg(\frac{1}{m}t\bigg) \prod_{j=1}^n M_{Y_j}\bigg(\frac{-1}{n}t\bigg)
\]

\[
= \bigg[ M_{X_i}\bigg(\frac{t}{m} \bigg) \bigg] ^m  \bigg[ M_{Y_j}\bigg( \frac{-t}{n} \bigg)  \bigg]^n = \bigg[ \exp\bigg(\frac{\mu_1 t}{m} + \frac{t^2 \sigma_1^2}{2m^2} \bigg) \bigg] ^m  \bigg[ \exp\bigg(\frac{-\mu_2 t}{n} + \frac{(-t)^2 \sigma_2^2}{2n^2} \bigg) \bigg]^n
\]

\[
= \exp\bigg(\frac{m \mu_1 t}{m} + \frac{m t^2 \sigma_1^2}{2m^2} \bigg)   \exp\bigg(\frac{-n \mu_2 t}{n} + \frac{n t^2 \sigma_2^2}{2n^2} \bigg)
\]

\[
\implies \boxed{
M_{\bar{X} - \bar{Y}}(t) = \exp \bigg[ (\mu_1 - \mu_2)t + \frac{1}{2}t^2 \bigg( \frac{\sigma_1^2}{m}  + \frac{\sigma_2^2}{n} \bigg) \bigg]}
\]

This is the moment generating function of a normal distribution with mean \( \mu_1 - \mu_2\) and variance \(\frac{\sigma_1^2}{m}  + \frac{\sigma_2^2}{n}\), consistent with the results from parts (a) and (b).



\end{enumerate}

\end{proof}

\subsubsection{The Delta Method}

\begin{theorem}[\textbf{Delta Method, Theorem 4.14 in 541A notes, 5.5.24 in Casella and Berger}]\label{mathstats.delta.method.thm} Let \(\theta \in \mathbb{R}\). Let \(Y_1, Y_2, \ldots\) be random variables such that \(\sqrt{n}(Y_n - \theta)\) converges in distribution to a mean zero Gaussian random variable with variance \(\sigma^2 > 0\). Let \(f: \mathbb{R} \to \mathbb{R}\). Assume that \(f'\) exists and is continuous, and \(f'(\theta) \neq 0\). Then

\[
\sqrt{n}(f(Y_n) - f(\theta)) \xrightarrow{d} \mathcal{N}(0, \sigma^2(f'(\theta))^2).
\]

%converges in distribution to a mean zero Gaussian random variable with variance \(\sigma^2(f'(\theta))^2\) as \(n \to \infty\).

\end{theorem}

\begin{proof}[Proof from new class notes] Since \(f'(\theta)\) exists, \(\lim_{y \to \theta} \frac{f(y) - f(\theta)}{y - \theta}\) exists. That is, there exists \(h: \mathbb{R} \to \mathbb{R}\) such that \(\lim_{z \to 0} \frac{h(z)}{z} = 0\) and for all \(y \in \mathbb{R}\),

\[
f'(\theta)  = \frac{f(y) - f(\theta)}{(y - \theta)}  + h(y-\theta)
\]

\[
 \iff f(y) = f(\theta) + f'(\theta) (y - \theta) + h(y-\theta).
\]

In particular,

\begin{equation}\label{mathstats.delta.method.new.proof}
\sqrt{n}[f(Y_n) - f(\theta)]  =  \underbrace{f'(\theta)}_{\text{(constant)}} \underbrace{\sqrt{n} (Y_n - \theta)}_{\implies \mathcal{N}(0,\sigma^2)} + \underbrace{\sqrt{n}h(Y_n-\theta)}_{\text{?}}.
\end{equation}

where we note that \( \sqrt{n} (Y_n - \theta) \xrightarrow{d} \mathcal{N}(0,\sigma^2)\) by assumption. Since it is multiplied by \(f'(\theta) \in \mathbb{R}\), the product of these two terms converges to \(\mathcal{N}(0, \sigma^2 [f'(\theta)]^2)\) by Slutsky's Theorem (Theorem \ref{asym.slutsky}(b)).  We seek to show what happens to the third term of (\ref{mathstats.delta.method.new.proof}) as \(n \to \infty\) (the result follows if the term converges in probability to 0). Note that for any \(n \geq 1\) and for any \(t >0\),

\[
\Pr(\sqrt{n}|h(Y_n - \theta )| > t) = \Pr \bigg(\sqrt{n}|h(Y_n - \theta)| > t \cap |Y_n - \theta | > \frac{t}{\sqrt{n}} \bigg) + \Pr \bigg(\sqrt{n}|h(Y_n - \theta )| > t \cap |Y_n - \theta | \leq \frac{t}{\sqrt{n}} \bigg)
\]

\begin{equation}\label{mathstats.delta.method.new.proof.b}
\iff \Pr(\sqrt{n}|h(Y_n - \theta )| > t)  \leq \Pr( |Y_n - \theta | > t/\sqrt{n}) + \Pr(\sqrt{n}|h(Y_n - \theta )| > t \cap |Y_n - \theta | \leq t/\sqrt{n}).
\end{equation}

Since we already have by assumption \( \sqrt{n} (Y_n - \theta) \xrightarrow{d} \mathcal{N}(0,\sigma^2)\), it follows that \(|Y_n - \theta |  \xrightarrow{p} 0 \). (For completeness, a detailed argument is included in the below lemma.) It then follows that the second term converges in probability to 0 if \(\lim_{n \to \infty} \Pr(|Y_n - \theta| > t/\sqrt{n}) = 0\) because \(\lim_{z \to 0} h(z)/z = 0\). Therefore for any \(t > 0\),

\[
\lim_{n \to \infty} \Pr(\sqrt{n}|h(Y_n - \theta )| > t)  = 0 \iff \sqrt{n}|h(Y_n - \theta )|  \xrightarrow{p}0
\]

which yields the result by (\ref{mathstats.delta.method.new.proof}).


\end{proof}

\begin{lemma} Under the same assumptions and notation as in Theorem \ref{mathstats.delta.method.thm},

\[
\lim_{n \to \infty} \Pr(|Y_n - \theta| > t/\sqrt{n}) = 0
\] 

\end{lemma}

\begin{proof} We will examine the behavior of the right side of (\ref{mathstats.delta.method.new.proof.b}) as \(n \to \infty\) by looking at the first term and showing that \(Y_n - \theta\) converges in probability to 0. If \(t >0\), then \(\Pr(|Y_n - \theta| > t) = \Pr(\sqrt{n} |Y_n - \theta| > t \sqrt{n})\), and if \(c >0 \) is a constant, then for sufficiently large \(n\), the last quantity is at most \(\Pr(\sqrt{n} |Y_n - \theta| > c)\). So we have

\[
\Pr(|Y_n - \theta | > t) = \Pr(\sqrt{n} |Y_n - \theta | > t \sqrt{n}) \leq \Pr(\sqrt{n} |Y_n - \theta|  > c)
\]

But as \(n \to \infty\), \(c\) can be any constant (arbitrarily large). So

\[
\lim_{n \to \infty} \Pr(\sqrt{n}|Y_n - \theta  |> t) \leq \int_{c}^\infty e^{-y^2/2} \frac{1}{\sqrt{2 \pi}} dy.
\] 

Therefore

\[
\lim_{n \to \infty} \Pr(|Y_n - \theta| > t/\sqrt{n}) = 0.
\] 

\end{proof}

%\begin{proof}[Original proof from class notes; seems to have changed in notes online] We will use a first order Taylor expansion (mean value theorem). From the Mean Value Theorem, there exists a random \(Z_n\) between \(\theta\) and \(Y_n\) such that
%
%\[
%f'(Z_n)  = \frac{f(Y_n) - f(\theta)}{(Y_n - \theta)}  
%\]
%
%\begin{equation}\label{mathstats.delta.method.proof}
% \iff f(Y_n) = f(\theta) + f'(Z_n) (Y_n - \theta).
%\end{equation}
%
%Note that \(\theta\) is fixed but \(Y_n\) is random, and therefore \(Z_n\) is also random. 
%
%\begin{itemize}
%
%\item We will first show that \(Y_n - \theta\) converges in probability to 0. If \(t <0\), then \(\Pr(Y_n - \theta < t) = \Pr(\sqrt{n} (Y_n - \theta) < t \sqrt{n})\), and if \(c <0 \) is a constant, then for sufficiently large \(n\), the last quantity is at most \(\Pr(\sqrt{n} (Y_n - \theta) < c)\). So we have
%
%\[
%\Pr(Y_n - \theta < t) = \Pr(\sqrt{n} (Y_n - \theta) < t \sqrt{n}) \leq \Pr(\sqrt{n} (Y_n - \theta) < c)
%\]
%
%But as \(n \to \infty\), \(c\) can be any constant (arbitrarily large in the negative direction).
%
%\[
%\lim_{n \to \infty} \Pr(Y_n - \theta < t) \leq \int_{-\infty}^c e^{-y^2/2} \frac{1}{\sqrt{2 \pi}} dy
%\] 
%
%Therefore, 
%
%\[
%\lim_{n \to \infty} \Pr(Y_n - \theta < t) = 0.
%\] 
%
%\item By a similar argument, for all \(t > 0\), \(\Pr(Y_n - \theta > t) =0\). So \(Y_n - \theta\) converges in distribution to 0, and therefore also converges in probability to 0 because converge in distribution to a constant is equivalent to convergence in probability to a constant.
%
%\end{itemize}
%
%\
%
%Since \(|Z_n - \theta| \leq | Y_n - \theta|\), \(Z_n - \theta\) also converges in probability to 0. That is, \(Z_n\) converges in probability to \(\theta\) as \(n \to \infty\). Since \(f'\) is continuous, \(f'(Z_n)\) converges in probability to \(f'(\theta)\) by Proposition 2.36 in the Math 541A notes (Theorem \ref{asym.contmappthm}, continuous functions conserve convergence in probability). Note that by (\ref{mathstats.delta.method.proof}),
%
%\[
%\sqrt{n}[f(Y_n) - f(\theta)]  = f'(Z_n) \sqrt{n}(Y_n - \theta).
%\]
%
%So since \(f'(Z_n)\) converges in probability to a constant and \(\sqrt{n} (Y_n - \theta)\) converges in distribution by assumption, by Proposition 2.36 in the Math 541A notes (Slutsky's Theorem, Theorem \ref{asym.slutsky}), the right side converges in probability to \(f'(\theta)\) multiplied by a mean zero Gaussian with variance \(\sigma^2\).
%
%\end{proof}

\begin{theorem}[\textbf{Convergence Theorem with Bounded Moment, Theorem 4.16 in 541A notes.}] Let \(X_1, X_2, \ldots\) be random variables that converge in distribution to a random variable \(X\). Assume \(\exists \ \epsilon > 0, c < \infty\) such that \(\E(|X_n|^{1 + \epsilon}) \leq c , \ \ \forall \ n \geq 1\). Then

\[
\E(X) = \lim_{n \to \infty} \E(X_n).
\]

\end{theorem}

\begin{proof} In Heilman's Graduate Probability Notes, Theorem 1.59 and Exercise 3.8(iii).

\end{proof}

If \(f'(\theta) = 0\) in the Delta Method, we can instead use a second order Taylor expansion as follows.

\begin{theorem}[\textbf{Second Order Delta Method, Theorem 4.17 in Math 541A Notes.}]Let \(\theta \in \mathbb{R}\). Let \(Y_1, Y_2, \ldots\) be random variables such that \(\sqrt{n}(Y_n - \theta)\) converges in distribution to a mean zero Gaussian random variable with variance \(\sigma^2 > 0\). Let \(f: \mathbb{R} \to \mathbb{R}\). Assume that \(f''\) exists and is continuous, \(f'(\theta) = 0\) and \(f''(\theta) \neq 0\). Then

\[
n(f(Y_n) - f(\theta))
\]

converges in distribution to a \(\chi_1^2\) random variable multiplied by \(\sigma^2 \frac{1}{2} | f''(\theta)|\) as \(n \to \infty\).

\end{theorem}

\begin{proof} Using a second order Taylor expansion of \(f\), there exists a random \(Z_n\) between \(\theta\) and \(Y_n\) such that

\begin{equation}\label{mathstats.delta.method.2nd.proof}
f(Y_n) = f(\theta) + f'(\theta)(Y_n - \theta) + \frac{1}{2} f''(Z_n)(Y_n - \theta)^2 = f(\theta) + \frac{1}{2} f''(Z_n)(Y_n - \theta)^2
\end{equation}

where the second equality follows because \(f'(\theta)=0\). As in the proof of Theorem \ref{mathstats.delta.method.thm}, \(Z_n \xrightarrow{p} \theta\). Since \(f''\) is continuous, \(f''(Z_n)\) converges in probability to \(f''(\theta)\) by Proposition 2.36 in the Math 541A notes (Theorem \ref{asym.contmappthm}, continuous functions conserve convergence in probability). Therefore using (\ref{mathstats.delta.method.2nd.proof}),

\[
n(f(Y_n) - f(\theta)) = \frac{1}{2} f''(Z_n) \cdot n(Y_n - \theta)^2
\]

Note that \(\sqrt{n} (Y_n - \theta)\) converges in distribution to a mean zero Gaussian random variable by assumption, so \( n(Y_n - \theta)^2\) converges in distribution to a \(\chi_1^2\) random variable by Proposition 2.36 in the Math 541A notes (Theorem \ref{asym.contmappthm}). So since \(f''(Z_n)\) converges in probability to a constant, by Proposition 2.36 in the Math 541A notes (Slutsky's Theorem, Theorem \ref{asym.slutsky}), the right side converges in probability to \(\frac{1}{2} f''(\theta) \sigma\) multiplied by a \(\chi_1^2\) random variable.

\end{proof}

\subsubsection{Simulation of Random Variables}

\begin{proposition} If \(X: \Omega \to \mathbb{R}\) is an arbitrary random variable with cumulative distribution function \(F: \mathbb{R} \to [0,1]\), then the function \(F^{-1}\) (if it exists) is a random variable on \([0,1]\) with the uniform probability law on \((0,1)\) that is equal in distribution to \(X\).

\end{proposition}

\begin{proof} Starting with the cdf of \(F^{-1}(u)\),

\[
\Pr(s \in [0,1]: F^{-1}(s) \leq t) = \Pr(s \in [0,1]: F(t) > s) = F(t) = \Pr( \omega \in \Omega: X(\omega) \leq t)
\]

where the third equality uses the definition of a uniform probability law on \((0,1)\).

\end{proof}

\begin{remark} If \(F^{-1}\) does not exist, it can still work if you construct a generalized inverse of \(F\). See Exercise 4.20 in the Math 541A notes.

\end{remark}

\begin{example}[\textbf{Example 4.22 in Math 541A notes}] Let \(X\) be an exponential random variable with parameter 1.

\[
\Pr(X \leq t) = \int_0^t e^{-x} dx = [- e^{-x}]_0^t = 1 - e^{-t} = F(t)
\]

We seek \(F^{-1}(t)\): 

\[
1 - e^{-y} = t \iff e^{-y} = 1 - t \iff -y = \log(1-t) \iff y = -\log(1-t) \implies F^{-1}(t) = -\log(1-t)
\]

So to simulate an exponential random variable with parameter 1, sample \(-\log(1-U)\) where \(U \sim \operatorname{U}(0,1)\).

\end{example}

\begin{remark} What if the cdf is hard to compute? For example, in a Gaussian distribution: 

\[
F(t) = \int_{-\infty}^t (2 \pi)^{-1/2} \exp(-x^2/2)  dx.
\]

\(F^{-1}\) cannot be described using elementary formulas, so \(F^{-1}(u)\) is not the best way to simulate a Gaussian random variable. When using the Central Limit Theorem approach (see 541A notes for details), Edgeworth expansion says: if we replace \(U_1, \ldots, U_n\) with i.i.d. \(X_1, \ldots, X_n\) and the first \(m\) moments of \(X_1\) agree with the first \(m\) moments of Gaussian random variables, then the error in the CLT approximation to a Gaussian is \(n^{-(m-1)/2}\). (See \url{https://en.wikipedia.org/wiki/Edgeworth_series}.) But this is still inefficient, because one Gaussian sample requires \(n\) uniform samples.

\end{remark}

\begin{proposition}[\textbf{Box-Muller Algorithm}] Let \(U_1, U_2\) be independent random variable distributed in \((0,1)\). Define

\[
R:= \sqrt{-2 \log(U_1)}
\]

\^ this density is something like \(e^{-x^2/2}\)

\[
\Psi := 2 \pi U_2
\]

\[
X := R \cos(\Psi), \ \ \ Y:= R \sin(\Psi)
\]

Then \(X, Y\) are independent standard Gaussian random variables. 
\end{proposition}

\begin{proof} Homework problem.

\end{proof}

\subsection{Data Reduction}

Suppose we have some data and an exponential family. We would like to find the parameter \(\theta\) among the exponential family that fits the data well. Suppose we have a large data set, maybe so large that you can't store all the data in RAM at once. What is the ``least memory" or ``most efficient" method for finding \(\theta\)? The answer: try to find a statistic that captures all the relevant information about \(\theta\). For example, to find the mean of a Gaussian sample, use the sample mean. You don't have to store all the raw data, you can just store the sample mean. The following is a generalization of this concept:

\subsubsection{Sufficient Statistics}

\begin{definition}[\textbf{Sufficient Statistic; definition 5.1 in Math 541A notes}] Suppose \(X_1, \ldots, X_n\) is a sample of size \(n\) from a distribution \(f\) where \(f \in \{f_\theta: \theta \in \Theta \}\) is a family of distributions (such as an exponential family). Let \(g: \mathbb{R}^n \to \mathbb{R}^k\) so that \(Y:= g(X_1, \ldots, X_n) \) is a statistic. We say that \(Y\) is a \textbf{sufficient statistic} for \(\theta\) if for every \(y \in \mathbb{R}^k\) and for every \(\theta \in \Theta\), the conditional distribution of \((X_1, \ldots, X_n)\) given \(Y=y\) (with respect to probabilities given by \(f_\theta\)) does not depend on \(\theta\). That is, \(Y\) provides sufficient information to determine \(\theta\) from \(X_1, \ldots, X_n\).

\end{definition}

\begin{remark} Based on a comment Heilman made on class, this definition assumes independence of the random variables? Basically everything in this class does?

\end{remark}

\textbf{Goldstein lecture:} Suppose we have a model \(\{f_\theta: \theta \in \Theta\}\) which we interpret as a set of densities or mass functions. We have \(\Theta \in \mathbb{R}^p\), and we know the model up to \(p\) parameters. Example; we have \(X_1, X_2, \ldots, X_n \sim \) i.i.d. \(f_\theta\) where \(\theta \in (\mu, \sigma^2), \mu \in \mathbb{R}\), where \(f_\theta \sim \mathcal{N}(\mu, \sigma^2)\), 
then
\[
\Theta = \{ (\mu, \sigma^2): \mu \in \mathbb{R}, \sigma^2 > 0\}.
\]

\begin{example}[\textbf{Example 5.2 in 541A notes}]\label{mathstats.541a.ex.5.2} Let \(X_1, \ldots, X_n\) be a random sample of size \(n\) from a Bernoulli distribution with parameter \(0 < \theta < 1\). Then \(Y:= X_1 = \ldots + X_n\) is sufficient for \(\theta\).

\end{example}

\begin{proposition}[\textbf{Example 5.2 in 541A notes}]\label{mathstats.541a.ex.5.2} Let \(X_1, \ldots, X_n\) be a random sample of size \(n\) from a Bernoulli distribution with parameter \(0 < \theta < 1\). Let \(Y:= X_1 = \ldots + X_n\). Then

\[
\mathbb{P}_\theta(X=x \mid Y=y) = \begin{cases}
0 & y \neq \sum_i x_i \\
\left. \middle/ \binom{n}{y} \right. & \sum_i x_i = y
\end{cases}
\]

\end{proposition}

\begin{remark} If a statistic is sufficient for \(\theta\), then we can use that sufficient statistic to re-create the data (or re-create an equivalent data set with the same statistical properties as far we are concerned with estimating the parameter of interest).

\end{remark}

\begin{proof} Let \(x_1, \ldots, x_n \in [0,1]\). Let \(0 \leq y \leq n\) be an integer. Then \(Y\) is binomial with parameters \(n\) and \(\theta\). We may assume \(y = x_1 + \ldots + x_n\), otherwise there is nothing to show. Using the definition of conditional probability, 

\[
\Pr\big( (X_1, \ldots, X_n) = (x_1, \ldots, x_n) \mid Y = y \big) = \frac{1}{\Pr(Y=y)} \cdot \Pr\big( (X_1, \ldots, X_n) = (x_1, \ldots, x_n) \cap Y = y \big) 
\]

\[
= \frac{1}{\Pr(Y=y)} \cdot \Pr\big( (X_1, \ldots, X_n) = (x_1, \ldots, x_n) \big) 
\]

Using independence and the definition of a binomial distribution, we have

\[
= \frac{1}{\binom{n}{y} \theta^y (1 - \theta)^{n-y}} \cdot \prod_{i=1}^n \Pr\big( X_i = x_i \big)  = \frac{1}{\binom{n}{y} \theta^y (1 - \theta)^{n-y}} \cdot \prod_{i=1}^n \theta^{x_i} (1- \theta)^{1-x_i}
\]

\[
= \frac{1}{\binom{n}{y} \theta^y (1 - \theta)^{n-y}} \cdot \theta^{y} (1- \theta)^{n-y} = \frac{1}{\binom{n}{y}}.
\]

Since this expression does not depend on \(\theta\), \(Y\) is sufficient for \(\theta\).

\end{proof}

\begin{example}[\textbf{Example 5.3 in 541A notes}]\label{mathstats.541a.ex.5.3} Let \(X_1, \ldots, X_n\) be a sample of size \(n\) from a Gaussian distribution with known variance \(\sigma^2 > 0\) and unknown mean \(\mu \in \mathbb{R}\). Then \(Y:= (X_1, \ldots, X_n)/n\) is a sufficient statistic for \(\mu\).

\end{example}

\begin{proof} Note that \(Y\) is a Gaussian random variable with mean \(\mu\) and variance \(\sigma^2/n\). Let \(x_1, \ldots, x_n \in \mathbb{R}\) and let \(y = (x_1 + \ldots + x_n)/n\). Then

\[
f_{X_1, \ldots, X_n \mid Y}(x_1, \ldots, x_n \mid y) = \frac{1}{f_Y(y)} \cdot f_{X_1, \ldots , X_n, Y} (x_1, \ldots, x_n, y) = \frac{1}{f_Y(y)} \cdot f_{X_1, \ldots , X_n} (x_1, \ldots, x_n, y)
\]

Since

\[
f_X(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \bigg(-\frac{ (x-\mu)^2}{2 \sigma^2} \bigg)  = \frac{1}{\sigma \sqrt{2 \pi}} \exp \bigg(\frac{ -x^2 - \mu^2 + 2 \mu x}{2 \sigma^2} \bigg) 
\]

we have

\[
= \frac{1}{f_Y(y)} \cdot \prod_{i=1}^n f_{X_i}(x_i) = \frac{1}{f_Y(y)} \cdot \bigg( \frac{1}{\sigma \sqrt{2 \pi}} \bigg)^n \cdot \exp \bigg( - \frac{1}{2 \sigma^2}(x_1^2 + \ldots + x_n^2) - \frac{n \mu^2}{2 \sigma^2} + \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i\bigg)
\]

\[
= \frac{ \bigg( \frac{1}{\sigma \sqrt{2 \pi}} \bigg)^n \cdot \exp \bigg( - \frac{1}{2 \sigma^2}(x_1^2 + \ldots + x_n^2) - \frac{n \mu^2}{2 \sigma^2} + \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i\bigg) } {n^{1/2} (\sigma^2 2 \pi)^{-1/2} \exp \bigg( - \frac{n}{2 \sigma^2} y^2 - \frac{n}{2 \sigma^2} \mu^2 + \frac{n \mu}{\sigma^2} y \bigg) }
\]

\[
= \frac{ \bigg( \frac{1}{\sigma \sqrt{2 \pi}} \bigg)^n \cdot \exp \bigg( - \frac{1}{2 \sigma^2}(x_1^2 + \ldots + x_n^2)\bigg) } {n^{1/2} (\sigma^2 2 \pi)^{-1/2} \exp \bigg( - \frac{n}{2 \sigma^2} y^2  \bigg) }
\]

Because \(\mu\) does not appear in this expression, \(Y\) is sufficient for \(\mu\).

\end{proof}

\begin{theorem}[\textbf{Factorization Theorem, Theorem 5.4 in 541A notes}]Suppose \(X = (X_1, \ldots, X_n)\) is a random sample of size \(n\) from a distribution \(f\) where \(f \in \{f_\theta: \theta \in \Theta\}\) is a family of probability density functions or probability mass functions.  Let \(t: \mathbb{R}^n \to \mathbb{R}^k\), so \(Y:= t(X_1, \ldots, X_n)\) is a statistic. Then \(Y\) is sufficient for \(\theta\) if and only if there exists a nonnegative \(\{g_\theta: \theta \in \Theta\}\), \(h: \mathbb{R}^n \to \mathbb{R}\), \(g_\theta: \mathbb{R}^k \to \mathbb{R}\) such that

\[
f_\theta(x) = g_\theta(t(x)) h(x), \ \ \ \forall \ x \in \mathbb{R}^n, \ \ \forall \theta \in \Theta.
\]


\end{theorem}

\begin{proof} We will prove only the discrete case to avoid measure theory. For a general case, see Keener Section 6.4. 

\

Suppose \(Y\) is sufficient. Let \(x \in \mathbb{R}^n\). Note that by definition and using \(Y = t(X)\),

\[
f_\theta(x) = \mathbb{P}_\theta (X = x) = \mathbb{P}_\theta (X = x \cap t(X) = t(x)) = \mathbb{P}_\theta ( Y = t(x))   \mathbb{P}_\theta (X = x \mid Y = t(x)) 
\]

By sufficiency, \( \mathbb{P}_\theta (X = x \mid Y = t(x)) \) does not depend on \(\theta\). Therefore we have found the equation in the theorem with \(g_\theta(t(x)) = \mathbb{P}_\theta ( Y = t(x))  \), \(h(x) = \mathbb{P}_\theta (X = x \mid Y = t(x))  \), so the factorization holds.

\

Now suppose there exists a nonnegative \(\{g_\theta: \theta \in \Theta\}\), \(h: \mathbb{R}^n \to \mathbb{R}\), \(g_\theta: \mathbb{R}^k \to \mathbb{R}\) such that

\[
f_\theta(x) = g_\theta(t(x)) h(x), \ \ \ \forall \ x \in \mathbb{R}^n, \ \ \forall \theta \in \Theta.
\]

Define \(r_\theta(z) := \mathbb{P}_\theta(t(X) = z)  \ \ \forall \ z \in \mathbb{R}^k\).  Also define \(t^{-1} t(x) := \{y \in \mathbb{R}6n; t(y) = t(x) \} \ \ \forall x \in \mathbb{R}^n\). To show sufficiency, we need to show that \(\mathbb{P}_\theta (X = x \mid Y = t(x))\) does not depend on \(\theta\). Note that

\[
\mathbb{P}_\theta (X = x \mid Y = t(x)) = \frac{f_\theta(x)}{f_Y(t(x))}= \frac{f_\theta(x)}{r_\theta(t(x))}
\]

Using our assumption and the Total Probability Theorem, we have

\[
= \frac{ g_\theta(t(x) h(x))}{\mathbb{P}_\theta(t(X) = t(x))} = \frac{ g_\theta(t(x)) h(x)}{\sum_{z \in t^{-1} t(x)} \mathbb{P}_\theta (X = z)} = \frac{ g_\theta(t(x)) h(x)}{\sum_{z \in t^{-1} t(x)} f_\theta(z)}  = \frac{ g_\theta(t(x)) h(x)}{\sum_{z \in t^{-1} t(x)}  g_\theta(t(z)) h(z) }
\]

By definition of \(t^{-1}t(z)\), we we can write this as

\[
= \frac{ g_\theta(t(x)) h(x)}{\sum_{z \in t^{-1} t(x)}  g_\theta(t(x)) h(z) } = \frac{  h(x)}{\sum_{z \in t^{-1} t(x)}   h(z) }
\]

Since this expression does not contain \(\theta\), \(Y\) is sufficient.

\end{proof}

\begin{remark} Intuition: data only cares about \(\theta\) through \(t(x)\). 

\end{remark}


\begin{exercise} Suppose \(X_1, X_2, \ldots, X_n \sim\) i.i.d. \(\mathcal{N}(0,1)\). So density is 

\[
\frac{1}{\sqrt{2 \pi}} e^{- 1/2(x- \theta)^2} 
\]

Show that 

\[
e^{-1/2(x^2 -2x \theta + \theta^2)} = 
\]

\[
f_\theta(x) = \bigg( \frac{1}{2\pi} \bigg)^{n/2} e^{2/12 \sum_i X_i^2} e^{\theta \sum X_i - n\theta^2/2}
\]

so if \(t(x) = \sum_{i=1}^n X_i\), \(h(x) =  \bigg( \frac{1}{2\pi} \bigg)^{n/2} e^{2/12 \sum_i X_i^2} \), \(g_\theta(t(x)) =  e^{\theta \sum X_i - n\theta^2/2}\), then by the Factorization Theorem this (\(\overline{x}\)) is a sufficient statistic.

\end{exercise}

\begin{remark} In this case, if we deleted the original data we could recreate the original data by sampling from a \(\mathcal{N}(0,1)\) distribution, then add the difference between the mean we get and the original sample mean to get an equivalent data set to the original one.

\end{remark}

\begin{remark} Suppose we define \(t(x):=x, \ \forall \ x \in \mathbb{R}^n\). Then \(Y = t(X_1, \ldots, X_n) = (X_1, \ldots, X_n)\) is (trivially) sufficient for \(\theta\). In general there will be infinitely many sufficient statistics for \(\theta\). For instance, in Example \ref{mathstats.541a.ex.5.2}, \((X_1 + \ldots + X_n)^2\) is also sufficient. So is  \((X_1 + \ldots + X_n)^3\), etc. More generally, any invertible function of any sufficient statistic is itself sufficient.

\

We can see that \((X_1k \ldots, X_n)\) is sufficient for \(\theta\) if \((t(x_1, \ldots, x_n) = (x_1, \ldots, x_n)\), \(g_\theta = f_\theta, h =1\). But this is not really helpful. We see we are interested in sufficient statistics that are smaller---reduce the data (in some sense) as much as possible.

\end{remark}

\subsubsection{Minimal Sufficient Statistics}

\begin{proposition}

Suppose \(X = (X_1, \ldots, X_n)\) is a random sample of size \(n\) from a distribution \(f\) where \(f \in \{f_\theta: \theta \in \Theta \}\) is a family of probability density functions or probability mass functions. Let \(t: \mathbb{R}^n \to \mathbb{R}^k\). Let \(Y:= t(X_1, \ldots, X_n)\). Assume \(Y\) is sufficient of \(\theta\). Let \(a: \mathbb{R}^n \to \mathbb{R}^m\), let \(Z:=u(X_1, \ldots, X_n)\). suppose there exists \(r: \mathbb{R}^m \to \mathbb{R}^k\) such that \(r(u(x)) = t(x)\) for all \(x \in \mathbb{R}^n\). That is, suppose \(Y = r(Z)\). Then \(Z\) is sufficient for \(\theta\).

\end{proposition}

\begin{proof}

\[
f_\theta(x) = g_\theta(t(x)) h(x) = g_\theta(r(u(x)) h(x)
\]


there exists \(g_\theta: \mathbb{R}^k \to [0, \infty)\). \(Y\) is sufficient.

\

Define 

\[
\tilde{g}_\theta(y) := g_\theta(r(y)) \ \ \forall \ y \in \mathbb{R}^m
\]

So

\(f_\theta(x) = \tilde{g}_\theta(u(x)) h(x) \ \ \forall x \in \mathbb{R}^n\). So \(Z\) is sufficient for \(\theta\) by the Factorization Theorem.

\end{proof}

\begin{definition}[\textbf{Minimal sufficient statistic}] Suppose \(X = (X_1, \ldots, X_n)\) is a random sample of size \(n\) from a distribution \(f\) where \(f \in \{f_\theta: \theta \in \Theta \}\) is a family of probability density functions or probability mass functions. Let \(t: \mathbb{R}^n \to \mathbb{R}^k\). Let \(Y:= t(X_1, \ldots, X_n)\). Assume \(Y\) is sufficient of \(\theta\). Then \(Y\) is a \textbf{minimal sufficient statistic} for \(\theta\) if for every statistic \(Z: \Omega \to \mathbb{R}^m\) that is sufficient for \(\theta\) there exists a function \(\mathbb{R}^m \to \mathbb{R}^k\) such that \(Y = r(Z)\).

\end{definition}

\begin{remark} Minimal sufficient statistics are not in general unique (because if you take any one-to-one function you get another one), but they are unique up to invertible transformations. (This is true because if \(Y\) and \(Z\) are both minimal sufficient, \(Y = r(Z)\) and \( Z = s(Y)\), so \(Y = r(s(Y)), Z = s(r(Z))\)). They exist under mild assumptions.

\end{remark}

\begin{theorem}[\textbf{Theorem 5.8 in 541A notes}] Let \(\{f_\theta: \theta \in \Theta\}\) be a family of probability density functions or probability mass functions. Let \(X_1, \ldots, X_n\) be a random sample from a member of the family. Let \(t: \mathbb{R}^n \to \mathbb{R}^m\) and define \(Y:= t(X_1, \ldots, X_n)\). Assume that \(Y\) is sufficient for \(\theta\). \(Y\) is minimal sufficient if and only if the following condition holds for every \(x, y \in \mathbb{R}^n\):


\noindent\fbox{
\parbox{\textwidth}{
\[
\text{There exists } c(x,y) \in \mathbb{R} \text{ that does not depend on } \theta \text{ such that } f_\theta(x) = c(x,y) f_\theta(y) \ \ \ \forall \ \theta \in \Theta 
\]

if and only if

\[
t(x) = t(y).
\]
}
}

\end{theorem}

\begin{proof} We are only considering probability mass functions to make things easier. We first prove sufficiency. We will show that the condition holding implies that \(Y\) is minimal sufficient.

\

Recall the likelihood ratio:

\[
\frac{f_\theta(x)}{f_\theta(y)}
\]

Note that the condition is equivalent to the likelihood ratio not depending on \(\theta\) if and only if \(t(x) = t(y)\). Consider the range \(R=\{t(x): x \in \mathbb{R}^n\}\) jnd then for \(t \in R\) let \(S_t = \{y: S(y)= t\}\). If \(t\) is in \(R\), then there must be some \(z\) so that \(t(z)\) is that \(z\). This ensure that \(S_t\) is nonempty (there is at least one \(z\) so that \(t(z) = t\). Let \(t(x) \in R\), then \(S_{t(x)}\) is nonempty (in particular it contains \(x\)).  Pick any \(y\) you like in \(t(x)\): \(y \in S\). \(S\) depends on \(t(x)\) so we can index it by \(t(x)\): \(y_{t(x)} \in S_{t(x)}\). let \(y_t \in S_t\). Note that 

\[
t(y_{t(x)}) = t(x)
\]

But now by the assumption, we have

\[
\text{There exists } c(x,y_{t(x)}) \in \mathbb{R} \text{ that does not depend on } \theta \text{ such that } f_\theta(x) = c(x,y_{t(x)}) f_\theta(y_{t(x)}) \ \ \ \forall \ \theta \in \Theta 
\]

%or
%
%\[
%\frac{f_\theta(x)}{f_\theta(y_{t(x)})} = c(x,y_{t(x)})
%\]

Then note that if \(h(x) =  c(x,y_{t(x)})\), \(g_\theta(t) = f_\theta(y_t) \iff g_\theta(t(x)) =  f_\theta(y_{t(x)})\), we meet the conditions for the Factorization Theorem. So using the Factorization Theorem, \(Y\) is sufficient.

\[
\vdots
\]

\textbf{Part we did in class on Friday 02/15: evidently (according to Goldstein) this shows that the statistic is minimal but not necessarily sufficient.} Let \(Z = u(X_1, \ldots, X_n)\) be any other sufficient statistic. We need to eventually show that \(Y\) is a function of \(Z\). By the Factorization Theorem, there exists \(h: \mathbb{R}^m \to \mathbb{R}, g_\theta; \mathbb{R}^n \to \mathbb{R}\) such that for all \(\theta \in \Theta\), 

\[
f_\theta(x) = g_\theta'(u(x))h'(x), \ \ \ \forall \ x \in \mathbb{R}^n.
\]

Let \(y \in \mathbb{R}^n\). If \(h'(y) = 0\), then \(f_\theta(y) = 0\) for all \(\theta \in \Theta\). So, \(\mathbb{P}_\theta(y \in \mathbb{R}^n : h'(y) =0) = 0\) for all \(\theta \in \Theta\). So we can ignore this possibility since it's a probability 0 event and assume \(h'(y) > 0, \forall y \in \mathbb{R}^n\).

\

Now let \(x, y \in \mathbb{R}^n\) such that \(u(x) = u(y)\). \textbf{By an exercise we're going to do later}, if \(t(x) = t(y)\) then \(t\) is a function of \(u\), so we will be done if we can show that \(t(x) = t(y)\).  Note that  since \(u(x) = u(y)\), for any \(\theta \in \Theta \)

\[
f_\theta(x) = g_\theta'(u(x)) h'(x) = \frac{g_\theta'(u(y))h'(x) = g_\theta'(u(y)) h'(y) }{f_\theta(y)} \frac{h'(x)}{h'(y)} = f_\theta(y) \frac{h'(x)}{h'(y)}, \ \ \ forall \theta \in \Theta
\]

So define \(c(x,y) = h'(x)/h'(y)\), we have 

\[
f_\theta(x) = f_\theta(y) c(x,y), \ \ \ \forall \theta \in \Theta
\]

Therefore \(t(x) = t(y)\), so we're done showing that if the condition holds then \(Y\) is minimal sufficient. 

\

Then next thing to show is that if \(Y\) is minimal sufficient then the condition holds.

\[
\vdots
\]

For any \(z \in \{t(x): x \in \mathbb{R}^n\}\), let \(x_z\) be any element of \(t^{-1}(z)\)

\end{proof}

\begin{proposition}[\textbf{Proposition Larry Goldstein gave in class}] If \(\{f_\theta: \theta \in \Theta\}\), \(\Theta \in \mathbb{R}^n\), \(f_\theta\) all densities or all mass functions. Then a minimal sufficient statistic exists.

\end{proposition}

\begin{proof}[Proof where \(\theta\) is countable] By relabeling, let \(\Theta = \{1, 2, \ldots\}\). We say for \(x, y \) sequences, we define the equivalence relation \(x \sim y \text{ if } \exists \ \alpha \in \mathbb{R} \text{ such that } x = \alpha y\). Finite

\[
t: \mathbb{R}^n \to \mathbb{R}^m/\sim, \ \ \ \ \ \Theta = \{1, \ldots, m\}
\]

\[
t(x) = (f_1(x), f_2(x), \ldots, f_m(x))
\]

these likelihood are multiples of each other where \(\alpha\) is a constant. The likelihood ratio is a constant not depending on \(\theta\). If they have the same \(t(x)\) then we have that.

\end{proof}

\subsection{Point Estimation}

\subsubsection{Method of Moments}

\subsubsection{Maximum likelihood estimator}

\begin{proposition}[\textbf{Stats 100B homework problem}] Suppose \(X_1, X_2, \ldots, X_n\) is a random sample from a \(\operatorname{Bernoulli}(p)\) distribution. Let \(X =\sum_{i=1}^n X_i\). Then

\begin{enumerate}[(a)]

\item The maximum likelihood estimator of \(p\) is \( \hat{p} = X/n\).

\item The maximum likelihood estimator attains the Cramer-Rao lower bound.

\item The maximum likelihood estimator is a consistent estimator for \(p\).

\item \( \frac{\hat{p}(1-\hat{p})}{n-1}\) is an unbiased estimator for \(\Var(\hat{p} = p(1-p)/n\). 

\end{enumerate}

\end{proposition}

\begin{proof}

\begin{enumerate}[a.]

% 1a
\item Bernoulli random variable:

\[
P(X_i = x) = p^x(1-p)^{1-x}
\]

Assuming independent samples,

\[
L = \prod_{i=1}^n p^{X_i}(1-p)^{1-X_i} = p^{\sum_{i=1}^nX_i}(1-p)^{\sum_{i=1}^n1 - X_i}
\]

\[
\log(L) = \sum_{i=1}^nX_i \log(p) + \bigg( \sum_{i=1}^n1 - X_i \bigg) \log(1-p)
\]

\[
\deriv{\log(L)}{p} = \frac{1}{p}\sum_{i=1}^nX_i - \frac{1}{1-p} \sum_{i=1}^n (1 - X_i) = 0
\]

\[
\frac{1}{\hat{p}}\sum_{i=1}^nX_i  = \frac{1}{1-\hat{p}} \sum_{i=1}^n (1 - X_i)
\]

\[
(1-\hat{p})\sum_{i=1}^nX_i = \hat{p} \sum_{i=1}^n (1 - X_i)
\]

\[
\sum_{i=1}^n X_i  = \hat{p}\sum_{i=1}^n (X_i + 1 - X_i)
\]

\[
\sum_{i=1}^n X_i  = n \hat{p}
\]

\[
\boxed{
\hat{p} = \frac{1}{n} \sum_{i=1}^n X_i}
\]

%1b
\item 

\[
\Var(\hat{p}) = \frac{1}{n^2} \Var\bigg( \sum_{i=1}^n X_i\bigg)
\]

Since \(X_i\) are independent, we can write this as

\[
\frac{1}{n^2} \sum_{i=1}^n \Var(X_i)
\]

And since \(X_i\) is Bernoulli, \(\Var(X_i) = p(1-p)\).

\[
= \frac{1}{n^2} \sum_{i=1}^n p(1-p) = \frac{1}{n^2} n p(1-p) = \boxed{\frac{p(1-p)}{n}}
\]


Cramer-Rao lower bound:

\[
\Var(\hat{\theta}) \geq 1/\bigg(-n \E \bigg[ \frac{\partial^2 \log(f(X; \theta))}{\partial \theta^2} \bigg] \bigg)
\]

\[
\pderiv{}{p} \log(p^x(1-p)^{1-x}) = \pderiv{}{p} (x\log(p) + (1-x)\log(1-p)) = \frac{x}{p} - \frac{1-x}{1-p}
\]

\[
 \frac{\partial^2 \log(f(X; \theta))}{\partial \theta^2} = \pderiv{}{p}(\frac{x}{p} - \frac{1-x}{1-p}) = -\frac{x}{p^2} - \frac{1-x}{(1-p)^2}
\]

\[
\E \bigg[  \frac{\partial^2 \log(f(X; \theta^2))}{\partial \theta}  \bigg] = \E(-\frac{x}{p^2} - \frac{1-x}{(1-p)^2}) =  -\frac{1}{p^2} \E(x) - \frac{1}{(1-p)^2} \E(1-x) = -\frac{1}{p^2}p - \frac{1}{(1-p)^2}(1 - p)
\]

\[
= -\frac{1}{p} - \frac{1}{1-p} = -\frac{1-p}{p(1-p)} - \frac{p}{p(1-p)} = \frac{-1}{p(1-p)} 
\]


\[
\implies \Var(\hat{p}) \geq 1 / \bigg( -n( \frac{-1}{p(1-p)}) \bigg) = \frac{p(1-p)}{n} = \Var(\hat{p})
\]

\item \begin{enumerate}[(1)]

\item \textbf{Unbiased:} 

\[
E\bigg( \frac{X}{n} \bigg) = \frac{np}{p} = p
\]

\item \textbf{\(\Var(\hat{\theta}) \to 0\)  as \( n \to \infty\) :}

\[
\lim_{n \to \infty} \Var\bigg( \frac{X}{n} \bigg)  = \lim_{n \to \infty} \frac{1}{n^2} \sum_{i=1}^n \Var(X_i) = \lim_{n \to \infty} \frac{1}{n^2}\cdot n p (1-p) = \lim_{n \to \infty} \frac{p(1-p)}{n} = \boxed{0}
\]

\end{enumerate}

Therefore \(\frac{X}{n} \) is a consistent estimator of \(p\).

\item \[
\E (\hat{\sigma^2}) = \E \bigg[ \frac{1}{n} \bigg( \frac{X}{n} \big( 1 - \frac{X}{n} \big) \bigg) \bigg] = \E \bigg[   \frac{X (n - X)}{n^3}  \ \bigg] = \frac{1}{n^3} \E[nX - (X)^2]  = \frac{1}{n^2} \E(X) - \frac{1}{n^3} \E(X^2)
\]

\[
= \frac{1}{n^2} \cdot np - \frac{1}{n^3}\big( \Var(X) + (E(X))^2 \big) = \frac{p}{n} - \frac{np(1-p)}{n^3} - \frac{p^2n^2}{n^3} = \frac{pn - p + p^2 - p^2n}{n^2}
\]

\[
= \frac{p(n - 1 + p - pn)}{n^2} = \frac{p(n-1)(1-p)}{n^2}
\]

This is a biased estimator since \( \Var(X) = \frac{p(1-p)}{n}\) (since \(X\) is bionomial).

\[
c \cdot \frac{p(n-1)(1-p)}{n^2} =  \frac{p(1-p)}{n} \implies \boxed{c = \frac{n}{n-1}}
\]

\end{enumerate}

\end{proof}

\begin{proposition}[\textbf{Stats 100B homework problem}] Suppose that \(X\) follows a geometric distribution and we take an i.i.d. sample of size \(n\). Then the maximum likelihood estimator of \(p\) is 

\[
\hat{p} = \frac{n}{\sum_{i=1}^n X_i} = \frac{1}{\bar{X}}.
\]

\end{proposition}

\begin{proof}
Since sample is i.i.d.:

\[
L = \prod_{i=1}^n p(1-p)^{X_i-1} = p^n(1-p)^{-n + \sum_{i=1}^n X_i }
\]

\[
\log(L) = n \log(p) + \bigg( -n + \sum_{i=1}^n X_i \bigg) \log(1-p)
\]

\[
\deriv{\log(L)}{p} =  \frac{n}{p} - \frac{1}{1-p} \bigg( -n + \sum_{i=1}^n X_i \bigg) = 0 
\]

\[
\frac{n}{\hat{p}} = \frac{1}{1-\hat{p}} \bigg( -n + \sum_{i=1}^n X_i \bigg)
\]

\[
(1-\hat{p}) \hat{p} = -n \hat{p} + \hat{p} \sum_{i=1}^n X_i
\]

\[
n = \hat{p} \sum_{i=1}^n X_i
\]

\[
\hat{p} = \frac{n}{\sum_{i=1}^n X_i} = \frac{1}{\bar{X}}
\]

\end{proof}

\begin{proposition}[\textbf{Stats 100B homework problem}] Suppose \(X_1, X_2, \ldots, X_n\) is a random sample from a \(\operatorname{Poisson}(\lambda)\) distribution. Then 

\begin{enumerate}[(a)]

\item The maximum likelihood estimator of \(\lambda\) is 

\[
\hat{\lambda} = \frac{1}{n} \sum_{i=1}^n X_i .
\]

\item The variance of the maximum likelihood estimator is

\[
\Var(\hat{\lambda})  = \frac{\lambda}{n}
\]

\item The maximum likelihood estimator is a minimum variance unbiased estimator.

\item The maximum likelihood estimator is consistent.

\end{enumerate}

\end{proposition}

\begin{proof}

\begin{enumerate}[(a)]

\item

\[
f(X_i; \lambda) = \frac{\lambda^{X_i} e^{-\lambda}}{X_i!}
\]

Assuming the samples are independent,

\[
L = \prod_{i=1}^n \frac{\lambda^{X_i} e^{-\lambda}}{X_i!} = \bigg(e^{-n\lambda}\lambda^{\sum_{i=1}^n X_i}\bigg)/ \prod_{i=1}^n X_i!
\]

\[
\log(L) = -n \lambda + \bigg( \sum_{i=1}^n X_i \bigg) \log(\lambda) - \sum_{i=1}^n \log(X_i!)
\]

\[
\deriv{\log(L)}{\lambda} = -n + \frac{1}{\lambda} \sum_{i=1}^n X_i  = 0
\]

\[
\implies 
\hat{\lambda} = \frac{1}{n} \sum_{i=1}^n X_i = \bar{x}
\]

\item 

\[
\Var(\hat{\lambda}) = \Var(\frac{1}{n} \sum_{i=1}^n X_i) = \frac{1}{n^2} \Var(\sum_{i=1}^n X_i)
\]

Since \(X_i\) are i.i.d. this can be written as

\[
= \frac{1}{n^2} \sum_{i=1}^n \Var(X_i) = \frac{1}{n^2} \sum_{i=1}^n  \lambda = \frac{\lambda}{n}
\]

\item Cramer-Rao lower bound:

\[
\Var(\hat{\lambda}) \geq 1/\bigg(-n \E \bigg[ \frac{\partial^2 \log(f(X; \lambda))}{\partial \lambda^2} \bigg] \bigg)
\]

\[
\log(f(X; \lambda)) = \log\bigg( \frac{\lambda^{X_i} e^{-\lambda}}{X_i!} \bigg) =   X_i \log(\lambda) -  \lambda -  \log(X_i!)
\]

\[
\pderiv{}{\lambda} \log(f(X; \lambda)) = \frac{1}{\lambda} X_i - 1
\]

\[
 \frac{\partial^2 \log(f(X; \lambda))}{\partial \lambda^2} = -\frac{1}{\lambda^2}  X_i 
\]

\[
\E \bigg[  \frac{\partial^2 \log(f(X; \lambda^2))}{\partial \lambda}  \bigg] = -\frac{1}{\lambda^2} \E ( X_i ) = -\frac{1}{\lambda^2} \lambda = -\frac{1}{\lambda} 
\]

\[
\implies \Var(\hat{\lambda}) \geq 1/\bigg(-n \E \bigg[ \frac{\partial^2 \log(f(X; \lambda))}{\partial \lambda^2} \bigg] \bigg) = \frac{1}{n/\lambda} = \boxed{\frac{\lambda}{n} = \Var(\hat{\lambda})}
\]

Since \(\Var(\hat{\lambda})\) equals the Cramer-Rao lower bound, \(\hat{\lambda}\) is a MVUE.

\item We already know the MLE is unbiased. To show consistency, we show \(\Var(\hat{\theta}) \to 0\)  as \( n \to \infty\).

\[
\lim_{n \to \infty} \Var(\hat{\lambda})  = \lim_{n \to \infty} \frac{\lambda}{n} = \boxed{0}
\]

Therefore \(\hat{\lambda}\) is a consistent estimator of \(\lambda\).

\end{enumerate}
\end{proof}

\begin{proposition}[\textbf{Stats 100B homework problem}] Suppose \(X_1, X_2, \ldots, X_n\) is a random sample from a \(\operatorname{Exponential}(\lambda)\) distribution. Then the maximum likelihood estimator of \(\lambda\) is

\[
\hat{\lambda} = n / \sum_{i=1}^n X_i = \frac{1}{\bar{X}}.
\]

\end{proposition}

\begin{proof}

\[
f(X_i; \lambda) = \lambda e^{-\lambda X_i}
\]

Assuming the samples are independent,

\[
L = \prod_{i=1}^n \lambda e^{-\lambda X_i} = \lambda^n \exp(-\lambda \sum_{i=1}^n X_i)
\]

\[
\log(L) = n \log(\lambda) + -\lambda \sum_{i=1}^n X_i
\]

\[
\deriv{\log(L)}{\lambda} = \frac{n}{\lambda} - \sum_{i=1}^n X_i = 0
\]

\[
\implies 
\hat{\lambda} = n / \sum_{i=1}^n X_i = \frac{1}{\bar{X}}
\]


\end{proof}

\begin{proposition}[\textbf{Stats 100B homework problem}] Let \(X_1, X_2, \ldots, X_n\) be an i.i.d. random sample from a normal population with mean zero and unknown variance \(\sigma^2\). Then

\begin{enumerate}[(a)]

\item The maximum likelihood estimator of \(\sigma^2\) is

\[
\hat{\sigma^2} = \frac{1}{n} \sum_{i=1}^n X_i^2.
\]

\item The maximum likelihood estimator of \(\sigma^2\) is unbiased.

\item The maximum likelihood estimator of \(\sigma^2\) has variance

\[
\Var \bigg( \frac{1}{n} \sum_{i=1}^n X_i^2 \bigg)  =  \frac{2 \sigma^4}{n}
\]

and is consistent.

\item The variance of the maximum likelihood estimator of \(\sigma^2\) reaches the Cramer-Rao lower bound.

\end{enumerate}

\end{proposition}

\begin{proof}

\begin{enumerate}[a.]

% 12a
\item Since sample is i.i.d. \(\mathcal{N}(0, \sigma^2) \):

\[
L = \prod_{i=1}^n \frac{1}{\sigma \sqrt{2 \pi}} \exp\bigg(- \frac{1}{2} \bigg[\frac{X_i}{\sigma}\bigg]^2 \bigg) = (2 \pi\sigma^2)^{-n/2} \exp\bigg(- \frac{1}{2 \sigma^2} \sum_{i=1}^n X_i^2 \bigg)
\]

\[
\log(L) = -\frac{n}{2} \log(2 \pi\sigma^2) - \frac{1}{2 \sigma^2} \sum_{i=1}^n X_i^2 =  -\frac{n}{2} \log(2 \pi) - \frac{n}{2} \log(\sigma^2) - (\sigma^2)^{-1} \frac{1}{2} \sum_{i=1}^n X_i^2
\]

\[
\pderiv{\log(L)}{\sigma^2} = -\frac{n}{2} \frac{1}{\sigma^2} +   ({\sigma^2})^{-2} \frac{1}{2}  \sum_{i=1}^n X_i^2 = 0
\]

\[
\frac{\sum_{i=1}^n X_i^2}{(\hat{\sigma^2})^2}  = \frac{n}{\hat{\sigma^2}}
\]

\[
\hat{\sigma^2} = \frac{1}{n} \sum_{i=1}^n X_i^2
\]


% 12b
\item 

\[
\E \bigg(  \frac{1}{n} \sum_{i=1}^n X_i^2 \bigg) = \frac{1}{n} \E \bigg(  \sum_{i=1}^n X_i^2 \bigg)
\]

Since the sample is i.i.d., this can be written as 

\[
 \frac{1}{n}   \sum_{i=1}^n \E (X_i^2)
\]

Since \(X_i \textapprox \mathcal{N}(0, \sigma^2) \), \(X_i^2/\sigma^2 \textapprox \chi_1^2\). So we have

\[
\E\bigg( \frac{X_i^2}{\sigma^2} \bigg) = 1
\]

\[
\frac{1}{\sigma^2} \E( X_i^2) = 1
\]

\[
\E( X_i^2) = \sigma^2
\]

Therefore
\[
\E \bigg(  \frac{1}{n} \sum_{i=1}^n X_i^2 \bigg) = \frac{1}{n}   \sum_{i=1}^n \ \sigma^2 = \frac{1}{n} n \sigma^2 = \boxed{\sigma^2}
\]



% 12c
\item

\[
\Var \bigg( \frac{1}{n} \sum_{i=1}^n X_i^2 \bigg) = \frac{1}{n^2} \Var \bigg( \sum_{i=1}^n X_i^2 \bigg)
\]

Since \(X_i\) is i.i.d. this can be written as 

\[
\frac{1}{n^2} \sum_{i=1}^n \Var (X_i^2 )
\]

Again, , \(X_i^2/\sigma^2 \textapprox \chi_1^2\), so we have

\[
\Var\bigg( \frac{X_i^2}{\sigma^2} \bigg) = 2
\]

\[
\frac{1}{\sigma^4} \Var(X_i^2) = 2
\]

\[
\Var(X_i^2) = 2 \sigma^4
\]

Therefore

\[
\Var \bigg( \frac{1}{n} \sum_{i=1}^n X_i^2 \bigg) = \frac{1}{n^2} \sum_{i=1}^n 2 \sigma^4 = \frac{2n\sigma^4}{n^2} = \boxed{ \frac{2 \sigma^4}{n}}
\]

Test for consistency (already known that estimate is unbiased):

\[
\lim_{n \to \infty} \Var \bigg( \frac{1}{n} \sum_{i=1}^n X_i^2 \bigg) = \lim_{n \to \infty} \frac{2 \sigma^4}{n} = \boxed{0}
\]

So this is a consistent estimator of \(\sigma^2\).

% 12d
\item Cramer-Rao lower bound:

\[
\Var(\hat{\theta}) \geq 1/\bigg(-n \E \bigg[ \frac{\partial^2 \log(f(X; \theta))}{\partial \theta^2} \bigg] \bigg)
\]

\[
 \log(f(X; \theta)) = \log \bigg[ \frac{1}{\sigma \sqrt{2 \pi}} \exp\bigg(- \frac{1}{2} \bigg[\frac{X_i}{\sigma}\bigg]^2 \bigg) \bigg] = - \frac{1}{2} \log(2 \pi) -  \frac{1}{2} \log(\sigma^2) - \frac{1}{2}X_i^2 (\sigma^2)^{-1}
\]

\[
\pderiv{}{\sigma^2}\bigg( - \frac{1}{2} \log(2 \pi) -  \frac{1}{2} \log(\sigma^2) - \frac{1}{2}X_i^2 (\sigma^2)^{-1}\bigg) = -\frac{1}{2 \sigma^2} + \frac{1}{2}(X_i)^2 (\sigma^2)^{-2}
\]

\[
 \frac{\partial^2 \log(f(X; \theta))}{\partial (\sigma^2)^2} = \frac{1}{2}(\sigma^2)^{-2}  - X_i^2(\sigma^2)^{-3}
\]

\[
\E \bigg[  \frac{\partial^2 \log(f(X; \theta^2))}{\partial \theta}  \bigg] = \E \bigg[  \frac{1}{2}(\sigma^2)^{-2}  - X_i^2(\sigma^2)^{-3}  \bigg] = \frac{1}{2 \theta^4} - \frac{1}{\theta^6} \E(X_i^2) = \frac{1}{2 \theta^4} - \frac{\theta^2}{\theta^6} = -\frac{1}{2 \theta^4}
\]

\[
\implies \Var(\hat{\sigma^2}) \geq 1/\bigg(-n \E \bigg[ \frac{\partial^2 \log(f(X; \theta))}{\partial \theta^2} \bigg] \bigg) = \frac{1}{n/(2 \theta^4)} = \boxed{\frac{2 \theta^4}{n}}
\]

Therefore the variance of this estimator is equal to the Cramer-Rao lower bound.

\end{enumerate}

\end{proof}

\subsubsection{Bayes estimator}

\subsubsection{EM Alglorithm}

\subsubsection{Comparison of estimators}

\subsection{Hypothesis Testing}

\begin{proposition}[\textbf{Stats 100B Homework problem}] Let \(Y_1, Y_2, \ldots, Y_n\) be the outcomes of \(n\) independent Bernoulli trials. Then by the Neyman-Pearson lemma, the best critical region for testing

\[
H_0: p = p_0 \ \ \ \ \ \ \ H_a: p > p_0
\]

is

\[
\frac{y}{n} = \frac{1}{n}\sum Y_i  > \frac{\log(K) + n \log \bigg( \frac{1 - p_a}{1 - p_0} \bigg)}{n\log \bigg( \frac{p_0(1 - p_a)}{p_a(1 - p_0)} \bigg)}.
\]

\end{proposition}

\begin{proof}

\[
\Pr(\sum Y_i = y) = {n \choose y}p^y(1-p)^{n-y}
\]

Using the Neyman-Pearson lemma (let \(p_a\) be some particular value of \(p > p_0\)):

\[
\frac{L(p_0)}{L(p_a)} = \frac{{n \choose y}p_0^y(1-p_0)^{n-y}}{{n \choose y}p_a^y(1-p_a)^{n-y}} < K
\]

\[
\bigg( \frac{p_0}{p_a} \bigg) ^y   \bigg( \frac{1 - p_0}{1 - p_a} \bigg)^{n} \bigg( \frac{1 - p_0}{1 - p_a} \bigg)^{-y}  < K
\]

\[
\bigg( \frac{p_0(1 - p_a)}{p_a(1 - p_0)} \bigg) ^y  < K  \bigg( \frac{1 - p_a}{1 - p_0} \bigg)^{n} 
\]

\[
y \log \bigg( \frac{p_0(1 - p_a)}{p_a(1 - p_0)} \bigg) < \log(K) + n \log \bigg( \frac{1 - p_a}{1 - p_0} \bigg)
\]

Aside:

\[
\frac{p_0(1 - p_a)}{p_a(1 - p_0)} = \frac{p_0 - p_0 p_a}{p_a - p_0 p_a} < 1
\]

since by assumption \(p_a > p_0\). Therefore \( \log \bigg( \frac{p_0(1 - p_a)}{p_a(1 - p_0)} \bigg) < 0 \). So we have

\[
\frac{y}{n} = \frac{1}{n}\sum Y_i  > \frac{\log(K) + n \log \bigg( \frac{1 - p_a}{1 - p_0} \bigg)}{n\log \bigg( \frac{p_0(1 - p_a)}{p_a(1 - p_0)} \bigg)}
\]

as the form for our critical region.

\end{proof}





%\end{document}







